# ==============================================================================
# SOEN General Purpose Training Configuration Template (Callbacks Version)
#
# This file serves as a comprehensive template and tutorial for configuring
# SOEN model training experiments. Uncomment and modify sections as needed.
# ==============================================================================

# ------------------------------------------------------------------------------
# Experiment Metadata
# ------------------------------------------------------------------------------
description: "A brief, human-readable description of the experiment's goal."
seed: 42  # A base seed for ensuring reproducibility.

# ==============================================================================
# Training Configuration
# ==============================================================================
training:
  # --- Basic Settings ---
  batch_size: 64                # Number of samples per batch.
  max_epochs: 100               # Maximum number of training epochs.
  
  # --- Autoregressive (AR) Mode for Sequence-to-Sequence Tasks ---
  autoregressive: false             # Set to `true` to enable AR mode.
  autoregressive_mode: next_token   # "next_token" (LM style) or "seq2seq"
  # time_steps_per_token: 1         # (AR Only) Number of simulation timesteps per token.
  # autoregressive_start_timestep: 0 # (AR Only) Timestep to start computing loss from.

  # --- Compute & Hardware Settings (for PyTorch Lightning Trainer) ---
  accelerator: "auto"             # "auto", "cpu", "gpu", "mps", "tpu". 'auto' is recommended.
  precision: "32-true"            # "32-true" (standard), "16-mixed" (for faster GPU training).
  devices: "auto"                 # "auto", 1 (for 1 GPU), [0, 1] (for specific GPUs).
  num_workers: 0                  # Number of CPU workers for data loading. Set to 0 for debugging.
  deterministic: false            # For reproducibility. May impact performance slightly.

  # --- Gradient & Optimization ---
  accumulate_grad_batches: 1      # Use >1 to simulate a larger batch size on limited memory.
  gradient_clip_val: 1.0          # Clip gradients to prevent explosion. `null` to disable.
  gradient_clip_algorithm: "norm" # "norm" or "value".

  # --- Checkpointing ---
  checkpoint_every_n_epochs: 1    # How often to save a checkpoint.
  checkpoint_save_top_k: 3        # Keep the top 3 best models based on validation loss.
  checkpoint_save_last: true      # Always save the state of the last completed epoch.
  save_initial_state: true        # Save a checkpoint of the model before training starts.
  save_soen_core_in_checkpoint: true # Writes a matching .soen sidecar next to each retained .ckpt

  # --- Optimizer ---
  optimizer:
    name: "adamw"                 # Options: "adamw", "adam", "lion", "sgd", "rmsprop".
    lr: 0.001                     # Base learning rate. Will be controlled by the scheduler.
    kwargs:
      weight_decay: 1e-4          # Optimizer-specific arguments.

  # --- Loss Functions ---
  # Define one or more weighted loss components.
  loss:
    losses:
      # Example 1: Standard Cross-Entropy for classification
      - name: cross_entropy
        weight: 1.0
        params: {}

      # Example 2: Cross-Entropy + Gap Loss Regularizer
      # - name: cross_entropy
      #   weight: 1.0
      # - name: gap_loss
      #   weight: 0.5
      #   params:
      #     margin: 0.2

      # or Gap loss
      - name: gap_loss
        weight: 1
        params:
          margin: 0.2

  # --- Resuming Training ---
  # train_from_checkpoint: "path/to/your/last.ckpt"

# ==============================================================================
# Data Configuration
# ==============================================================================
data:
  data_path: "path/to/your/dataset.h5" # IMPORTANT: Update this path.
  cache_data: true                # Cache dataset in RAM for speed. Disable if dataset is too large.
  num_classes: 10                 # Number of classes for classification tasks.
  val_split: 0.2
  test_split: 0.1
  
  # --- Preprocessing ---
  target_seq_len: 100             # Resample all sequences to this length. `null` to disable.

  # --- Input Encoding for Sequence Data (e.g., text) ---
  input_encoding: "raw"           # "raw" (default), "one_hot".
  # vocab_size: 65                # For "one_hot", you MUST specify vocab_size.

# ==============================================================================
# Model Configuration
# ==============================================================================
model:
  base_model_path: "path/to/your/base_model.pth"
  load_exact_model_state: false
  time_pooling: 
    name: "max"
    params: {scale: 1.0}
  dt: 195.3125 # remember, 1 dt is 1.28ps.
  dt_learnable: false

# ==============================================================================
# Logging Configuration
# ==============================================================================
logging:
  project_dir: "experiments/"            # Base directory for the entire project
  project_name: "SOEN_Experiments"
  group_name: "My_Experiment_Group"
  experiment_name: null
  
  metrics:
    - "accuracy"
    - "perplexity"
    - "bits_per_character"
    # - "top_3_accuracy"

  log_freq: 50
  log_batch_metrics: true
  log_level: "INFO"
  log_gradients: false
  track_layer_params: false
  track_connections: false

  # --- MLflow (optional) ---
  # Enable to log to MLflow alongside TensorBoard
  mlflow_active: false
  mlflow_tracking_uri: "file:./mlruns"   # or e.g., "http://localhost:5000"
  mlflow_password: "xxx" # ask team for password if you want to use the EC2 MLFlow server (website is at https://mlflow-greatsky.duckdns.org)
  # Experiment derives from project_name; run name derives from repeat dir
  mlflow_log_artifacts: true              # log checkpoints and .soen sidecars
  mlflow_tags:
    env: local

# ==============================================================================
# Callbacks Configuration
# ==============================================================================
callbacks:
  # --- Learning Rate Scheduler ---
  # The LR scheduler is now a callback. Choose one of the following options.
  lr_scheduler:
    # Option 1: Constant Learning Rate
    type: "constant"
    lr: 0.001

    # Option 2: Cosine Annealing with Warmup (Popular Choice)
    # type: "cosine"
    # max_lr: 0.001
    # min_lr: 1e-6
    # warmup_epochs: 5
    # cycle_epochs: 50

  # --- Early Stopping ---
  # Stop training if a metric doesn't improve for a number of epochs.
  # early_stopping:
  #   monitor: "val_loss"
  #   patience: 10
  #   mode: "min"

  # --- Other Custom Callbacks ---
  # Example: A scheduler that gradually increases sequence length during training.
  # seq_len_scheduler:
  #   active: true
  #   start_len: 20
  #   end_len: 100
  #   growth_type: "linear"
  #   start_epoch: 5
