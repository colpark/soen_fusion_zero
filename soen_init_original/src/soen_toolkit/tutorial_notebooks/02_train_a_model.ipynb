{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02 — Train a SOEN Model\n",
    "\n",
    "In this tutorial, we’ll walk through training a pre-built SOEN model using the training configuration file located at:\n",
    "`tutorial_notebooks/training/training_configs/pulse_net.yaml`.\n",
    "\n",
    "We’ll use the `run_from_config` function to launch training. This function makes it easy to set up an experiment — once all training settings are defined in your YAML file, you can start training with a single command.\n",
    "\n",
    "You can run it either in a script or directly from the command line.\n",
    "Python:\n",
    "`run_from_config(str(BASE_CONFIG), script_dir=Path.cwd())`\n",
    "CLI:\n",
    "`python -m soen_toolkit.training --config path/to/training_config.yaml`\n",
    "\n",
    "### ML Task Overview\n",
    "\n",
    "This example tackles a binary classification problem on time-series inputs:\n",
    "- Class 1: Input contains a single pulse.\n",
    "- Class 2: Input contains two distinct pulses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Ensure soen_toolkit is importable\nimport sys\nfrom pathlib import Path\n\n# Add src directory to path if running from notebook location\nnotebook_dir = Path.cwd()\nfor parent in [notebook_dir] + list(notebook_dir.parents):\n    candidate = parent / \"src\"\n    if (candidate / \"soen_toolkit\").exists():\n        sys.path.insert(0, str(candidate))\n        break\n\nfrom soen_toolkit.training.trainers.experiment import run_from_config"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "We’ll use the example model and dataset to launch a local test training run. You can experiment by modifying the training YAML file as needed. For more detailed configurations, see: `src/soen_toolkit/training/examples/training_configs`.\n",
    "\n",
    "Additional information about the training process can be found in: `src/soen_toolkit/training/README.md`.\n",
    "\n",
    "If you wish to construct your own datasets, please use hdf5 file format. All instructions can be found at: `docs/DATASETS.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training via Python API\n",
    "run_from_config(\"training/training_configs/pulse_net.yaml\", script_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Diagnostic: Check if weights are updating**\n\nRun this cell after training to verify the model is actually learning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# DIAGNOSTIC: Check if model weights are updating during training\n# ============================================================================\nimport torch\nimport glob\nfrom pathlib import Path\n\ndef diagnose_training():\n    \"\"\"Check if the trained model has meaningful weight updates.\"\"\"\n    \n    # Find the latest checkpoint\n    ckpt_patterns = [\n        \"training/temp/**/checkpoints/**/*.ckpt\",\n        \"training/temp/**/*.ckpt\",\n    ]\n    \n    all_ckpts = []\n    for pattern in ckpt_patterns:\n        all_ckpts.extend(glob.glob(pattern, recursive=True))\n    \n    if not all_ckpts:\n        print(\"No checkpoint found. Run training first.\")\n        return\n    \n    latest_ckpt = max(all_ckpts, key=lambda x: Path(x).stat().st_mtime)\n    print(f\"Loading checkpoint: {latest_ckpt}\\n\")\n    \n    # Load checkpoint\n    ckpt = torch.load(latest_ckpt, map_location='cpu')\n    state_dict = ckpt.get('state_dict', ckpt)\n    \n    print(\"=\" * 70)\n    print(\"WEIGHT DIAGNOSTICS\")\n    print(\"=\" * 70)\n    \n    # Analyze each parameter\n    total_params = 0\n    zero_params = 0\n    tiny_params = 0  # params with very small magnitude\n    \n    print(f\"\\n{'Parameter':<50} {'Shape':<15} {'Mean':<12} {'Std':<12} {'Max':<12}\")\n    print(\"-\" * 100)\n    \n    for name, param in state_dict.items():\n        if 'weight' in name.lower() or 'bias' in name.lower() or 'connection' in name.lower():\n            p = param.float()\n            num_params = p.numel()\n            total_params += num_params\n            \n            # Count zeros and tiny values\n            zero_params += (p == 0).sum().item()\n            tiny_params += (p.abs() < 1e-6).sum().item()\n            \n            mean_val = p.mean().item()\n            std_val = p.std().item()\n            max_val = p.abs().max().item()\n            \n            shape_str = str(list(p.shape))\n            print(f\"{name:<50} {shape_str:<15} {mean_val:>11.6f} {std_val:>11.6f} {max_val:>11.6f}\")\n    \n    print(\"-\" * 100)\n    print(f\"\\nTotal trainable params analyzed: {total_params:,}\")\n    print(f\"Zero-valued params: {zero_params:,} ({100*zero_params/max(total_params,1):.1f}%)\")\n    print(f\"Tiny params (<1e-6): {tiny_params:,} ({100*tiny_params/max(total_params,1):.1f}%)\")\n    \n    # Check for gradient flow issues\n    print(\"\\n\" + \"=\" * 70)\n    print(\"DIAGNOSIS\")\n    print(\"=\" * 70)\n    \n    if zero_params > total_params * 0.9:\n        print(\"⚠️  WARNING: >90% of weights are zero - possible dead network\")\n    elif tiny_params > total_params * 0.8:\n        print(\"⚠️  WARNING: >80% of weights are tiny - gradients may not be flowing\")\n    else:\n        print(\"✓ Weights have reasonable magnitude distribution\")\n    \n    # Check if weights look initialized vs trained\n    for name, param in state_dict.items():\n        if 'connection' in name.lower() and 'weight' not in name.lower():\n            p = param.float()\n            if p.std() < 0.001:\n                print(f\"⚠️  {name}: Very low variance ({p.std():.6f}) - may not have trained\")\n\n# Run diagnostic\ndiagnose_training()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View logs in TensorBoard (Optional)**\n",
    "\n",
    "Start TensorBoard in a terminal so you can watch metrics live.\n",
    "\n",
    "1. Activate your environment (if not already):\n",
    "2. Run TensorBoard, pointing at the logs root printed above (\"Logs root:\"):\n",
    "```bash\n",
    "tensorboard --logdir \"/path/to/logs/root\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Quick Notes on Datasets\n",
    "\n",
    "soen_toolkit.training models expect datasets in **HDF5 format** with the following structure:\n",
    "\n",
    "- **Inputs** (`data`): `[N, T, D]`  \n",
    "  - `N`: number of samples  \n",
    "  - `T`: sequence length  \n",
    "  - `D`: feature dimension (should be equal to the number of units in the input layer - ID=0)\n",
    "\n",
    "- **Labels** (`labels`): shape depends on the task  \n",
    "  - Classification (seq2static): `[N]` (int64 class indices)  \n",
    "  - Classification (seq2seq): `[N, T]` (int64 per-timestep classes)  \n",
    "  - Regression (seq2static): `[N, K]` (float32)  \n",
    "  - Regression (seq2seq): `[N, T, K]` (float32)  \n",
    "  - Unsupervised (seq2seq): labels optional; inputs are used as targets  \n",
    "\n",
    "**Recommended layout:**\n",
    "\n",
    "root/\n",
    "train/{data, labels}\n",
    "val/{data, labels}\n",
    "test/{data, labels}\n",
    "\n",
    "**Key config notes:**\n",
    "- Set `training.paradigm` and `training.mapping` in your YAML (e.g., `supervised` + `seq2static`).  \n",
    "- Use `data.target_seq_len` to align input/output sequence lengths.  \n",
    "- Pooling for seq2static tasks is controlled via `model.time_pooling`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (soen-toolkit)",
   "language": "python",
   "name": "soen_toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}