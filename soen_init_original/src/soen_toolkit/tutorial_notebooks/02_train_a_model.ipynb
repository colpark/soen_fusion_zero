{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02 — Train a SOEN Model\n",
    "\n",
    "In this tutorial, we’ll walk through training a pre-built SOEN model using the training configuration file located at:\n",
    "`tutorial_notebooks/training/training_configs/pulse_net.yaml`.\n",
    "\n",
    "We’ll use the `run_from_config` function to launch training. This function makes it easy to set up an experiment — once all training settings are defined in your YAML file, you can start training with a single command.\n",
    "\n",
    "You can run it either in a script or directly from the command line.\n",
    "Python:\n",
    "`run_from_config(str(BASE_CONFIG), script_dir=Path.cwd())`\n",
    "CLI:\n",
    "`python -m soen_toolkit.training --config path/to/training_config.yaml`\n",
    "\n",
    "### ML Task Overview\n",
    "\n",
    "This example tackles a binary classification problem on time-series inputs:\n",
    "- Class 1: Input contains a single pulse.\n",
    "- Class 2: Input contains two distinct pulses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Ensure soen_toolkit is importable\nimport sys\nfrom pathlib import Path\n\n# Add src directory to path if running from notebook location\nnotebook_dir = Path.cwd()\nfor parent in [notebook_dir] + list(notebook_dir.parents):\n    candidate = parent / \"src\"\n    if (candidate / \"soen_toolkit\").exists():\n        sys.path.insert(0, str(candidate))\n        break\n\nfrom soen_toolkit.training.trainers.experiment import run_from_config"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "We’ll use the example model and dataset to launch a local test training run. You can experiment by modifying the training YAML file as needed. For more detailed configurations, see: `src/soen_toolkit/training/examples/training_configs`.\n",
    "\n",
    "Additional information about the training process can be found in: `src/soen_toolkit/training/README.md`.\n",
    "\n",
    "If you wish to construct your own datasets, please use hdf5 file format. All instructions can be found at: `docs/DATASETS.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training via Python API\n",
    "run_from_config(\"training/training_configs/pulse_net.yaml\", script_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "**Diagnostic: Check if weights are updating**\n\nRun this cell after training to verify the model is actually learning.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# DIAGNOSTIC: Check if model weights are updating during training\n# ============================================================================\nimport torch\nimport glob\nfrom pathlib import Path\n\ndef diagnose_training():\n    \"\"\"Check if the trained model has meaningful weight updates.\"\"\"\n    \n    # Find the latest checkpoint\n    ckpt_patterns = [\n        \"training/temp/**/checkpoints/**/*.ckpt\",\n        \"training/temp/**/*.ckpt\",\n    ]\n    \n    all_ckpts = []\n    for pattern in ckpt_patterns:\n        all_ckpts.extend(glob.glob(pattern, recursive=True))\n    \n    if not all_ckpts:\n        print(\"No checkpoint found. Run training first.\")\n        return\n    \n    latest_ckpt = max(all_ckpts, key=lambda x: Path(x).stat().st_mtime)\n    print(f\"Loading checkpoint: {latest_ckpt}\\n\")\n    \n    # Load checkpoint\n    ckpt = torch.load(latest_ckpt, map_location='cpu')\n    state_dict = ckpt.get('state_dict', ckpt)\n    \n    print(\"=\" * 70)\n    print(\"WEIGHT DIAGNOSTICS\")\n    print(\"=\" * 70)\n    \n    # Analyze each parameter\n    total_params = 0\n    zero_params = 0\n    tiny_params = 0  # params with very small magnitude\n    \n    print(f\"\\n{'Parameter':<50} {'Shape':<15} {'Mean':<12} {'Std':<12} {'Max':<12}\")\n    print(\"-\" * 100)\n    \n    for name, param in state_dict.items():\n        if 'weight' in name.lower() or 'bias' in name.lower() or 'connection' in name.lower():\n            p = param.float()\n            num_params = p.numel()\n            total_params += num_params\n            \n            # Count zeros and tiny values\n            zero_params += (p == 0).sum().item()\n            tiny_params += (p.abs() < 1e-6).sum().item()\n            \n            mean_val = p.mean().item()\n            std_val = p.std().item()\n            max_val = p.abs().max().item()\n            \n            shape_str = str(list(p.shape))\n            print(f\"{name:<50} {shape_str:<15} {mean_val:>11.6f} {std_val:>11.6f} {max_val:>11.6f}\")\n    \n    print(\"-\" * 100)\n    print(f\"\\nTotal trainable params analyzed: {total_params:,}\")\n    print(f\"Zero-valued params: {zero_params:,} ({100*zero_params/max(total_params,1):.1f}%)\")\n    print(f\"Tiny params (<1e-6): {tiny_params:,} ({100*tiny_params/max(total_params,1):.1f}%)\")\n    \n    # Check for gradient flow issues\n    print(\"\\n\" + \"=\" * 70)\n    print(\"DIAGNOSIS\")\n    print(\"=\" * 70)\n    \n    if zero_params > total_params * 0.9:\n        print(\"⚠️  WARNING: >90% of weights are zero - possible dead network\")\n    elif tiny_params > total_params * 0.8:\n        print(\"⚠️  WARNING: >80% of weights are tiny - gradients may not be flowing\")\n    else:\n        print(\"✓ Weights have reasonable magnitude distribution\")\n    \n    # Check if weights look initialized vs trained\n    for name, param in state_dict.items():\n        if 'connection' in name.lower() and 'weight' not in name.lower():\n            p = param.float()\n            if p.std() < 0.001:\n                print(f\"⚠️  {name}: Very low variance ({p.std():.6f}) - may not have trained\")\n\n# Run diagnostic\ndiagnose_training()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# DIAGNOSTIC 2: Check if gradients are flowing during forward/backward pass\n# ============================================================================\nimport torch\nimport h5py\nfrom pathlib import Path\n\ndef check_gradient_flow():\n    \"\"\"Test if gradients actually flow through the SOEN model.\"\"\"\n    \n    # Load the model architecture\n    from soen_toolkit.core.model_yaml import build_model_from_yaml\n    \n    model_path = Path(\"training/test_models/model_specs/1D_5D_2D_PulseNetSpec.yaml\")\n    if not model_path.exists():\n        print(f\"Model spec not found: {model_path}\")\n        return\n    \n    model = build_model_from_yaml(model_path)\n    model.train()\n    \n    # Load a small batch of data\n    data_path = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n    with h5py.File(data_path, 'r') as f:\n        x = torch.tensor(f['train']['data'][:8], dtype=torch.float32)\n        y = torch.tensor(f['train']['labels'][:8], dtype=torch.long)\n    \n    print(\"=\" * 70)\n    print(\"GRADIENT FLOW DIAGNOSTIC\")\n    print(\"=\" * 70)\n    print(f\"Input shape: {x.shape}\")\n    print(f\"Labels: {y.tolist()}\")\n    \n    # Forward pass\n    x.requires_grad_(True)\n    output, all_states = model(x)\n    \n    print(f\"\\nOutput shape: {output.shape}\")\n    print(f\"Output range: [{output.min():.4f}, {output.max():.4f}]\")\n    print(f\"Output mean per class: {output.mean(dim=0).tolist()}\")\n    \n    # Apply time pooling (max over time) like training does\n    if output.dim() == 3:\n        pooled = output.max(dim=1)[0]  # [batch, num_classes]\n    else:\n        pooled = output\n    \n    print(f\"Pooled output shape: {pooled.shape}\")\n    \n    # Compute loss\n    loss_fn = torch.nn.CrossEntropyLoss()\n    loss = loss_fn(pooled, y)\n    print(f\"\\nLoss: {loss.item():.6f}\")\n    \n    # Backward pass\n    loss.backward()\n    \n    # Check gradients\n    print(\"\\n\" + \"-\" * 70)\n    print(\"GRADIENT ANALYSIS\")\n    print(\"-\" * 70)\n    print(f\"{'Parameter':<45} {'Grad Norm':<15} {'Has Grad':<10}\")\n    print(\"-\" * 70)\n    \n    grad_issues = []\n    for name, param in model.named_parameters():\n        if param.grad is not None:\n            grad_norm = param.grad.norm().item()\n            has_grad = \"✓\" if grad_norm > 1e-10 else \"✗ (zero)\"\n            print(f\"{name:<45} {grad_norm:<15.8f} {has_grad}\")\n            if grad_norm < 1e-10:\n                grad_issues.append(name)\n        else:\n            print(f\"{name:<45} {'None':<15} ✗ (no grad)\")\n            grad_issues.append(name)\n    \n    # Check input gradient\n    if x.grad is not None:\n        print(f\"\\n{'Input gradient norm':<45} {x.grad.norm().item():<15.8f}\")\n    else:\n        print(f\"\\n{'Input gradient':<45} None\")\n    \n    print(\"\\n\" + \"=\" * 70)\n    print(\"DIAGNOSIS\")\n    print(\"=\" * 70)\n    \n    if grad_issues:\n        print(f\"⚠️  {len(grad_issues)} parameters have zero/no gradients:\")\n        for name in grad_issues[:5]:\n            print(f\"   - {name}\")\n        if len(grad_issues) > 5:\n            print(f\"   ... and {len(grad_issues) - 5} more\")\n        print(\"\\n→ Gradients may not be flowing through SOEN dynamics!\")\n        print(\"→ Check if the physics simulation is differentiable.\")\n    else:\n        print(\"✓ All parameters have non-zero gradients\")\n        print(\"→ Gradient flow looks healthy. Issue may be:\")\n        print(\"   - Learning rate too low (try 0.001 instead of 0.0001)\")\n        print(\"   - Not enough epochs (try 50 instead of 10)\")\n        print(\"   - Model capacity too small\")\n\n# Run gradient check\ncheck_gradient_flow()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View logs in TensorBoard (Optional)**\n",
    "\n",
    "Start TensorBoard in a terminal so you can watch metrics live.\n",
    "\n",
    "1. Activate your environment (if not already):\n",
    "2. Run TensorBoard, pointing at the logs root printed above (\"Logs root:\"):\n",
    "```bash\n",
    "tensorboard --logdir \"/path/to/logs/root\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Quick Notes on Datasets\n",
    "\n",
    "soen_toolkit.training models expect datasets in **HDF5 format** with the following structure:\n",
    "\n",
    "- **Inputs** (`data`): `[N, T, D]`  \n",
    "  - `N`: number of samples  \n",
    "  - `T`: sequence length  \n",
    "  - `D`: feature dimension (should be equal to the number of units in the input layer - ID=0)\n",
    "\n",
    "- **Labels** (`labels`): shape depends on the task  \n",
    "  - Classification (seq2static): `[N]` (int64 class indices)  \n",
    "  - Classification (seq2seq): `[N, T]` (int64 per-timestep classes)  \n",
    "  - Regression (seq2static): `[N, K]` (float32)  \n",
    "  - Regression (seq2seq): `[N, T, K]` (float32)  \n",
    "  - Unsupervised (seq2seq): labels optional; inputs are used as targets  \n",
    "\n",
    "**Recommended layout:**\n",
    "\n",
    "root/\n",
    "train/{data, labels}\n",
    "val/{data, labels}\n",
    "test/{data, labels}\n",
    "\n",
    "**Key config notes:**\n",
    "- Set `training.paradigm` and `training.mapping` in your YAML (e.g., `supervised` + `seq2static`).  \n",
    "- Use `data.target_seq_len` to align input/output sequence lengths.  \n",
    "- Pooling for seq2static tasks is controlled via `model.time_pooling`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (soen-toolkit)",
   "language": "python",
   "name": "soen_toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}