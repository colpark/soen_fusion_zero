{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DisruptCNN with decimated ECEi data\n",
        "\n",
        "This notebook adapts the decimated dataset into the original DisruptCNN file/metadata format, then runs the original dataloader and training loop with minimal changes.\n",
        "\n",
        "Decimated data root used here:\n",
        "`/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated`\n"
      ],
      "id": "20625427"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "import time\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DECIMATED_ROOT = Path(\"/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated\")\n",
        "COMPAT_ROOT = Path(os.environ.get(\"DISRUPTCNN_COMPAT_ROOT\", DECIMATED_ROOT.parent / \"disruptcnn_compat\"))\n",
        "DATA_ROOT = COMPAT_ROOT / \"data\"\n",
        "\n",
        "DISRUPT_DIR = DECIMATED_ROOT / \"disrupt\"\n",
        "CLEAR_DIR = DECIMATED_ROOT / \"clear\"\n",
        "\n",
        "USE_SYMLINKS = True  # set False to copy files into COMPAT_ROOT\n",
        "WRITE_OFFSETS = True  # writes 'offsets' dataset into H5 files\n",
        "\n",
        "OFFSET_WINDOW_MS = (-50.0, -10.0)  # same window as create_offsets.py\n",
        "NORMALIZE = True\n",
        "NORM_MAX_SHOTS = None  # set int to limit for quick stats\n",
        "\n",
        "BATCH_SIZE = 12\n",
        "DATA_STEP = 1  # decimated data already reduces rate; keep 1 to preserve timing\n",
        "NRECEPT = 30000\n",
        "NSUB = 78125\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "134ad39d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import original DisruptCNN modules\n",
        "\n",
        "We keep the original code intact and only adapt the data to its expected format.\n"
      ],
      "id": "e28e031a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "repo_root = Path.cwd()\n",
        "if repo_root.name != \"disruptcnn\":\n",
        "    repo_root = Path(\"/path/to/disruptcnn\")  # update if needed\n",
        "\n",
        "sys.path.insert(0, str(repo_root.parent))\n",
        "\n",
        "try:\n",
        "    from disruptcnn.loader import EceiDataset, data_generator\n",
        "    import disruptcnn.main as disrupt_main\n",
        "except Exception:\n",
        "    from loader import EceiDataset, data_generator\n",
        "    import main as disrupt_main\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "84874d3f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load decimated metadata and map columns\n",
        "\n",
        "This maps `meta.csv` columns into the original DisruptCNN shot list format.\n"
      ],
      "id": "519b65c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pick_col(df, candidates, required=True):\n",
        "    for col in candidates:\n",
        "        if col in df.columns:\n",
        "            return col\n",
        "    if required:\n",
        "        raise KeyError(f\"None of {candidates} found in meta.csv\")\n",
        "    return None\n",
        "\n",
        "META_PATH = DECIMATED_ROOT / \"meta.csv\"\n",
        "if not META_PATH.exists():\n",
        "    raise FileNotFoundError(f\"meta.csv not found at {META_PATH}\")\n",
        "\n",
        "meta = pd.read_csv(META_PATH)\n",
        "\n",
        "shot_col = pick_col(meta, [\"shot\", \"shot_id\", \"shotnum\", \"shot_number\"])\n",
        "tstart_col = pick_col(meta, [\"tstart\", \"t_start\", \"t_start_ms\"])\n",
        "\n",
        "dt_col = pick_col(meta, [\"dt\", \"dt_ms\", \"sample_dt\", \"sample_dt_ms\"])\n",
        "\n",
        "tlast_col = pick_col(\n",
        "    meta,\n",
        "    [\"tlast\", \"t_last\", \"tstop\", \"t_stop\", \"t_end\", \"t_segment_end\", \"t_segment_end_ms\"],\n",
        "    required=False,\n",
        ")\n",
        "\n",
        "tdisrupt_col = pick_col(\n",
        "    meta,\n",
        "    [\"tdisrupt\", \"t_disrupt\", \"t_disrupt_ms\", \"tdisrupt_ms\"],\n",
        "    required=False,\n",
        ")\n",
        "\n",
        "tflat_start_col = pick_col(meta, [\"t_flat_start\", \"tflat_start\", \"t_flat_start_ms\"], required=False)\n",
        "tflat_stop_col = pick_col(meta, [\"t_flat_stop\", \"tflat_stop\", \"t_flat_last\", \"tflat_last\", \"t_flat_end\"], required=False)\n",
        "tflat_dur_col = pick_col(meta, [\"t_flat_duration\", \"tflat_duration\"], required=False)\n",
        "\n",
        "shots = meta[shot_col].to_numpy().astype(int)\n",
        "tstart_ms = meta[tstart_col].to_numpy().astype(float)\n",
        "dt_ms = meta[dt_col].to_numpy().astype(float)\n",
        "\n",
        "def shot_to_h5(shot):\n",
        "    for base in (DISRUPT_DIR, CLEAR_DIR, DECIMATED_ROOT):\n",
        "        path = base / f\"{int(shot)}.h5\"\n",
        "        if path.exists():\n",
        "            return path\n",
        "    raise FileNotFoundError(f\"H5 file for shot {shot} not found in {DECIMATED_ROOT}\")\n",
        "\n",
        "if tlast_col is None:\n",
        "    tlast_ms = np.zeros_like(tstart_ms)\n",
        "    for i, shot in enumerate(shots):\n",
        "        path = shot_to_h5(shot)\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            n_samples = f[\"LFS\"].shape[-1]\n",
        "        tlast_ms[i] = tstart_ms[i] + n_samples * dt_ms[i]\n",
        "else:\n",
        "    tlast_ms = meta[tlast_col].to_numpy().astype(float)\n",
        "\n",
        "if tdisrupt_col is None:\n",
        "    disrupted_col = pick_col(meta, [\"disrupted\", \"is_disrupt\", \"is_disruptive\"], required=True)\n",
        "    tdisrupt_ms = np.where(meta[disrupted_col].to_numpy().astype(bool), tlast_ms, -1000.0)\n",
        "else:\n",
        "    tdisrupt_ms = meta[tdisrupt_col].to_numpy().astype(float)\n",
        "\n",
        "if tflat_start_col and tflat_dur_col:\n",
        "    tflat_start_ms = meta[tflat_start_col].to_numpy().astype(float)\n",
        "    tflat_duration_ms = meta[tflat_dur_col].to_numpy().astype(float)\n",
        "elif tflat_start_col and tflat_stop_col:\n",
        "    tflat_start_ms = meta[tflat_start_col].to_numpy().astype(float)\n",
        "    tflat_stop_ms = meta[tflat_stop_col].to_numpy().astype(float)\n",
        "    tflat_duration_ms = tflat_stop_ms - tflat_start_ms\n",
        "else:\n",
        "    tflat_start_ms = np.zeros_like(tstart_ms)\n",
        "    tflat_duration_ms = tlast_ms - tstart_ms\n",
        "\n",
        "is_disrupt = tdisrupt_ms > 0\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "a99bdd1b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create DisruptCNN-compatible data layout and shot lists\n",
        "\n",
        "This builds `data/disrupt`, `data/clear`, and the shot list files expected by the original loader.\n"
      ],
      "id": "fa82ddf5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def ensure_dir(path):\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def link_or_copy(src, dst):\n",
        "    if dst.exists():\n",
        "        return\n",
        "    if USE_SYMLINKS:\n",
        "        os.symlink(src, dst)\n",
        "    else:\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "\n",
        "def write_shot_file(path, shot_indices):\n",
        "    # The loader treats the 8th column as duration (t_flat_start + duration).\n",
        "    header = \"# Shot\\t# segments\\ttstart\\ttlast\\tdt\\tSNR min\\tt_flat_start\\tt_flat_last\\ttdisrupt\\n\"\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(header)\n",
        "        for i in shot_indices:\n",
        "            f.write(\n",
        "                f\"{shots[i]}\\t1\\t{tstart_ms[i]:.3f}\\t{tlast_ms[i]:.3f}\\t{dt_ms[i]:.6f}\\t0.00\\t\"\n",
        "                f\"{tflat_start_ms[i]:.3f}\\t{tflat_duration_ms[i]:.3f}\\t{tdisrupt_ms[i]:.3f}\\n\"\n",
        "            )\n",
        "\n",
        "\n",
        "ensure_dir(DATA_ROOT / \"disrupt\")\n",
        "ensure_dir(DATA_ROOT / \"clear\")\n",
        "\n",
        "for i, shot in enumerate(shots):\n",
        "    src = shot_to_h5(shot)\n",
        "    dst_dir = DATA_ROOT / (\"disrupt\" if is_disrupt[i] else \"clear\")\n",
        "    dst = dst_dir / f\"{int(shot)}.h5\"\n",
        "    link_or_copy(src, dst)\n",
        "\n",
        "CLEAR_FILE = COMPAT_ROOT / \"d3d_clear_ecei.final.txt\"\n",
        "DISRUPT_FILE = COMPAT_ROOT / \"d3d_disrupt_ecei.final.txt\"\n",
        "\n",
        "write_shot_file(CLEAR_FILE, np.where(~is_disrupt)[0])\n",
        "write_shot_file(DISRUPT_FILE, np.where(is_disrupt)[0])\n",
        "\n",
        "print(f\"Wrote {CLEAR_FILE} and {DISRUPT_FILE}\")\n",
        "print(f\"Data root: {DATA_ROOT}\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "17312c8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Offsets (per-shot baseline)\n",
        "\n",
        "Creates `offsets` datasets if missing. This matches `create_offsets.py` but scales the window to the decimated sampling rate.\n"
      ],
      "id": "5f6e75b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def compute_offset_indices(tstart, dt, window_ms, n_samples):\n",
        "    start_ms, end_ms = window_ms\n",
        "    start_idx = int(np.round((start_ms - tstart) / dt))\n",
        "    end_idx = int(np.round((end_ms - tstart) / dt))\n",
        "    start_idx = max(start_idx, 0)\n",
        "    end_idx = max(end_idx, start_idx + 1)\n",
        "    end_idx = min(end_idx, n_samples)\n",
        "    start_idx = min(start_idx, max(end_idx - 1, 0))\n",
        "    return start_idx, end_idx\n",
        "\n",
        "if WRITE_OFFSETS:\n",
        "    for i, shot in enumerate(shots):\n",
        "        path = DATA_ROOT / (\"disrupt\" if is_disrupt[i] else \"clear\") / f\"{int(shot)}.h5\"\n",
        "        with h5py.File(path, \"r+\") as f:\n",
        "            if \"offsets\" in f:\n",
        "                continue\n",
        "            n_samples = f[\"LFS\"].shape[-1]\n",
        "            start_idx, end_idx = compute_offset_indices(tstart_ms[i], dt_ms[i], OFFSET_WINDOW_MS, n_samples)\n",
        "            data = f[\"LFS\"][..., start_idx:end_idx]\n",
        "            offsets = data.mean(axis=-1).astype(\"float32\")\n",
        "            f.create_dataset(\"offsets\", data=offsets)\n",
        "\n",
        "    print(\"Offsets ensured.\")\n",
        "else:\n",
        "    print(\"WRITE_OFFSETS is False; skipping offsets creation.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "2a09d67b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization (per-channel mean/std)\n",
        "\n",
        "Writes `normalization.npz` to the data root in the exact format expected by the original loader.\n"
      ],
      "id": "ca42e044"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def update_running_stats(mean, M2, count, x):\n",
        "    n = x.shape[-1]\n",
        "    x_mean = x.mean(axis=-1)\n",
        "    x_var = x.var(axis=-1)\n",
        "    if count == 0:\n",
        "        return x_mean, x_var * n, n\n",
        "    delta = x_mean - mean\n",
        "    total = count + n\n",
        "    mean = mean + delta * n / total\n",
        "    M2 = M2 + x_var * n + delta * delta * count * n / total\n",
        "    return mean, M2, total\n",
        "\n",
        "if NORMALIZE:\n",
        "    paths = sorted((DATA_ROOT / \"disrupt\").glob(\"*.h5\")) + sorted((DATA_ROOT / \"clear\").glob(\"*.h5\"))\n",
        "    if NORM_MAX_SHOTS is not None:\n",
        "        paths = paths[: int(NORM_MAX_SHOTS)]\n",
        "\n",
        "    mean = None\n",
        "    M2 = None\n",
        "    count = 0\n",
        "    chunk_size = 20000\n",
        "\n",
        "    for path in paths:\n",
        "        with h5py.File(path, \"r\") as f:\n",
        "            offsets = f[\"offsets\"][...]\n",
        "            n_samples = f[\"LFS\"].shape[-1]\n",
        "            for start in range(0, n_samples, chunk_size):\n",
        "                end = min(start + chunk_size, n_samples)\n",
        "                x = f[\"LFS\"][..., start:end] - offsets[..., np.newaxis]\n",
        "                if mean is None:\n",
        "                    mean, M2, count = update_running_stats(0, 0, 0, x)\n",
        "                else:\n",
        "                    mean, M2, count = update_running_stats(mean, M2, count, x)\n",
        "\n",
        "    std = np.sqrt(M2 / max(count, 1))\n",
        "\n",
        "    norm_path = DATA_ROOT / \"normalization.npz\"\n",
        "    np.savez(norm_path, mean_all=mean, std_all=std, mean_flat=mean, std_flat=std)\n",
        "    print(f\"Saved {norm_path}\")\n",
        "else:\n",
        "    print(\"NORMALIZE is False; skipping normalization stats.\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4613d3b8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate the original dataloader\n",
        "\n",
        "We now use the untouched `EceiDataset` and `data_generator` from DisruptCNN.\n"
      ],
      "id": "4a9b49fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataset = EceiDataset(\n",
        "    root=str(DATA_ROOT) + \"/\",\n",
        "    clear_file=str(CLEAR_FILE),\n",
        "    disrupt_file=str(DISRUPT_FILE),\n",
        "    flattop_only=False,\n",
        "    Twarn=300,\n",
        "    label_balance=\"const\",\n",
        "    normalize=NORMALIZE,\n",
        "    data_step=DATA_STEP,\n",
        "    nsub=NSUB,\n",
        "    nrecept=NRECEPT,\n",
        ")\n",
        "\n",
        "dataset.train_val_test_split()\n",
        "train_loader, val_loader, test_loader = data_generator(\n",
        "    dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=4,\n",
        "    undersample=1.0,\n",
        ")\n",
        "\n",
        "x, y, idx, w = dataset[0]\n",
        "print(\"Sample shapes:\", x.shape, y.shape, w.shape)\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "7950c01d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training (original loop, single GPU)\n",
        "\n",
        "This mirrors the original `main.py` logic but runs in a single process.\n"
      ],
      "id": "b5511bff"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "args = SimpleNamespace(\n",
        "    input_channels=160,\n",
        "    n_classes=1,\n",
        "    dropout=0.1,\n",
        "    clip=0.3,\n",
        "    kernel_size=15,\n",
        "    dilation_size=10,\n",
        "    levels=4,\n",
        "    nhid=80,\n",
        "    nrecept=NRECEPT,\n",
        "    nsub=NSUB,\n",
        "    lr=0.5,\n",
        "    cuda=torch.cuda.is_available(),\n",
        "    thresholds=np.linspace(0.05, 0.95, 19),\n",
        "    distributed=False,\n",
        "    backend=\"gloo\",\n",
        "    rank=0,\n",
        "    plot=False,\n",
        ")\n",
        "args.tstart = time.time()\n",
        "args.iterations_valid = len(train_loader)\n",
        "args.iterations_warmup = 5 * len(train_loader)\n",
        "args.multiplier_warmup = 8\n",
        "\n",
        "model = disrupt_main.create_model(args)\n",
        "if args.cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, nesterov=True)\n",
        "\n",
        "lambda1 = (\n",
        "    lambda iteration: (1.0 - 1.0 / args.multiplier_warmup) / args.iterations_warmup * iteration\n",
        "    + 1.0 / args.multiplier_warmup\n",
        ")\n",
        "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n",
        "scheduler_plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5)\n",
        "\n",
        "EPOCHS = 1  # set higher for full training\n",
        "steps = 0\n",
        "total_loss = 0.0\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    if hasattr(train_loader.sampler, \"set_epoch\"):\n",
        "        train_loader.sampler.set_epoch(epoch)\n",
        "    for batch_idx, (data, target, global_index, weight) in enumerate(train_loader):\n",
        "        iteration = epoch * len(train_loader) + batch_idx\n",
        "        args.iteration = iteration\n",
        "\n",
        "        if iteration < args.iterations_warmup:\n",
        "            scheduler_warmup.step(iteration)\n",
        "        elif iteration > 0 and iteration % args.iterations_valid == 0:\n",
        "            scheduler_plateau.step(total_loss)\n",
        "\n",
        "        loss = disrupt_main.train_seq(data, target, weight, model, optimizer, args)\n",
        "        total_loss += float(loss)\n",
        "        steps += data.shape[0] * data.shape[-1]\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            lr_epoch = [group[\"lr\"] for group in optimizer.param_groups][0]\n",
        "            print(\n",
        "                f\"Epoch {epoch} batch {batch_idx}/{len(train_loader)} \"\n",
        "                f\"loss={total_loss / max(batch_idx + 1, 1):.6e} steps={steps} lr={lr_epoch:.2e}\"\n",
        "            )\n",
        "\n",
        "        if iteration > 0 and iteration % args.iterations_valid == 0:\n",
        "            valid_loss, valid_acc, valid_f1, TP, TN, FP, FN, threshold = disrupt_main.evaluate(\n",
        "                val_loader, model, args\n",
        "            )\n",
        "            best_acc = max(best_acc, valid_acc)\n",
        "            total_loss = 0.0\n",
        "\n",
        "print(f\"Best validation accuracy: {best_acc:.4f}\")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "33cee3b6"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}