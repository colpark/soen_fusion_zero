{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing & DataLoader Visualisation\n",
        "\n",
        "Walk through every preprocessing step of `ECEiTCNDataset` and inspect the\n",
        "resulting tensors, labels, and class balance."
      ],
      "id": "title"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from pathlib import Path\n",
        "\n",
        "from dataset_ecei_tcn import (\n",
        "    ECEiTCNDataset, create_loaders,\n",
        "    read_raw_shot, remove_offset, normalize_array, decimate,\n",
        ")\n",
        "\n",
        "ROOT = '/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt'\n",
        "CLEAR_ROOT = '/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear'\n",
        "FS   = 1_000_000   # 1 MHz"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "imports"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Metadata overview"
      ],
      "id": "sec-meta"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "meta = pd.read_csv(Path(ROOT) / 'meta.csv')\n",
        "print(f'Columns: {list(meta.columns)}')\n",
        "print(f'Total shots: {len(meta)}')\n",
        "print(meta.groupby('split').size())\n",
        "meta.head(10)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "meta"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# shot duration distribution\n",
        "dur_ms = meta['t_disruption']  # already in ms\n",
        "for sp in meta['split'].unique():\n",
        "    mask = meta['split'] == sp\n",
        "    axes[0].hist(dur_ms[mask], bins=30, alpha=0.6, label=sp)\n",
        "axes[0].set_xlabel('t_disruption (ms)')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_title('Shot duration distribution')\n",
        "axes[0].legend()\n",
        "\n",
        "# shots per split\n",
        "counts = meta.groupby('split').size()\n",
        "axes[1].bar(counts.index, counts.values, color=['steelblue', 'firebrick'][:len(counts)])\n",
        "axes[1].set_ylabel('# Shots')\n",
        "axes[1].set_title('Shots per split')\n",
        "for i, v in enumerate(counts.values):\n",
        "    axes[1].text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "meta-hist"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1b. Data composition diagnosis\n",
        "\n",
        "**Critical check**: Does the dataset contain non-disruptive (clean) shots?\n",
        "\n",
        "Churchill et al. (2019) trained on **2,747 shots (42% disruptive, 58% non-disruptive)**.\n",
        "If our dataset only contains disruptive shots, the model never sees truly healthy\n",
        "plasma — all \"clear\" labels come from the early parts of shots that eventually disrupt.\n",
        "This makes the classification task fundamentally harder and limits the achievable F1."
      ],
      "id": "b05bd82a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "#  DATA COMPOSITION DIAGNOSIS\n",
        "# ═══════════════════════════════════════════════════════════════════════\n",
        "\n",
        "print('=' * 70)\n",
        "print('  DATA COMPOSITION DIAGNOSIS')\n",
        "print('=' * 70)\n",
        "\n",
        "# ── 1. Check for a disruption-status column ──────────────────────────\n",
        "has_label_col = False\n",
        "for col in ['is_disruptive', 'disruptive', 'label', 'disrupted', 'type']:\n",
        "    if col in meta.columns:\n",
        "        has_label_col = True\n",
        "        print(f'\\n  Found label column: \"{col}\"')\n",
        "        print(f'  Value counts:')\n",
        "        print(meta[col].value_counts().to_string().replace('\\n', '\\n    '))\n",
        "        break\n",
        "\n",
        "if not has_label_col:\n",
        "    print('\\n  No explicit disruption-status column found.')\n",
        "    print(f'  Columns available: {list(meta.columns)}')\n",
        "\n",
        "# ── 2. Analyse t_disruption values ──────────────────────────────────\n",
        "print(f'\\n  t_disruption statistics (ms):')\n",
        "t_dis = meta['t_disruption']\n",
        "print(f'    count   = {t_dis.count()} / {len(meta)} (non-NaN)')\n",
        "print(f'    NaN     = {t_dis.isna().sum()}')\n",
        "print(f'    min     = {t_dis.min():.1f} ms')\n",
        "print(f'    max     = {t_dis.max():.1f} ms')\n",
        "print(f'    median  = {t_dis.median():.1f} ms')\n",
        "\n",
        "n_nan  = int(t_dis.isna().sum())\n",
        "n_inf  = int(np.isinf(t_dis.values.astype(float)).sum()) if n_nan == 0 else 0\n",
        "n_zero = int((t_dis == 0).sum())\n",
        "n_neg  = int((t_dis < 0).sum())\n",
        "n_valid = len(meta) - n_nan\n",
        "\n",
        "# Heuristic: shots with t_disruption=NaN, 0, negative, or very large (>20s)\n",
        "# are likely non-disruptive\n",
        "n_suspect_nondisrupt = n_nan + n_zero + n_neg\n",
        "very_long_threshold = 20_000  # 20 seconds — unusually long for a disruptive shot\n",
        "n_very_long = int((t_dis > very_long_threshold).sum())\n",
        "\n",
        "print(f'\\n  ── Classification heuristic ──')\n",
        "print(f'    NaN t_disruption (likely non-disruptive) : {n_nan}')\n",
        "print(f'    Zero t_disruption                        : {n_zero}')\n",
        "print(f'    Negative t_disruption                    : {n_neg}')\n",
        "print(f'    t_disruption > {very_long_threshold/1000:.0f}s (very long)          : {n_very_long}')\n",
        "print(f'    Remaining (clearly disruptive)            : {n_valid - n_zero - n_neg - n_very_long}')\n",
        "\n",
        "# ── 3. Check actual h5 file lengths vs t_disruption ─────────────────\n",
        "print(f'\\n  ── Spot-checking shot durations vs t_disruption ──')\n",
        "n_check = min(10, len(meta))\n",
        "sample_shots = meta.sample(n_check, random_state=42) if len(meta) > n_check else meta\n",
        "\n",
        "for _, row in sample_shots.iterrows():\n",
        "    shot = int(row['shot'])\n",
        "    t_dis_ms = row['t_disruption']\n",
        "    h5_path = Path(ROOT) / f'{shot}.h5'\n",
        "    if h5_path.exists():\n",
        "        with h5py.File(h5_path, 'r') as f:\n",
        "            T_total = f['LFS'].shape[-1]\n",
        "        dur_ms = T_total / FS * 1000\n",
        "        # If the shot is much longer than t_disruption, data extends past disruption\n",
        "        # If t_disruption ≈ shot length, it disrupted near the end (typical)\n",
        "        ratio = t_dis_ms / dur_ms * 100 if dur_ms > 0 else 0\n",
        "        flag = '' if 50 < ratio < 105 else '  ⚠️'\n",
        "        print(f'    shot {shot:>8d}: length={dur_ms:>8.1f} ms, '\n",
        "              f't_dis={t_dis_ms:>8.1f} ms, '\n",
        "              f'ratio={ratio:>5.1f}%{flag}')\n",
        "    else:\n",
        "        print(f'    shot {shot:>8d}: h5 file not found')\n",
        "\n",
        "# ── 4. Summary verdict ──────────────────────────────────────────────\n",
        "print()\n",
        "print('  ' + '─' * 66)\n",
        "all_disruptive = (n_nan == 0 and n_zero == 0 and n_neg == 0)\n",
        "\n",
        "if all_disruptive and not has_label_col:\n",
        "    pct_dis = 100.0\n",
        "    print(f'  ⚠️  ALL {len(meta)} shots appear DISRUPTIVE (t_disruption is valid for all)')\n",
        "    print(f'  ⚠️  No non-disruptive shots detected.')\n",
        "    print(f'  ⚠️  Churchill et al. used 42% disruptive / 58% non-disruptive.')\n",
        "    print(f'  ⚠️  This limits the \"clear\" class to early regions of disruptive shots,')\n",
        "    print(f'  ⚠️  making the classification task fundamentally harder.')\n",
        "    print(f'  ⚠️  RECOMMENDATION: Add non-disruptive shots to improve performance.')\n",
        "elif n_nan > 0 or has_label_col:\n",
        "    n_nondis = n_nan if not has_label_col else int((meta.get('is_disruptive', meta.get('disruptive', meta.get('label', pd.Series()))) == 0).sum())\n",
        "    n_dis_shots = len(meta) - n_nondis if n_nondis > 0 else n_valid\n",
        "    pct_dis = n_dis_shots / len(meta) * 100\n",
        "    pct_nondis = 100 - pct_dis\n",
        "    print(f'  ✓  Dataset composition:')\n",
        "    print(f'       Disruptive    : {n_dis_shots:>5d} ({pct_dis:.1f}%)')\n",
        "    print(f'       Non-disruptive: {n_nondis:>5d} ({pct_nondis:.1f}%)')\n",
        "    print(f'       (Churchill et al.: 42% / 58%)')\n",
        "    if pct_nondis < 30:\n",
        "        print(f'  ⚠️  Non-disruptive fraction is low ({pct_nondis:.0f}%). '\n",
        "              f'Consider adding more clean shots.')\n",
        "else:\n",
        "    print(f'  ✓  {len(meta)} shots with valid t_disruption.')\n",
        "    print(f'      Could not determine disruptive/non-disruptive split automatically.')\n",
        "\n",
        "print('  ' + '─' * 66)\n",
        "print('=' * 70)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "9db55728"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Visualise the composition ────────────────────────────────────────\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# (a) t_disruption distribution — look for bimodal / NaN gaps\n",
        "ax = axes[0]\n",
        "valid_tdis = t_dis.dropna()\n",
        "ax.hist(valid_tdis, bins=40, color='firebrick', alpha=0.6, edgecolor='k', linewidth=0.5)\n",
        "if n_nan > 0:\n",
        "    ax.axvline(0, color='gray', ls='--', lw=2)\n",
        "    ax.text(0.05, 0.95, f'{n_nan} NaN\\n(non-disruptive?)',\n",
        "            transform=ax.transAxes, va='top', fontsize=10,\n",
        "            bbox=dict(boxstyle='round', fc='yellow', alpha=0.7))\n",
        "ax.set_xlabel('t_disruption (ms)')\n",
        "ax.set_ylabel('# Shots')\n",
        "ax.set_title('Distribution of disruption times')\n",
        "ax.grid(True, alpha=0.2)\n",
        "\n",
        "# (b) Per-split composition\n",
        "ax = axes[1]\n",
        "splits = meta['split'].unique()\n",
        "x_pos = np.arange(len(splits))\n",
        "width = 0.35\n",
        "\n",
        "for i, sp in enumerate(splits):\n",
        "    sp_meta = meta[meta['split'] == sp]\n",
        "    n_total_sp = len(sp_meta)\n",
        "    n_nan_sp = int(sp_meta['t_disruption'].isna().sum())\n",
        "    n_dis_sp = n_total_sp - n_nan_sp\n",
        "\n",
        "    ax.bar(i - width/2, n_dis_sp, width, color='firebrick', alpha=0.7,\n",
        "           label='Disruptive' if i == 0 else '')\n",
        "    ax.bar(i + width/2, n_nan_sp, width, color='steelblue', alpha=0.7,\n",
        "           label='Non-disruptive' if i == 0 else '')\n",
        "    ax.text(i - width/2, n_dis_sp + 0.5, str(n_dis_sp), ha='center', fontsize=9)\n",
        "    ax.text(i + width/2, n_nan_sp + 0.5, str(n_nan_sp), ha='center', fontsize=9)\n",
        "\n",
        "ax.set_xticks(x_pos)\n",
        "ax.set_xticklabels(splits)\n",
        "ax.set_ylabel('# Shots')\n",
        "ax.set_title('Disruptive vs Non-disruptive per split')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.2, axis='y')\n",
        "\n",
        "# (c) Pie chart of overall composition\n",
        "ax = axes[2]\n",
        "n_dis_total = len(meta) - n_nan\n",
        "labels_pie = [f'Disruptive\\n({n_dis_total})', f'Non-disruptive\\n({n_nan})']\n",
        "sizes = [n_dis_total, max(n_nan, 0.001)]  # avoid zero-size\n",
        "colors_pie = ['firebrick', 'steelblue']\n",
        "explode = (0, 0.05)\n",
        "\n",
        "if n_nan > 0:\n",
        "    ax.pie(sizes, explode=explode, labels=labels_pie, colors=colors_pie,\n",
        "           autopct='%1.1f%%', startangle=90, textprops={'fontsize': 11})\n",
        "else:\n",
        "    ax.pie([1], labels=[f'ALL Disruptive\\n({len(meta)} shots)'],\n",
        "           colors=['firebrick'], autopct='',\n",
        "           startangle=90, textprops={'fontsize': 12, 'fontweight': 'bold'})\n",
        "    ax.text(0, -0.15, '⚠️ No non-disruptive shots', ha='center', fontsize=11,\n",
        "            color='darkorange', fontweight='bold')\n",
        "\n",
        "ax.set_title('Overall dataset composition')\n",
        "\n",
        "plt.suptitle('Data Composition Diagnosis', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "67bf6315"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Look for sibling directories that might contain non-disruptive data ──\n",
        "parent = Path(ROOT).parent\n",
        "print(f'Scanning {parent} for related data directories...\\n')\n",
        "\n",
        "found_dirs = []\n",
        "if parent.exists():\n",
        "    for p in sorted(parent.iterdir()):\n",
        "        if p.is_dir():\n",
        "            has_h5 = len(list(p.glob('*.h5'))[:1]) > 0\n",
        "            has_meta = (p / 'meta.csv').exists()\n",
        "            marker = ''\n",
        "            if has_meta:\n",
        "                marker += ' [meta.csv ✓]'\n",
        "                try:\n",
        "                    m = pd.read_csv(p / 'meta.csv')\n",
        "                    n_shots = len(m)\n",
        "                    n_nan_t = int(m['t_disruption'].isna().sum()) if 't_disruption' in m.columns else '?'\n",
        "                    marker += f'  {n_shots} shots, {n_nan_t} with NaN t_disruption'\n",
        "                except Exception:\n",
        "                    pass\n",
        "            if has_h5:\n",
        "                marker += ' [h5 ✓]'\n",
        "            print(f'  {p.name:40s}{marker}')\n",
        "            found_dirs.append(p)\n",
        "\n",
        "if not found_dirs:\n",
        "    print('  (no sibling directories found)')\n",
        "\n",
        "print(f'\\n  Current data root: {ROOT}')\n",
        "print(f'  If a \"nondsrpt\" or \"clean\" directory exists above, it may contain')\n",
        "print(f'  non-disruptive shots that should be merged into training.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "8feb7ba0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Raw signal inspection\n",
        "\n",
        "Load one full shot and show the raw (20, 8) ECEi grid."
      ],
      "id": "sec-raw"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "SHOT = meta['shot'].iloc[0]\n",
        "T_DIS_MS = meta['t_disruption'].iloc[0]\n",
        "T_DIS = int(T_DIS_MS * 1000)  # in samples\n",
        "\n",
        "raw = read_raw_shot(ROOT, SHOT)\n",
        "print(f'Shot {SHOT}: shape = {raw.shape}, t_disruption = {T_DIS_MS:.1f} ms')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "raw-shot"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot a few channels across the full shot\n",
        "time_ms = np.arange(raw.shape[-1]) / (FS / 1000)  # ms\n",
        "channels = [(0, 0), (10, 4), (19, 7)]  # (row, col) in the 20x8 grid\n",
        "\n",
        "fig, axes = plt.subplots(len(channels), 1, figsize=(16, 3 * len(channels)), sharex=True)\n",
        "for ax, (r, c) in zip(axes, channels):\n",
        "    ax.plot(time_ms, raw[r, c, :], linewidth=0.3, color='k')\n",
        "    ax.axvline(T_DIS_MS, color='red', linestyle='--', linewidth=1.5, label='t_disruption')\n",
        "    ax.set_ylabel(f'Ch ({r},{c})')\n",
        "    ax.legend(loc='upper right', fontsize=9)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "axes[-1].set_xlabel('Time (ms)')\n",
        "axes[0].set_title(f'Raw signal — shot {SHOT}')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "raw-timeseries"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Spatial snapshot at a single time instant (middle of the shot)\n",
        "t_snap = raw.shape[-1] // 2\n",
        "fig, ax = plt.subplots(figsize=(6, 8))\n",
        "im = ax.imshow(raw[:, :, t_snap], aspect='auto', cmap='RdBu_r')\n",
        "ax.set_xlabel('Radial channel')\n",
        "ax.set_ylabel('Vertical channel')\n",
        "ax.set_title(f'Spatial snapshot at t = {t_snap/FS*1e3:.1f} ms')\n",
        "plt.colorbar(im, ax=ax, label='Raw amplitude')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "raw-grid"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Step 1 — DC offset removal\n",
        "\n",
        "Mean of the first 40 ms (40 000 samples) is subtracted per channel (matches disruptcnn)."
      ],
      "id": "sec-offset"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "BASELINE_LEN = 40_000\n",
        "corrected, offset = remove_offset(raw, baseline_length=BASELINE_LEN)\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 6), sharex=True)\n",
        "\n",
        "r, c = 10, 4\n",
        "axes[0].plot(time_ms, raw[r, c, :], linewidth=0.3, color='k')\n",
        "axes[0].axhline(offset[r, c], color='orange', linestyle='--', label=f'Offset = {offset[r,c]:.1f}')\n",
        "axes[0].axvspan(0, BASELINE_LEN / FS * 1e3, alpha=0.15, color='orange', label='Baseline window')\n",
        "axes[0].set_title(f'Before offset removal — Ch ({r},{c})')\n",
        "axes[0].legend(loc='upper right')\n",
        "axes[0].grid(True, alpha=0.2)\n",
        "\n",
        "axes[1].plot(time_ms, corrected[r, c, :], linewidth=0.3, color='steelblue')\n",
        "axes[1].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
        "axes[1].set_title(f'After offset removal — Ch ({r},{c})')\n",
        "axes[1].set_xlabel('Time (ms)')\n",
        "axes[1].grid(True, alpha=0.2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "offset"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Offset heatmap across all channels\n",
        "fig, ax = plt.subplots(figsize=(6, 8))\n",
        "im = ax.imshow(offset, aspect='auto', cmap='coolwarm')\n",
        "ax.set_xlabel('Radial channel')\n",
        "ax.set_ylabel('Vertical channel')\n",
        "ax.set_title(f'DC offset per channel — shot {SHOT}')\n",
        "plt.colorbar(im, ax=ax, label='Offset value')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "offset-heatmap"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Step 2 — Temporal decimation (10×)\n",
        "\n",
        "Every 10th sample is kept → effective 100 kHz."
      ],
      "id": "sec-decimate"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "DATA_STEP = 10\n",
        "decimated = decimate(corrected, DATA_STEP)\n",
        "time_dec_ms = np.arange(decimated.shape[-1]) / (FS / DATA_STEP / 1000)\n",
        "\n",
        "print(f'Before decimation: {corrected.shape}  →  After: {decimated.shape}')\n",
        "\n",
        "# Zoom into a 10-ms window to see the effect\n",
        "t0, t1 = 500.0, 510.0  # ms\n",
        "mask_full = (time_ms >= t0) & (time_ms < t1)\n",
        "mask_dec  = (time_dec_ms >= t0) & (time_dec_ms < t1)\n",
        "\n",
        "r, c = 10, 4\n",
        "fig, ax = plt.subplots(figsize=(16, 3))\n",
        "ax.plot(time_ms[mask_full], corrected[r, c, mask_full],\n",
        "        linewidth=0.5, color='k', alpha=0.4, label='1 MHz (original)')\n",
        "ax.plot(time_dec_ms[mask_dec], decimated[r, c, mask_dec],\n",
        "        linewidth=1.2, color='steelblue', marker='.', markersize=3, label='100 kHz (decimated)')\n",
        "ax.set_xlabel('Time (ms)')\n",
        "ax.set_title(f'Decimation comparison — Ch ({r},{c}), {t0}–{t1} ms')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.2)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "decimate"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4a. Check for corrupted HDF5 files\n",
        "\n",
        "Scan raw and decimated directories and try to read a small chunk from each `.h5` file.\n",
        "Files that raise **OSError** (e.g. \"filter returned failure during read\") are listed as **corrupted** — often due to an unsupported compression filter on the current system. Use this to see whether corruption is in **raw** files only, **decimated** only, or both, so you can re-run preprocessing or fix the source files if needed."
      ],
      "id": "d6b3fb13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Paths (same as 4b) — run after cells above so ROOT, CLEAR_ROOT, meta exist\n",
        "DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated')\n",
        "CLEAR_DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear_decimated')\n",
        "READ_CHUNK = 1000  # samples to read per file (enough to trigger filter if present)\n",
        "\n",
        "def _check_one_file(root: Path, shot: int, key: str = 'LFS') -> tuple[int, str | None]:\n",
        "    \"\"\"Try to read a small chunk; return (shot, None) if OK else (shot, error_msg).\"\"\"\n",
        "    p = root / f'{shot}.h5'\n",
        "    if not p.exists():\n",
        "        return (shot, 'file not found')\n",
        "    try:\n",
        "        with h5py.File(p, 'r') as f:\n",
        "            if key not in f:\n",
        "                return (shot, f'missing key \"{key}\"')\n",
        "            d = f[key]\n",
        "            # Force a synchronous read (triggers filter); use minimal slice\n",
        "            n = min(READ_CHUNK, d.shape[-1])\n",
        "            _ = np.asarray(d[..., :n], dtype=np.float32)\n",
        "        return (shot, None)\n",
        "    except (OSError, IOError) as e:\n",
        "        return (shot, str(e))\n",
        "    except Exception as e:\n",
        "        return (shot, str(e))\n",
        "\n",
        "def scan_directory(label: str, root: Path, shots: list[int]) -> list[tuple[int, str]]:\n",
        "    \"\"\"Return list of (shot, error_msg) for corrupted files.\"\"\"\n",
        "    corrupted = []\n",
        "    for shot in shots:\n",
        "        _, err = _check_one_file(root, shot)\n",
        "        if err is not None:\n",
        "            corrupted.append((shot, err))\n",
        "    return corrupted\n",
        "\n",
        "# ── 1. Raw disruptive (ROOT) ─────────────────────────────────────────────\n",
        "dsrpt_shots = meta['shot'].values.astype(int).tolist()\n",
        "root_path = Path(ROOT)\n",
        "corrupt_raw_dsrpt = scan_directory('raw disruptive', root_path, dsrpt_shots)\n",
        "\n",
        "# ── 2. Decimated disruptive ──────────────────────────────────────────────\n",
        "corrupt_dec_dsrpt = []\n",
        "if DECIMATED_ROOT.exists():\n",
        "    dec_h5 = [f.stem for f in DECIMATED_ROOT.glob('*.h5') if f.stem.isdigit()]\n",
        "    dec_shots = [int(s) for s in dec_h5]\n",
        "    corrupt_dec_dsrpt = scan_directory('decimated disruptive', DECIMATED_ROOT, dec_shots)\n",
        "else:\n",
        "    print(f'  (decimated root not found: {DECIMATED_ROOT})')\n",
        "\n",
        "# ── 3. Raw clear ─────────────────────────────────────────────────────────\n",
        "corrupt_raw_clear = []\n",
        "clear_path = Path(CLEAR_ROOT)\n",
        "if clear_path.exists():\n",
        "    if (clear_path / 'meta.csv').exists():\n",
        "        clear_meta = pd.read_csv(clear_path / 'meta.csv')\n",
        "        clear_shots = clear_meta['shot'].values.astype(int).tolist()\n",
        "    else:\n",
        "        clear_shots = [int(f.stem) for f in clear_path.glob('*.h5') if f.stem.isdigit()]\n",
        "        if not clear_shots:\n",
        "            clear_shots = [int(f.stem) for f in clear_path.glob('*.h5')]\n",
        "    corrupt_raw_clear = scan_directory('raw clear', clear_path, clear_shots)\n",
        "else:\n",
        "    print(f'  (clear root not found: {CLEAR_ROOT})')\n",
        "\n",
        "# ── 4. Decimated clear ────────────────────────────────────────────────────\n",
        "corrupt_dec_clear = []\n",
        "if CLEAR_DECIMATED_ROOT.exists():\n",
        "    dec_clear_h5 = [f.stem for f in CLEAR_DECIMATED_ROOT.glob('*.h5') if f.stem.isdigit()]\n",
        "    dec_clear_shots = [int(s) for s in dec_clear_h5]\n",
        "    corrupt_dec_clear = scan_directory('decimated clear', CLEAR_DECIMATED_ROOT, dec_clear_shots)\n",
        "else:\n",
        "    print(f'  (clear_decimated not found: {CLEAR_DECIMATED_ROOT})')\n",
        "\n",
        "# ── Report ───────────────────────────────────────────────────────────────\n",
        "print('=' * 70)\n",
        "print('  CORRUPTED HDF5 FILES (read failed: filter / OSError)')\n",
        "print('=' * 70)\n",
        "\n",
        "def report(name: str, corrupted: list) -> None:\n",
        "    if not corrupted:\n",
        "        print(f'\\n  {name}: none')\n",
        "        return\n",
        "    print(f'\\n  {name}: {len(corrupted)} file(s)')\n",
        "    for shot, err in corrupted[:30]:\n",
        "        short = (err[:60] + '...') if len(err) > 60 else err\n",
        "        print(f'    shot {shot}: {short}')\n",
        "    if len(corrupted) > 30:\n",
        "        print(f'    ... and {len(corrupted) - 30} more')\n",
        "\n",
        "report('Raw disruptive (ROOT)', corrupt_raw_dsrpt)\n",
        "report('Decimated disruptive', corrupt_dec_dsrpt)\n",
        "report('Raw clear', corrupt_raw_clear)\n",
        "report('Decimated clear', corrupt_dec_clear)\n",
        "\n",
        "# Summary: raw-only vs decimated-only vs both\n",
        "print('\\n' + '─' * 70)\n",
        "shots_corrupt_raw = set(s for s, _ in corrupt_raw_dsrpt) | set(s for s, _ in corrupt_raw_clear)\n",
        "shots_corrupt_dec = set(s for s, _ in corrupt_dec_dsrpt) | set(s for s, _ in corrupt_dec_clear)\n",
        "both = shots_corrupt_raw & shots_corrupt_dec\n",
        "raw_only = shots_corrupt_raw - shots_corrupt_dec\n",
        "dec_only = shots_corrupt_dec - shots_corrupt_raw\n",
        "print('  Summary (by shot):')\n",
        "print(f'    Corrupted in BOTH raw and decimated : {len(both)} shot(s)')\n",
        "print(f'    Corrupted in RAW only               : {len(raw_only)} shot(s)')\n",
        "print(f'    Corrupted in DECIMATED only        : {len(dec_only)} shot(s)')\n",
        "if dec_only:\n",
        "    print(f'    → Decimated-only corrupted shots: {sorted(dec_only)[:20]}' + (' ...' if len(dec_only) > 20 else ''))\n",
        "print('=' * 70)"
      ],
      "id": "fa9fd8cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4b. Save decimated data to disk\n",
        "\n",
        "Run once to create offset-removed + 10×-decimated h5 files for both:\n",
        "- **Disruptive** → `dsrpt_decimated/` (meta.csv + `{shot}.h5`)\n",
        "- **Clear** → `clear_decimated/` (meta.csv with shot, split + `{shot}.h5`)\n",
        "\n",
        "Subsequent dataset loads from these directories skip offset removal and decimation entirely."
      ],
      "id": "8ad03a88"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "\n",
        "DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated')\n",
        "CLEAR_DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear_decimated')\n",
        "BASELINE_LEN = 40_000\n",
        "DATA_STEP    = 10\n",
        "N_WORKERS    = 10\n",
        "\n",
        "\n",
        "def _process_one_shot(args):\n",
        "    \"\"\"Worker: read one shot, offset-remove, decimate, save. Returns shot id or (shot, error) on failure.\"\"\"\n",
        "    import numpy as np\n",
        "    import h5py\n",
        "    from pathlib import Path\n",
        "    from dataset_ecei_tcn import read_raw_shot, remove_offset, decimate\n",
        "    shot, root_str, out_dir_str, baseline_len, data_step, fs = args\n",
        "    root = Path(root_str)\n",
        "    out_dir = Path(out_dir_str)\n",
        "    try:\n",
        "        raw = read_raw_shot(root, shot)\n",
        "        corrected, _ = remove_offset(raw, baseline_length=baseline_len)\n",
        "        dec = decimate(corrected, data_step)\n",
        "        out_path = out_dir / f'{shot}.h5'\n",
        "        with h5py.File(out_path, 'w') as f:\n",
        "            f.create_dataset('LFS', data=dec.astype(np.float32),\n",
        "                             compression='gzip', compression_opts=4)\n",
        "            f.attrs['data_step']       = data_step\n",
        "            f.attrs['baseline_length'] = baseline_len\n",
        "            f.attrs['source_fs_hz']    = fs\n",
        "            f.attrs['effective_fs_hz'] = fs // data_step\n",
        "        return shot\n",
        "    except Exception as e:\n",
        "        return (shot, str(e))\n",
        "\n",
        "\n",
        "# ── Disruptive shots: save decimated to dsrpt_decimated ───────────────\n",
        "if DECIMATED_ROOT.exists() and (DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    n_existing = len(list(DECIMATED_ROOT.glob('*.h5')))\n",
        "    print(f'Decimated data already exists at {DECIMATED_ROOT}  ({n_existing} h5 files)')\n",
        "else:\n",
        "    DECIMATED_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "    shutil.copy(Path(ROOT) / 'meta.csv', DECIMATED_ROOT / 'meta.csv')\n",
        "\n",
        "    dsrpt_tasks = [\n",
        "        (int(row['shot']), str(ROOT), str(DECIMATED_ROOT), BASELINE_LEN, DATA_STEP, FS)\n",
        "        for _, row in meta.iterrows()\n",
        "    ]\n",
        "    with Pool(N_WORKERS) as pool:\n",
        "        list(tqdm(\n",
        "            pool.imap_unordered(_process_one_shot, dsrpt_tasks),\n",
        "            total=len(dsrpt_tasks),\n",
        "            desc='Saving decimated (dsrpt)',\n",
        "        ))\n",
        "    n_saved = len(list(DECIMATED_ROOT.glob('*.h5')))\n",
        "    print(f'Saved {n_saved} decimated shots to {DECIMATED_ROOT}')\n",
        "\n",
        "# ── Clear (non-disruptive) shots: save decimated to clear_decimated ────\n",
        "clear_root = Path(CLEAR_ROOT)\n",
        "if clear_root.exists():\n",
        "    clear_meta_path = clear_root / 'meta.csv'\n",
        "    if clear_meta_path.exists():\n",
        "        clear_meta = pd.read_csv(clear_meta_path)\n",
        "        clear_shots = clear_meta['shot'].values.astype(int)\n",
        "        clear_splits = clear_meta['split'].values.astype(str) if 'split' in clear_meta.columns else np.array(['train'] * len(clear_shots))\n",
        "    else:\n",
        "        h5_files = list(clear_root.glob('*.h5'))\n",
        "        clear_shots = np.array([int(f.stem) for f in h5_files if f.stem.isdigit()])\n",
        "        if len(clear_shots) == 0:\n",
        "            clear_shots = np.array([int(f.stem) for f in h5_files])\n",
        "        n_train = int(len(clear_shots) * 0.8)\n",
        "        clear_splits = np.array(['train'] * n_train + ['test'] * (len(clear_shots) - n_train), dtype=object)\n",
        "\n",
        "    if CLEAR_DECIMATED_ROOT.exists() and (CLEAR_DECIMATED_ROOT / 'meta.csv').exists():\n",
        "        n_clear = len(list(CLEAR_DECIMATED_ROOT.glob('*.h5')))\n",
        "        print(f'Clear decimated already exists at {CLEAR_DECIMATED_ROOT}  ({n_clear} h5 files)')\n",
        "    else:\n",
        "        CLEAR_DECIMATED_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "        clear_meta_out = pd.DataFrame({'shot': clear_shots, 'split': clear_splits})\n",
        "        clear_meta_out.to_csv(CLEAR_DECIMATED_ROOT / 'meta.csv', index=False)\n",
        "\n",
        "        clear_tasks = [\n",
        "            (int(shot), str(clear_root), str(CLEAR_DECIMATED_ROOT), BASELINE_LEN, DATA_STEP, FS)\n",
        "            for shot in clear_shots\n",
        "            if (clear_root / f'{shot}.h5').exists()\n",
        "        ]\n",
        "        with Pool(N_WORKERS) as pool:\n",
        "            list(tqdm(\n",
        "                pool.imap_unordered(_process_one_shot, clear_tasks),\n",
        "                total=len(clear_tasks),\n",
        "                desc='Saving decimated (clear)',\n",
        "            ))\n",
        "        n_saved = len(list(CLEAR_DECIMATED_ROOT.glob('*.h5')))\n",
        "        print(f'Saved {n_saved} decimated clear shots to {CLEAR_DECIMATED_ROOT}')\n",
        "else:\n",
        "    print(f'Clear root not found ({clear_root}); skipping clear_decimated.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5a571cdc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Step 3 — Z-score normalisation\n",
        "\n",
        "Per-channel mean/std computed from training shots; then applied to all splits."
      ],
      "id": "sec-norm"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build the dataset — uses decimated data when available for speed\n",
        "DECIMATED_ROOT = '/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated'\n",
        "CLEAR_DECIMATED_ROOT = '/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear_decimated'\n",
        "\n",
        "ds = ECEiTCNDataset(\n",
        "    root                 = ROOT,\n",
        "    decimated_root       = DECIMATED_ROOT,\n",
        "    clear_root           = CLEAR_ROOT,\n",
        "    clear_decimated_root = CLEAR_DECIMATED_ROOT,\n",
        "    Twarn                = 300_000,\n",
        "    baseline_length      = 40_000,\n",
        "    data_step            = 10,\n",
        "    nsub                 = 781_250,    # ~781 ms (matches disruptcnn)\n",
        "    stride               = 481_090,    # (nsub/step - nrecept + 1) * step\n",
        "    normalize            = True,\n",
        ")\n",
        "ds.summary()\n",
        "\n",
        "# Compute normalisation from train split (cache to disk)\n",
        "NORM_STATS_PATH = Path('norm_stats.npz')\n",
        "\n",
        "if NORM_STATS_PATH.exists():\n",
        "    ds.load_norm_stats(str(NORM_STATS_PATH))\n",
        "    norm_mean, norm_std = ds.norm_mean, ds.norm_std\n",
        "    print(f'Loaded cached norm stats from {NORM_STATS_PATH}')\n",
        "else:\n",
        "    norm_mean, norm_std = ds.compute_norm_stats(split='train', max_shots=100)\n",
        "    ds.save_norm_stats(str(NORM_STATS_PATH))\n",
        "    print(f'Computed and saved norm stats to {NORM_STATS_PATH}')\n",
        "\n",
        "print(f'  mean range: [{norm_mean.min():.4f}, {norm_mean.max():.4f}]')\n",
        "print(f'  std  range: [{norm_std.min():.4f}, {norm_std.max():.4f}]')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "norm-build"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
        "\n",
        "im0 = axes[0].imshow(norm_mean, aspect='auto', cmap='coolwarm')\n",
        "axes[0].set_title('Per-channel mean (train)')\n",
        "axes[0].set_xlabel('Radial')\n",
        "axes[0].set_ylabel('Vertical')\n",
        "plt.colorbar(im0, ax=axes[0])\n",
        "\n",
        "im1 = axes[1].imshow(norm_std, aspect='auto', cmap='viridis')\n",
        "axes[1].set_title('Per-channel std (train)')\n",
        "axes[1].set_xlabel('Radial')\n",
        "axes[1].set_ylabel('Vertical')\n",
        "plt.colorbar(im1, ax=axes[1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "norm-heatmaps"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Apply normalisation to the example shot\n",
        "normalised = normalize_array(decimated, norm_mean, norm_std)\n",
        "\n",
        "r, c = 10, 4\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 7))\n",
        "\n",
        "# Time series comparison\n",
        "axes[0, 0].plot(time_dec_ms, decimated[r, c, :], linewidth=0.3, color='k')\n",
        "axes[0, 0].set_title(f'Before normalisation — Ch ({r},{c})')\n",
        "axes[0, 0].set_ylabel('Amplitude')\n",
        "axes[0, 0].grid(True, alpha=0.2)\n",
        "\n",
        "axes[0, 1].plot(time_dec_ms, normalised[r, c, :], linewidth=0.3, color='steelblue')\n",
        "axes[0, 1].set_title(f'After normalisation — Ch ({r},{c})')\n",
        "axes[0, 1].set_ylabel('z-score')\n",
        "axes[0, 1].grid(True, alpha=0.2)\n",
        "\n",
        "# Histograms\n",
        "axes[1, 0].hist(decimated[r, c, :], bins=100, color='gray', alpha=0.7)\n",
        "axes[1, 0].set_xlabel('Amplitude')\n",
        "axes[1, 0].set_title('Value distribution (before)')\n",
        "\n",
        "axes[1, 1].hist(normalised[r, c, :], bins=100, color='steelblue', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('z-score')\n",
        "axes[1, 1].set_title('Value distribution (after)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "norm-before-after"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Distribution across ALL channels before vs. after normalisation\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "# Flatten all channels, subsample time for speed\n",
        "step = 100\n",
        "vals_before = decimated[:, :, ::step].flatten()\n",
        "vals_after  = normalised[:, :, ::step].flatten()\n",
        "\n",
        "axes[0].hist(vals_before, bins=200, color='gray', alpha=0.7)\n",
        "axes[0].set_title('All-channel amplitude (before norm)')\n",
        "axes[0].set_xlabel('Amplitude')\n",
        "axes[0].set_xlim(np.percentile(vals_before, [0.5, 99.5]))\n",
        "\n",
        "axes[1].hist(vals_after, bins=200, color='steelblue', alpha=0.7)\n",
        "axes[1].set_title('All-channel z-score (after norm)')\n",
        "axes[1].set_xlabel('z-score')\n",
        "axes[1].set_xlim(np.percentile(vals_after, [0.5, 99.5]))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "norm-all-channels"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Per-timestep labels & weights\n",
        "\n",
        "The label is 0 (clear) until `Twarn` before disruption, then 1 (disruptive)."
      ],
      "id": "sec-label"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Show the label on the full shot time axis\n",
        "Twarn_ms = 300  # ms\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 5), sharex=True)\n",
        "\n",
        "r, c = 10, 4\n",
        "axes[0].plot(time_dec_ms, normalised[r, c, :], linewidth=0.3, color='k')\n",
        "axes[0].set_ylabel('z-score')\n",
        "axes[0].set_title(f'Normalised signal — Ch ({r},{c}), shot {SHOT}')\n",
        "axes[0].axvline(T_DIS_MS, color='red', ls='--', lw=1.5, label='t_disruption')\n",
        "axes[0].axvline(T_DIS_MS - Twarn_ms, color='orange', ls='--', lw=1.5, label=f't_dis − Twarn ({Twarn_ms} ms)')\n",
        "axes[0].legend(loc='upper right')\n",
        "axes[0].grid(True, alpha=0.2)\n",
        "\n",
        "# Per-timestep label\n",
        "label_full = np.zeros(len(time_dec_ms), dtype=np.float32)\n",
        "d_idx_dec = int((T_DIS - 300_000) / DATA_STEP)\n",
        "d_idx_dec = max(0, min(d_idx_dec, len(label_full)))\n",
        "label_full[d_idx_dec:] = 1.0\n",
        "\n",
        "axes[1].fill_between(time_dec_ms, 0, label_full, color='firebrick', alpha=0.4, label='Disruptive (1)')\n",
        "axes[1].fill_between(time_dec_ms, 0, 1 - label_full, color='steelblue', alpha=0.2, label='Clear (0)')\n",
        "axes[1].set_ylabel('Label')\n",
        "axes[1].set_xlabel('Time (ms)')\n",
        "axes[1].set_title('Per-timestep binary label (Twarn = 300 ms)')\n",
        "axes[1].set_ylim(-0.05, 1.15)\n",
        "axes[1].legend(loc='upper right')\n",
        "axes[1].grid(True, alpha=0.2)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "label-full-shot"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Subsequence tiling\n",
        "\n",
        "Each shot is split into fixed-length windows (500 ms). Windows that straddle the\n",
        "disruption boundary have a label transition inside."
      ],
      "id": "sec-subseq"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find all subsequences for this shot\n",
        "shot_meta_idx = np.where(ds.shots == SHOT)[0][0]\n",
        "seq_mask = ds.seq_shot_idx == shot_meta_idx\n",
        "seq_starts = ds.seq_start[seq_mask]\n",
        "seq_stops  = ds.seq_stop[seq_mask]\n",
        "seq_disrupt = ds.seq_disrupt_local[seq_mask]\n",
        "\n",
        "print(f'Shot {SHOT}: {seq_mask.sum()} subsequences')\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 3))\n",
        "for i, (a, b, d) in enumerate(zip(seq_starts, seq_stops, seq_disrupt)):\n",
        "    color = 'firebrick' if d >= 0 else 'steelblue'\n",
        "    ax.barh(0, (b - a) / FS * 1e3, left=a / FS * 1e3, height=0.6,\n",
        "            color=color, alpha=0.5, edgecolor='k', linewidth=0.5)\n",
        "ax.axvline(T_DIS_MS, color='red', ls='--', lw=2, label='t_disruption')\n",
        "ax.axvline(T_DIS_MS - Twarn_ms, color='orange', ls='--', lw=1.5, label='label boundary')\n",
        "ax.set_xlabel('Time (ms)')\n",
        "ax.set_title(f'Subsequence windows for shot {SHOT} (red = contains disruption label)')\n",
        "ax.set_yticks([])\n",
        "ax.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "subseq-viz"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. DataLoader end-to-end test\n",
        "\n",
        "Pull a batch from the train loader and verify shapes + label distribution."
      ],
      "id": "sec-dataloader"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loaders = create_loaders(ds, batch_size=4, num_workers=0)\n",
        "print('Splits available:', list(loaders.keys()))\n",
        "\n",
        "split_name = 'train' if 'train' in loaders else list(loaders.keys())[0]\n",
        "batch = next(iter(loaders[split_name]))\n",
        "X_b, target_b, weight_b = batch\n",
        "\n",
        "print(f'\\nBatch from \"{split_name}\":')\n",
        "print(f'  X      : {X_b.shape}  dtype={X_b.dtype}')\n",
        "print(f'  target : {target_b.shape}  dtype={target_b.dtype}')\n",
        "print(f'  weight : {weight_b.shape}  dtype={weight_b.dtype}')\n",
        "print(f'  label frac per sample: {[f\"{t.mean():.2f}\" for t in target_b]}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "loader-test"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualise one sample from the batch\n",
        "idx = 0\n",
        "x_np = X_b[idx].numpy()          # (20, 8, T_sub)\n",
        "t_np = target_b[idx].numpy()     # (T_sub,)\n",
        "w_np = weight_b[idx].numpy()     # (T_sub,)\n",
        "T_sub = x_np.shape[-1]\n",
        "t_ax = np.arange(T_sub) / (FS / ds.data_step / 1000)  # ms\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(16, 10), sharex=True,\n",
        "                          gridspec_kw={'height_ratios': [3, 3, 1, 1]})\n",
        "\n",
        "# Signal (one channel)\n",
        "axes[0].plot(t_ax, x_np[10, 4, :], linewidth=0.4, color='k')\n",
        "axes[0].set_ylabel('z-score')\n",
        "axes[0].set_title('Preprocessed signal — Ch (10, 4)')\n",
        "axes[0].grid(True, alpha=0.2)\n",
        "\n",
        "# All 160 channels as heatmap\n",
        "flat = x_np.reshape(160, -1)\n",
        "axes[1].imshow(flat, aspect='auto', cmap='RdBu_r',\n",
        "               vmin=np.percentile(flat, 1), vmax=np.percentile(flat, 99),\n",
        "               extent=[t_ax[0], t_ax[-1], 159, 0])\n",
        "axes[1].set_ylabel('Channel')\n",
        "axes[1].set_title('All 160 channels (20×8 flattened)')\n",
        "\n",
        "# Per-timestep label\n",
        "axes[2].fill_between(t_ax, 0, t_np, color='firebrick', alpha=0.5)\n",
        "axes[2].set_ylabel('Label')\n",
        "axes[2].set_ylim(-0.05, 1.15)\n",
        "\n",
        "# Weight\n",
        "axes[3].plot(t_ax, w_np, color='darkorange', linewidth=1)\n",
        "axes[3].set_ylabel('Weight')\n",
        "axes[3].set_xlabel('Time (ms) within subsequence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "loader-sample-viz"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Class balance statistics"
      ],
      "id": "sec-balance"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
        "\n",
        "for i, sp in enumerate(np.unique(ds.splits)):\n",
        "    idx = ds.get_split_indices(sp)\n",
        "    n_dis  = int(ds.seq_has_disrupt[idx].sum())\n",
        "    n_clr  = len(idx) - n_dis\n",
        "\n",
        "    axes[0].bar(i * 3, n_clr, color='steelblue', width=0.8, label='Clear' if i == 0 else '')\n",
        "    axes[0].bar(i * 3 + 1, n_dis, color='firebrick', width=0.8, label='Disruptive' if i == 0 else '')\n",
        "    axes[0].text(i * 3, n_clr + 1, str(n_clr), ha='center', fontsize=9)\n",
        "    axes[0].text(i * 3 + 1, n_dis + 1, str(n_dis), ha='center', fontsize=9)\n",
        "\n",
        "axes[0].set_xticks([i * 3 + 0.5 for i in range(len(np.unique(ds.splits)))])\n",
        "axes[0].set_xticklabels(np.unique(ds.splits))\n",
        "axes[0].set_ylabel('# Subsequences')\n",
        "axes[0].set_title('Subsequences per split (clear vs disruptive)')\n",
        "axes[0].legend()\n",
        "\n",
        "# Fraction of disruptive time steps per subsequence\n",
        "T_sub = ds.nsub // ds.data_step\n",
        "fracs = []\n",
        "for dl in ds.seq_disrupt_local:\n",
        "    if dl < 0:\n",
        "        fracs.append(0.0)\n",
        "    else:\n",
        "        d = min(dl // ds.data_step, T_sub)\n",
        "        fracs.append((T_sub - d) / T_sub)\n",
        "fracs = np.array(fracs)\n",
        "\n",
        "axes[1].hist(fracs, bins=50, color='gray', alpha=0.7, edgecolor='k')\n",
        "axes[1].set_xlabel('Fraction of disruptive time steps')\n",
        "axes[1].set_ylabel('# Subsequences')\n",
        "axes[1].set_title('Distribution of disruptive fraction per subsequence')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'\\nClass weights: pos_weight = {ds.pos_weight:.3f}, neg_weight = {ds.neg_weight:.3f}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "class-balance"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}