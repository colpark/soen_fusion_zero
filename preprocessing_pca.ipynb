{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36531d06",
      "metadata": {},
      "source": [
        "# PCA from decimated data — **global** PCA (recommended)\n",
        "\n",
        "**Why global PCA:** A per-sequence or per–time-index PCA gives different coordinate systems per sample (PC1 in shot A ≠ PC1 in shot B). That breaks learnability: the NN sees incompatible features. A **global** PCA gives a consistent embedding: every time point in every sequence is projected onto the same axes.\n",
        "\n",
        "**Pipeline:**\n",
        "- Split train/val/test at the **sequence (shot) level**.\n",
        "- Fit PCA **only on training time points**: stream through training shots, accumulate mean μ ∈ ℝ¹⁶⁰ and covariance C ∈ ℝ¹⁶⁰×¹⁶⁰ with a single-pass online algorithm (Welford). Memory stays O(160²). Eigendecompose C and take top K eigenvectors W_K.\n",
        "- Transform **every** sequence: z_t = (x_t − μ) W_K → each shot becomes Z^(n) ∈ ℝ^(T_n × K). Save as `*_pca` (dsrpt_decimated_pca, clear_decimated_pca)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630fea1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from pathlib import Path\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths — decimated inputs (disruptive + clear)\n",
        "DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated')\n",
        "CLEAR_DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear_decimated')\n",
        "# Outputs: N_PCA_OUT in {1, 4, 8, 16} → dirs dsrpt_decimated_pca{N}, clear_decimated_pca{N}\n",
        "N_PCA_OUT = 1   # number of PCs to write (training default is 1)\n",
        "OUT_DSRPT_PCA = DECIMATED_ROOT.parent / f'dsrpt_decimated_pca{N_PCA_OUT}'\n",
        "OUT_CLEAR_PCA = DECIMATED_ROOT.parent / f'clear_decimated_pca{N_PCA_OUT}'\n",
        "\n",
        "N_COMPONENTS = 16   # fit top K (global PCA); must be >= N_PCA_OUT\n",
        "CHANNELS = 20 * 8   # 160"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb35c919",
      "metadata": {},
      "source": [
        "## 1. List shots, lengths, and splits (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6abcdd48",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_shot_lengths(root: Path) -> dict[int, int]:\n",
        "    \"\"\"Return {shot_id: T} for each .h5 in root.\"\"\"\n",
        "    out = {}\n",
        "    for p in root.glob('*.h5'):\n",
        "        if not p.stem.isdigit():\n",
        "            continue\n",
        "        shot = int(p.stem)\n",
        "        with h5py.File(p, 'r') as f:\n",
        "            T = f['LFS'].shape[-1]\n",
        "        out[shot] = T\n",
        "    return out\n",
        "\n",
        "def get_splits(root: Path, lengths: dict) -> dict[int, str]:\n",
        "    \"\"\"Return {shot_id: 'train'|'val'|'test'} from meta.csv or 80/20 default.\"\"\"\n",
        "    if (root / 'meta.csv').exists():\n",
        "        meta = pd.read_csv(root / 'meta.csv')\n",
        "        if 'split' in meta.columns:\n",
        "            return dict(zip(meta['shot'].astype(int), meta['split'].astype(str)))\n",
        "    # Default: 80% train, 20% test\n",
        "    shots = list(lengths.keys())\n",
        "    n = len(shots)\n",
        "    n_train = int(0.8 * n)\n",
        "    out = {s: 'train' for s in shots[:n_train]}\n",
        "    for s in shots[n_train:]:\n",
        "        out[s] = 'test'\n",
        "    return out\n",
        "\n",
        "dsrpt_lengths = get_shot_lengths(DECIMATED_ROOT) if DECIMATED_ROOT.exists() else {}\n",
        "clear_lengths = get_shot_lengths(CLEAR_DECIMATED_ROOT) if CLEAR_DECIMATED_ROOT.exists() else {}\n",
        "dsrpt_splits = get_splits(DECIMATED_ROOT, dsrpt_lengths) if dsrpt_lengths else {}\n",
        "clear_splits = get_splits(CLEAR_DECIMATED_ROOT, clear_lengths) if clear_lengths else {}\n",
        "\n",
        "train_shots_d = [s for s, sp in dsrpt_splits.items() if sp == 'train']\n",
        "train_shots_c = [s for s, sp in clear_splits.items() if sp == 'train']\n",
        "print(f'Disruptive: {len(dsrpt_lengths)} shots  (train={len(train_shots_d)})')\n",
        "print(f'Clear:      {len(clear_lengths)} shots  (train={len(train_shots_c)})')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f02f502",
      "metadata": {},
      "source": [
        "## 2. Streaming mean and covariance (training time points only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e450420d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Welford in batches: merge (n, mean, M2) with batch (B, 160). Memory O(160²) + O(B*160).\n",
        "BATCH_SIZE = 10_000   # time points per batch (vectorized update)\n",
        "\n",
        "def welford_merge(n, mean, M2, batch: np.ndarray):\n",
        "    \"\"\"Merge batch (B, 160) into running stats. Returns (n_new, mean_new, M2_new).\"\"\"\n",
        "    B = len(batch)\n",
        "    if B == 0:\n",
        "        return n, mean, M2\n",
        "    batch_mean = batch.mean(axis=0)\n",
        "    batch_M2 = (batch - batch_mean).T @ (batch - batch_mean)\n",
        "    n_new = n + B\n",
        "    mean_new = (n * mean + B * batch_mean) / n_new\n",
        "    delta = mean - batch_mean\n",
        "    M2_new = M2 + batch_M2 + (n * B / n_new) * np.outer(delta, delta)\n",
        "    return n_new, mean_new, M2_new\n",
        "\n",
        "def load_shot_flat(root: Path, shot: int, T: int) -> np.ndarray:\n",
        "    \"\"\"Load one shot as (T, 160) float64.\"\"\"\n",
        "    with h5py.File(root / f'{shot}.h5', 'r') as f:\n",
        "        data = np.asarray(f['LFS'][:], dtype=np.float64)  # (20, 8, T)\n",
        "    return data.reshape(CHANNELS, -1).T  # (T, 160)\n",
        "\n",
        "def transform_shot_global(root: Path, shot: int, T: int, mu: np.ndarray, W_K: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"z_t = (x_t - μ) W_K. Return (K, T) for LFS layout.\"\"\"\n",
        "    X = load_shot_flat(root, shot, T)  # (T, 160)\n",
        "    Z = (X - mu) @ W_K  # (T, K)\n",
        "    return Z.T.astype(np.float32)  # (K, T)\n",
        "\n",
        "# Training shots only: (root, shot, T)\n",
        "train_tuples = [(DECIMATED_ROOT, s, dsrpt_lengths[s]) for s in train_shots_d]\n",
        "train_tuples += [(CLEAR_DECIMATED_ROOT, s, clear_lengths[s]) for s in train_shots_c]\n",
        "\n",
        "n_total = 0\n",
        "mean = np.zeros(CHANNELS, dtype=np.float64)\n",
        "M2 = np.zeros((CHANNELS, CHANNELS), dtype=np.float64)\n",
        "\n",
        "for root, shot, T in tqdm(train_tuples, desc='Streaming μ and M2 (train only)'):\n",
        "    X = load_shot_flat(root, shot, T)  # (T, 160)\n",
        "    for start in range(0, len(X), BATCH_SIZE):\n",
        "        batch = X[start:start + BATCH_SIZE]\n",
        "        n_total, mean, M2 = welford_merge(n_total, mean, M2, batch)\n",
        "\n",
        "print(f'Training time points: {n_total}')\n",
        "if n_total < 2:\n",
        "    raise ValueError('Need at least 2 training time points to fit PCA')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825cfeac",
      "metadata": {},
      "source": [
        "## 3. Eigendecompose C and take top K eigenvectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5968d86a",
      "metadata": {},
      "outputs": [],
      "source": [
        "C = M2 / (n_total - 1)\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(C)\n",
        "idx = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[idx]\n",
        "eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "K = min(N_COMPONENTS, CHANNELS, len(eigenvalues))\n",
        "mu = mean.copy()\n",
        "W_K = eigenvectors[:, :K].astype(np.float32)  # (160, K)\n",
        "\n",
        "var_explained = eigenvalues / eigenvalues.sum()\n",
        "print(f'Global PCA: μ ∈ R^{CHANNELS}, W_K ∈ R^{CHANNELS}×{K}')\n",
        "print(f'Cumulative variance (top {K}): {var_explained[:K].sum():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84126840",
      "metadata": {},
      "source": [
        "## 3b. How many components for ≥99% variance? (from global C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35148eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "target_variance = 0.99\n",
        "cumvar = np.cumsum(var_explained)\n",
        "k_99 = int(np.searchsorted(cumvar, target_variance)) + 1\n",
        "k_99 = min(k_99, len(cumvar))\n",
        "print(f'Components needed for ≥{target_variance*100:.0f}% variance (global PCA): {k_99}')\n",
        "print(f'  → Consider N_COMPONENTS >= {k_99} to retain ≥99% of the information.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff93473",
      "metadata": {},
      "source": [
        "## 3c. Save PCA result (top 16 PCs; write 1/4/8/16 via N_PCA_OUT in 3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4ea640",
      "metadata": {},
      "outputs": [],
      "source": [
        "N_PC_SAVE = 16   # save top 16 so we can write _pca1, _pca4, _pca8, _pca16\n",
        "W_full = eigenvectors[:, :N_PC_SAVE].astype(np.float32)  # (160, 16)\n",
        "pca_save_path = Path('pca_global_top16.npz')\n",
        "np.savez(pca_save_path, mu=mu.astype(np.float32), W_K=W_full, var_explained=var_explained[:N_PC_SAVE].astype(np.float32))\n",
        "\n",
        "info_saved = var_explained[:N_PC_SAVE].sum()\n",
        "print(f'Saved top {N_PC_SAVE} PCs to {pca_save_path}')\n",
        "print(f'Variance (information) retained: {info_saved*100:.2f}%')\n",
        "print(f'Per-component: {var_explained[:N_PC_SAVE]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1325f5",
      "metadata": {},
      "source": [
        "## 3d. Save transformed data (top N_PCA_OUT PCs → *_pca1, *_pca4, *_pca8, *_pca16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb79785",
      "metadata": {},
      "outputs": [],
      "source": [
        "W_save = W_full[:, :N_PCA_OUT]\n",
        "\n",
        "OUT_DSRPT_PCA.mkdir(parents=True, exist_ok=True)\n",
        "if (DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    import shutil\n",
        "    shutil.copy(DECIMATED_ROOT / 'meta.csv', OUT_DSRPT_PCA / 'meta.csv')\n",
        "for shot, T in tqdm(list(dsrpt_lengths.items()), desc=f'Save disruptive {N_PCA_OUT}-PC'):\n",
        "    data = transform_shot_global(DECIMATED_ROOT, shot, T, mu, W_save)\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(dsrpt_lengths)} shots to {OUT_DSRPT_PCA} (shape {N_PCA_OUT}×T per shot)')\n",
        "\n",
        "OUT_CLEAR_PCA.mkdir(parents=True, exist_ok=True)\n",
        "if CLEAR_DECIMATED_ROOT.exists() and (CLEAR_DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    shutil.copy(CLEAR_DECIMATED_ROOT / 'meta.csv', OUT_CLEAR_PCA / 'meta.csv')\n",
        "for shot, T in tqdm(list(clear_lengths.items()), desc=f'Save clear {N_PCA_OUT}-PC'):\n",
        "    data = transform_shot_global(CLEAR_DECIMATED_ROOT, shot, T, mu, W_save)\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(clear_lengths)} shots to {OUT_CLEAR_PCA} (shape {N_PCA_OUT}×T per shot)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transform and save disruptive shots -> dsrpt_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeefead7",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DSRPT_PCA.mkdir(parents=True, exist_ok=True)\n",
        "if (DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    import shutil\n",
        "    shutil.copy(DECIMATED_ROOT / 'meta.csv', OUT_DSRPT_PCA / 'meta.csv')\n",
        "\n",
        "for shot, T in tqdm(list(dsrpt_lengths.items()), desc='Save disruptive PCA'):\n",
        "    data = transform_shot_global(DECIMATED_ROOT, shot, T, mu, W_K)\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "\n",
        "print(f'Saved {len(dsrpt_lengths)} shots to {OUT_DSRPT_PCA}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transform and save clear shots -> clear_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec72cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_CLEAR_PCA.mkdir(parents=True, exist_ok=True)\n",
        "if CLEAR_DECIMATED_ROOT.exists() and (CLEAR_DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    import shutil\n",
        "    shutil.copy(CLEAR_DECIMATED_ROOT / 'meta.csv', OUT_CLEAR_PCA / 'meta.csv')\n",
        "\n",
        "for shot, T in tqdm(list(clear_lengths.items()), desc='Save clear PCA'):\n",
        "    data = transform_shot_global(CLEAR_DECIMATED_ROOT, shot, T, mu, W_K)\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "\n",
        "print(f'Saved {len(clear_lengths)} shots to {OUT_CLEAR_PCA}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sanity: shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dsrpt_lengths:\n",
        "    shot0 = list(dsrpt_lengths.keys())[0]\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Disruptive PCA example: shot {shot0} LFS shape = {sh} (N_components, T)')\n",
        "if clear_lengths:\n",
        "    shot0 = list(clear_lengths.keys())[0]\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Clear PCA example:      shot {shot0} LFS shape = {sh} (N_components, T)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
