{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCA from decimated data — **global** PCA (recommended)\n",
        "\n",
        "**Why global PCA:** A per-sequence or per–time-index PCA gives different coordinate systems per sample (PC1 in shot A ≠ PC1 in shot B). That breaks learnability: the NN sees incompatible features. A **global** PCA gives a consistent embedding: every time point in every sequence is projected onto the same axes.\n",
        "\n",
        "**Pipeline:**\n",
        "- **SNR > 3 (paper):** Optionally add `snr_min` to meta (from shot lists or computed from decimated data). Fit PCA and write output **only for shots with SNR > 3**.\n",
        "- Split train/val/test at the **sequence (shot) level**.\n",
        "- Fit PCA **only on training time points** (and only on training shots with SNR > 3): stream through those shots, accumulate mean μ ∈ ℝ¹⁶⁰ and covariance C ∈ ℝ¹⁶⁰×¹⁶⁰ with a single-pass online algorithm (Welford). Memory stays O(160²). Eigendecompose C and take top K eigenvectors W_K.\n",
        "- Transform and save **only SNR > 3 shots**: z_t = (x_t − μ) W_K. Output dirs get `meta.csv` with `snr_min` column so downstream training (e.g. `run_fusion_soen --snr-min-threshold 3.0`) can use the same filter."
      ],
      "id": "36531d06"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths — decimated inputs (disruptive + clear)\n",
        "DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated')\n",
        "CLEAR_DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear_decimated')\n",
        "# Outputs: N_PCA_OUT in {1, 4, 8, 16} → dirs dsrpt_decimated_pca{N}, clear_decimated_pca{N}\n",
        "N_PCA_OUT = 1   # number of PCs to write (training default is 1)\n",
        "OUT_DSRPT_PCA = DECIMATED_ROOT.parent / f'dsrpt_decimated_pca{N_PCA_OUT}'\n",
        "OUT_CLEAR_PCA = DECIMATED_ROOT.parent / f'clear_decimated_pca{N_PCA_OUT}'\n",
        "\n",
        "N_COMPONENTS = 16   # fit top K (global PCA); must be >= N_PCA_OUT\n",
        "CHANNELS = 20 * 8   # 160\n",
        "\n",
        "# Paper (Churchill et al.): \"good ECEi data (SNR > 3)\" — fit PCA and write output only for these shots\n",
        "SNR_MIN_THRESHOLD = 3.0\n",
        "# If True, computed SNR is 20*log10(ratio) so threshold 3 means 3 dB. If False, SNR = ratio (linear); paper may use linear.\n",
        "SNR_IN_DB = False\n",
        "# Baseline fraction of time used as \"noise\" (first part of shot). Rest = \"signal\".\n",
        "SNR_BASELINE_FRAC = 0.1\n",
        "# Optional: DisruptCNN-format shot lists (columns: Shot, ..., SNR min at index 5). If None, SNR is computed from decimated H5.\n",
        "SHOT_LIST_DSRPT = None   # e.g. Path('disruptcnn/shots/d3d_disrupt_ecei.final.txt')\n",
        "SHOT_LIST_CLEAR = None   # e.g. Path('disruptcnn/shots/d3d_clear_ecei.final.txt')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "630fea1e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. SNR > 3 (paper): add snr_min to meta, use only for PCA fit and output\n",
        "\n",
        "Load SNR from DisruptCNN-format shot list files (column \"SNR min\") if set; otherwise **compute** from decimated H5: for each shot, **noise** = first `SNR_BASELINE_FRAC` of time (e.g. 10%), **signal** = rest. Per channel: ratio = std(signal)/std(noise); **snr_min = min over 160 channels**. Set `SNR_IN_DB=True` to store 20×log10(ratio) (then threshold 3 = 3 dB). **Computed values may not match the paper's curator SNR** — use shot lists when possible. Missing snr_min (NaN) → shot **kept**; only known snr_min ≤ threshold are dropped. Computation runs in parallel over shots."
      ],
      "id": "b96af41b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def _get_shot_lengths(root: Path, meta_path: Path = None) -> dict[int, int]:\n",
        "    \"\"\"Get {shot: T} from meta (n_samples/T column) if present, else from H5 shape without loading data.\"\"\"\n",
        "    if meta_path and meta_path.exists():\n",
        "        meta = pd.read_csv(meta_path)\n",
        "        for col in ('n_samples', 'T', 'length', 'n_samples_decimated'):\n",
        "            if col in meta.columns and 'shot' in meta.columns:\n",
        "                return dict(zip(meta['shot'].astype(int), meta[col].astype(int)))\n",
        "    out = {}\n",
        "    for p in (root.glob('*.h5') if root.exists() else []):\n",
        "        if not p.stem.isdigit():\n",
        "            continue\n",
        "        try:\n",
        "            with h5py.File(p, 'r') as f:\n",
        "                out[int(p.stem)] = f['LFS'].shape[-1]\n",
        "        except Exception:\n",
        "            pass\n",
        "    return out\n",
        "\n",
        "dsrpt_lengths = _get_shot_lengths(DECIMATED_ROOT, DECIMATED_ROOT / 'meta.csv') if DECIMATED_ROOT.exists() else {}\n",
        "clear_lengths = _get_shot_lengths(CLEAR_DECIMATED_ROOT, CLEAR_DECIMATED_ROOT / 'meta.csv') if CLEAR_DECIMATED_ROOT.exists() else {}\n",
        "\n",
        "meta_d = pd.read_csv(DECIMATED_ROOT / 'meta.csv') if (DECIMATED_ROOT / 'meta.csv').exists() else pd.DataFrame(columns=['shot', 'split', 't_disruption'])\n",
        "meta_c = pd.read_csv(CLEAR_DECIMATED_ROOT / 'meta.csv') if CLEAR_DECIMATED_ROOT.exists() and (CLEAR_DECIMATED_ROOT / 'meta.csv').exists() else pd.DataFrame(columns=['shot', 'split'])\n",
        "\n",
        "# Good shots = all shots in meta (no SNR filter); fallback to all in lengths if no meta\n",
        "good_shots_d = set(meta_d['shot'].astype(int)) if not meta_d.empty and 'shot' in meta_d.columns else set(dsrpt_lengths.keys())\n",
        "good_shots_c = set(meta_c['shot'].astype(int)) if not meta_c.empty and 'shot' in meta_c.columns else set(clear_lengths.keys())\n",
        "# No SNR computation; leave empty so downstream meta output gets snr_min=NaN\n",
        "snr_d, snr_c = {}, {}\n",
        "if meta_d.empty and dsrpt_lengths:\n",
        "    good_shots_d = set(dsrpt_lengths.keys())\n",
        "if meta_c.empty and clear_lengths:\n",
        "    good_shots_c = set(clear_lengths.keys())\n",
        "\n",
        "print(f'Disruptive: {len(dsrpt_lengths)} shots → {len(good_shots_d)} kept (no SNR filter).')\n",
        "print(f'Clear:      {len(clear_lengths)} shots → {len(good_shots_c)} kept (no SNR filter).')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "34060607"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shot list vs. data: `d3d_disrupt_ecei.final.txt`\n",
        "\n",
        "Check whether the **disrupt** and **clear** shots used in this notebook (from decimated dirs / meta) are included in the DisruptCNN-format shot list file. Path is relative to the notebook directory or set `SHOT_LIST_FINAL_TXT` to an absolute path."
      ],
      "id": "502c0361"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Path to DisruptCNN disrupt shot list (disruptcnn dir on idies scratch)\n",
        "DISRUPTCNN_ROOT = Path('/home/idies/workspace/Temporary/dpark1/scratch/soen_fusion_zero/disruptcnn')\n",
        "SHOT_LIST_FINAL_TXT = DISRUPTCNN_ROOT / 'shots' / 'd3d_disrupt_ecei.final.txt'\n",
        "if not SHOT_LIST_FINAL_TXT.is_absolute():\n",
        "    SHOT_LIST_FINAL_TXT = Path.cwd() / SHOT_LIST_FINAL_TXT\n",
        "\n",
        "def load_shots_from_final_txt(path: Path) -> set:\n",
        "    \"\"\"Load shot IDs (column 0) from DisruptCNN-format shot list; header line starts with #.\"\"\"\n",
        "    if not path.exists():\n",
        "        return set()\n",
        "    data = np.loadtxt(path, skiprows=1)\n",
        "    if data.size == 0:\n",
        "        return set()\n",
        "    if data.ndim == 1:\n",
        "        data = data.reshape(1, -1)\n",
        "    return set(data[:, 0].astype(int))\n",
        "\n",
        "shots_in_final_txt = load_shots_from_final_txt(SHOT_LIST_FINAL_TXT)\n",
        "# Shots we use: disrupt = from meta_d or dsrpt_lengths; clear = from meta_c or clear_lengths\n",
        "disrupt_shots_we_use = set(meta_d['shot'].astype(int)) if not meta_d.empty and 'shot' in meta_d.columns else set(dsrpt_lengths.keys())\n",
        "clear_shots_we_use = set(meta_c['shot'].astype(int)) if not meta_c.empty and 'shot' in meta_c.columns else set(clear_lengths.keys())\n",
        "\n",
        "print(f\"Shot list file: {SHOT_LIST_FINAL_TXT}\")\n",
        "print(f\"  Exists: {SHOT_LIST_FINAL_TXT.exists()}\")\n",
        "print(f\"  Shots in file: {len(shots_in_final_txt)}\")\n",
        "print()\n",
        "print(\"Disrupt shots (in our data):\")\n",
        "print(f\"  Total: {len(disrupt_shots_we_use)}\")\n",
        "if shots_in_final_txt:\n",
        "    in_both_d = disrupt_shots_we_use & shots_in_final_txt\n",
        "    only_ours_d = disrupt_shots_we_use - shots_in_final_txt\n",
        "    only_file_d = shots_in_final_txt - disrupt_shots_we_use\n",
        "    print(f\"  In shot list (included): {len(in_both_d)}\")\n",
        "    print(f\"  NOT in shot list (missing from file): {len(only_ours_d)}\")\n",
        "    if only_ours_d:\n",
        "        print(f\"    Example shots only in our data: {sorted(only_ours_d)[:10]}{'...' if len(only_ours_d) > 10 else ''}\")\n",
        "    print(f\"  In shot list but NOT in our data: {len(only_file_d)}\")\n",
        "    if only_file_d:\n",
        "        print(f\"    Example shots only in file: {sorted(only_file_d)[:10]}{'...' if len(only_file_d) > 10 else ''}\")\n",
        "print()\n",
        "print(\"Clear shots (in our data):\")\n",
        "print(f\"  Total: {len(clear_shots_we_use)}\")\n",
        "if shots_in_final_txt:\n",
        "    in_both_c = clear_shots_we_use & shots_in_final_txt\n",
        "    only_ours_c = clear_shots_we_use - shots_in_final_txt\n",
        "    print(f\"  In shot list (disrupt file; clear usually 0): {len(in_both_c)}\")\n",
        "    print(f\"  NOT in shot list: {len(only_ours_c)}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "b68ec1bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List shots, lengths, and splits (train/val/test)"
      ],
      "id": "eb35c919"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_shot_lengths(root: Path) -> dict[int, int]:\n",
        "    \"\"\"Return {shot_id: T} for each .h5 in root.\"\"\"\n",
        "    out = {}\n",
        "    for p in root.glob('*.h5'):\n",
        "        if not p.stem.isdigit():\n",
        "            continue\n",
        "        shot = int(p.stem)\n",
        "        with h5py.File(p, 'r') as f:\n",
        "            T = f['LFS'].shape[-1]\n",
        "        out[shot] = T\n",
        "    return out\n",
        "\n",
        "def get_splits(root: Path, lengths: dict) -> dict[int, str]:\n",
        "    \"\"\"Return {shot_id: 'train'|'val'|'test'} from meta.csv or 80/20 default.\"\"\"\n",
        "    if (root / 'meta.csv').exists():\n",
        "        meta = pd.read_csv(root / 'meta.csv')\n",
        "        if 'split' in meta.columns:\n",
        "            return dict(zip(meta['shot'].astype(int), meta['split'].astype(str)))\n",
        "    # Default: 80% train, 20% test\n",
        "    shots = list(lengths.keys())\n",
        "    n = len(shots)\n",
        "    n_train = int(0.8 * n)\n",
        "    out = {s: 'train' for s in shots[:n_train]}\n",
        "    for s in shots[n_train:]:\n",
        "        out[s] = 'test'\n",
        "    return out\n",
        "\n",
        "dsrpt_lengths = get_shot_lengths(DECIMATED_ROOT) if DECIMATED_ROOT.exists() else {}\n",
        "clear_lengths = get_shot_lengths(CLEAR_DECIMATED_ROOT) if CLEAR_DECIMATED_ROOT.exists() else {}\n",
        "dsrpt_splits = get_splits(DECIMATED_ROOT, dsrpt_lengths) if dsrpt_lengths else {}\n",
        "clear_splits = get_splits(CLEAR_DECIMATED_ROOT, clear_lengths) if clear_lengths else {}\n",
        "\n",
        "train_shots_d = [s for s, sp in dsrpt_splits.items() if sp == 'train']\n",
        "train_shots_c = [s for s, sp in clear_splits.items() if sp == 'train']\n",
        "# Restrict to SNR > threshold (good_shots from section 0) for PCA fit\n",
        "train_shots_d = [s for s in train_shots_d if s in good_shots_d]\n",
        "train_shots_c = [s for s in train_shots_c if s in good_shots_c]\n",
        "print(f'Disruptive: {len(dsrpt_lengths)} shots  (train with SNR>3={len(train_shots_d)})')\n",
        "print(f'Clear:      {len(clear_lengths)} shots  (train with SNR>3={len(train_shots_c)})')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6abcdd48"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Streaming mean and covariance (training time points only)"
      ],
      "id": "7f02f502"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Welford in batches: merge (n, mean, M2) with batch (B, 160). Memory O(160²) + O(B*160).\n",
        "BATCH_SIZE = 10_000   # time points per batch (vectorized update)\n",
        "\n",
        "def welford_merge(n, mean, M2, batch: np.ndarray):\n",
        "    \"\"\"Merge batch (B, 160) into running stats. Returns (n_new, mean_new, M2_new).\"\"\"\n",
        "    B = len(batch)\n",
        "    if B == 0:\n",
        "        return n, mean, M2\n",
        "    batch_mean = batch.mean(axis=0)\n",
        "    batch_M2 = (batch - batch_mean).T @ (batch - batch_mean)\n",
        "    n_new = n + B\n",
        "    mean_new = (n * mean + B * batch_mean) / n_new\n",
        "    delta = mean - batch_mean\n",
        "    M2_new = M2 + batch_M2 + (n * B / n_new) * np.outer(delta, delta)\n",
        "    return n_new, mean_new, M2_new\n",
        "\n",
        "def load_shot_flat(root: Path, shot: int, T: int) -> np.ndarray:\n",
        "    \"\"\"Load one shot as (T, 160) float64.\"\"\"\n",
        "    with h5py.File(root / f'{shot}.h5', 'r') as f:\n",
        "        data = np.asarray(f['LFS'][:], dtype=np.float64)  # (20, 8, T)\n",
        "    return data.reshape(CHANNELS, -1).T  # (T, 160)\n",
        "\n",
        "def transform_shot_global(root: Path, shot: int, T: int, mu: np.ndarray, W_K: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"z_t = (x_t - μ) W_K. Return (K, T) for LFS layout.\"\"\"\n",
        "    X = load_shot_flat(root, shot, T)  # (T, 160)\n",
        "    Z = (X - mu) @ W_K  # (T, K)\n",
        "    return Z.T.astype(np.float32)  # (K, T)\n",
        "\n",
        "# Training shots only: (root, shot, T)\n",
        "train_tuples = [(DECIMATED_ROOT, s, dsrpt_lengths[s]) for s in train_shots_d]\n",
        "train_tuples += [(CLEAR_DECIMATED_ROOT, s, clear_lengths[s]) for s in train_shots_c]\n",
        "\n",
        "n_total = 0\n",
        "mean = np.zeros(CHANNELS, dtype=np.float64)\n",
        "M2 = np.zeros((CHANNELS, CHANNELS), dtype=np.float64)\n",
        "\n",
        "for root, shot, T in tqdm(train_tuples, desc='Streaming μ and M2 (train only)'):\n",
        "    X = load_shot_flat(root, shot, T)  # (T, 160)\n",
        "    for start in range(0, len(X), BATCH_SIZE):\n",
        "        batch = X[start:start + BATCH_SIZE]\n",
        "        n_total, mean, M2 = welford_merge(n_total, mean, M2, batch)\n",
        "\n",
        "print(f'Training time points: {n_total}')\n",
        "if n_total < 2:\n",
        "    raise ValueError('Need at least 2 training time points to fit PCA')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "e450420d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Eigendecompose C and take top K eigenvectors"
      ],
      "id": "825cfeac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "C = M2 / (n_total - 1)\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(C)\n",
        "idx = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[idx]\n",
        "eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "K = min(N_COMPONENTS, CHANNELS, len(eigenvalues))\n",
        "mu = mean.copy()\n",
        "W_K = eigenvectors[:, :K].astype(np.float32)  # (160, K)\n",
        "\n",
        "var_explained = eigenvalues / eigenvalues.sum()\n",
        "print(f'Global PCA: μ ∈ R^{CHANNELS}, W_K ∈ R^{CHANNELS}×{K}')\n",
        "print(f'Cumulative variance (top {K}): {var_explained[:K].sum():.4f}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5968d86a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3b. How many components for ≥99% variance? (from global C)"
      ],
      "id": "84126840"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_variance = 0.99\n",
        "cumvar = np.cumsum(var_explained)\n",
        "k_99 = int(np.searchsorted(cumvar, target_variance)) + 1\n",
        "k_99 = min(k_99, len(cumvar))\n",
        "print(f'Components needed for ≥{target_variance*100:.0f}% variance (global PCA): {k_99}')\n",
        "print(f'  → Consider N_COMPONENTS >= {k_99} to retain ≥99% of the information.')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "c35148eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3c. Save PCA result (top 16 PCs; write 1/4/8/16 via N_PCA_OUT in 3d)"
      ],
      "id": "0ff93473"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "N_PC_SAVE = 16   # save top 16 so we can write _pca1, _pca4, _pca8, _pca16\n",
        "W_full = eigenvectors[:, :N_PC_SAVE].astype(np.float32)  # (160, 16)\n",
        "pca_save_path = Path('pca_global_top16.npz')\n",
        "np.savez(pca_save_path, mu=mu.astype(np.float32), W_K=W_full, var_explained=var_explained[:N_PC_SAVE].astype(np.float32))\n",
        "\n",
        "info_saved = var_explained[:N_PC_SAVE].sum()\n",
        "print(f'Saved top {N_PC_SAVE} PCs to {pca_save_path}')\n",
        "print(f'Variance (information) retained: {info_saved*100:.2f}%')\n",
        "print(f'Per-component: {var_explained[:N_PC_SAVE]}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "3f4ea640"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3d. Save transformed data (top N_PCA_OUT PCs → *_pca1, *_pca4, *_pca8, *_pca16)"
      ],
      "id": "5a1325f5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "W_save = W_full[:, :N_PCA_OUT]\n",
        "import shutil\n",
        "\n",
        "# Only write shots with SNR > threshold; write meta with snr_min column\n",
        "OUT_DSRPT_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_d_out = meta_d[meta_d['shot'].astype(int).isin(good_shots_d)] if not meta_d.empty else pd.DataFrame()\n",
        "if meta_d_out.empty and good_shots_d:\n",
        "    meta_d_out = pd.DataFrame({'shot': list(good_shots_d), 'snr_min': [snr_d.get(s, np.nan) for s in good_shots_d], 'split': [dsrpt_splits.get(s, 'train') for s in good_shots_d]})\n",
        "    if not meta_d.empty and 't_disruption' in meta_d.columns:\n",
        "        meta_d_out['t_disruption'] = meta_d_out['shot'].map(meta_d.set_index('shot')['t_disruption'])\n",
        "if not meta_d_out.empty:\n",
        "    meta_d_out.to_csv(OUT_DSRPT_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in dsrpt_lengths if s in good_shots_d], desc=f'Save disruptive {N_PCA_OUT}-PC (SNR>3)'):\n",
        "    T = dsrpt_lengths[shot]\n",
        "    data = transform_shot_global(DECIMATED_ROOT, shot, T, mu, W_save)\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_d)} disruptive shots (SNR>3) to {OUT_DSRPT_PCA} (shape {N_PCA_OUT}×T per shot)')\n",
        "\n",
        "OUT_CLEAR_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_c_out = meta_c[meta_c['shot'].astype(int).isin(good_shots_c)] if not meta_c.empty else pd.DataFrame()\n",
        "if meta_c_out.empty and good_shots_c:\n",
        "    meta_c_out = pd.DataFrame({'shot': list(good_shots_c), 'snr_min': [snr_c.get(s, np.nan) for s in good_shots_c], 'split': [clear_splits.get(s, 'train') for s in good_shots_c]})\n",
        "if not meta_c_out.empty:\n",
        "    meta_c_out.to_csv(OUT_CLEAR_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in clear_lengths if s in good_shots_c], desc=f'Save clear {N_PCA_OUT}-PC (SNR>3)'):\n",
        "    T = clear_lengths[shot]\n",
        "    data = transform_shot_global(CLEAR_DECIMATED_ROOT, shot, T, mu, W_save)\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_c)} clear shots (SNR>3) to {OUT_CLEAR_PCA} (shape {N_PCA_OUT}×T per shot)')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "efb79785"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transform and save disruptive shots -> dsrpt_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "OUT_DSRPT_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_d_out = meta_d[meta_d['shot'].astype(int).isin(good_shots_d)] if not meta_d.empty else meta_d\n",
        "if not meta_d_out.empty:\n",
        "    meta_d_out.to_csv(OUT_DSRPT_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in dsrpt_lengths if s in good_shots_d], desc='Save disruptive PCA (SNR>3)'):\n",
        "    data = transform_shot_global(DECIMATED_ROOT, shot, dsrpt_lengths[shot], mu, W_K)\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_d)} disruptive shots (SNR>3) to {OUT_DSRPT_PCA}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "aeefead7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transform and save clear shots -> clear_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "OUT_CLEAR_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_c_out = meta_c[meta_c['shot'].astype(int).isin(good_shots_c)] if not meta_c.empty else meta_c\n",
        "if not meta_c_out.empty:\n",
        "    meta_c_out.to_csv(OUT_CLEAR_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in clear_lengths if s in good_shots_c], desc='Save clear PCA (SNR>3)'):\n",
        "    data = transform_shot_global(CLEAR_DECIMATED_ROOT, shot, clear_lengths[shot], mu, W_K)\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_c)} clear shots (SNR>3) to {OUT_CLEAR_PCA}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "5ec72cd9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sanity: shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if good_shots_d:\n",
        "    shot0 = next(iter(good_shots_d))\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Disruptive PCA example: shot {shot0} LFS shape = {sh} (N_components, T)')\n",
        "if good_shots_c:\n",
        "    shot0 = next(iter(good_shots_c))\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Clear PCA example:      shot {shot0} LFS shape = {sh} (N_components, T)')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "4f163e3a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}