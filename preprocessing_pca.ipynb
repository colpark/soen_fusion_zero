{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "36531d06",
      "metadata": {},
      "source": [
        "# PCA from decimated data — **global** PCA (recommended)\n",
        "\n",
        "**Why global PCA:** A per-sequence or per–time-index PCA gives different coordinate systems per sample (PC1 in shot A ≠ PC1 in shot B). That breaks learnability: the NN sees incompatible features. A **global** PCA gives a consistent embedding: every time point in every sequence is projected onto the same axes.\n",
        "\n",
        "**Pipeline:**\n",
        "- **SNR > 3 (paper):** Optionally add `snr_min` to meta (from shot lists or computed from decimated data). Fit PCA and write output **only for shots with SNR > 3**.\n",
        "- Split train/val/test at the **sequence (shot) level**.\n",
        "- Fit PCA **only on training time points** (and only on training shots with SNR > 3): stream through those shots, accumulate mean μ ∈ ℝ¹⁶⁰ and covariance C ∈ ℝ¹⁶⁰×¹⁶⁰ with a single-pass online algorithm (Welford). Memory stays O(160²). Eigendecompose C and take top K eigenvectors W_K.\n",
        "- Transform and save **only SNR > 3 shots**: z_t = (x_t − μ) W_K. Output dirs get `meta.csv` with `snr_min` column so downstream training (e.g. `run_fusion_soen --snr-min-threshold 3.0`) can use the same filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630fea1e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths — decimated inputs (disruptive + clear)\n",
        "DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated')\n",
        "CLEAR_DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear_decimated')\n",
        "# Outputs: N_PCA_OUT in {1, 4, 8, 16} → dirs dsrpt_decimated_pca{N}, clear_decimated_pca{N}\n",
        "N_PCA_OUT = 1   # number of PCs to write (training default is 1)\n",
        "OUT_DSRPT_PCA = DECIMATED_ROOT.parent / f'dsrpt_decimated_pca{N_PCA_OUT}'\n",
        "OUT_CLEAR_PCA = DECIMATED_ROOT.parent / f'clear_decimated_pca{N_PCA_OUT}'\n",
        "\n",
        "N_COMPONENTS = 16   # fit top K (global PCA); must be >= N_PCA_OUT\n",
        "CHANNELS = 20 * 8   # 160\n",
        "\n",
        "# Paper (Churchill et al.): \"good ECEi data (SNR > 3)\" — fit PCA and write output only for these shots\n",
        "SNR_MIN_THRESHOLD = 3.0\n",
        "# Optional: DisruptCNN-format shot lists (columns: Shot, ..., SNR min at index 5). If None, SNR is computed from decimated H5.\n",
        "SHOT_LIST_DSRPT = None   # e.g. Path('disruptcnn/shots/d3d_disrupt_ecei.final.txt')\n",
        "SHOT_LIST_CLEAR = None   # e.g. Path('disruptcnn/shots/d3d_clear_ecei.final.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96af41b",
      "metadata": {},
      "source": [
        "## 0. SNR > 3 (paper): add snr_min to meta, use only for PCA fit and output\n",
        "\n",
        "Load SNR from DisruptCNN-format shot list files (column \"SNR min\") if `SHOT_LIST_DSRPT` / `SHOT_LIST_CLEAR` are set; otherwise compute a simple per-shot proxy from decimated data (min over channels of std(rest)/std(baseline)). **The computed proxy is not the same scale as the paper's curator SNR** — use shot lists when possible. Shots with **missing** snr_min (NaN) are **kept**; only shots with snr_min **known and ≤ threshold** are dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34060607",
      "metadata": {},
      "outputs": [],
      "source": [
        "def _get_shot_lengths(root: Path, meta_path: Path = None) -> dict[int, int]:\n",
        "    \"\"\"Get {shot: T} from meta (n_samples/T column) if present, else from H5 shape without loading data. Uses parallel file opens for speed.\"\"\"\n",
        "    if meta_path and meta_path.exists():\n",
        "        meta = pd.read_csv(meta_path)\n",
        "        for col in ('n_samples', 'T', 'length', 'n_samples_decimated'):\n",
        "            if col in meta.columns and 'shot' in meta.columns:\n",
        "                return dict(zip(meta['shot'].astype(int), meta[col].astype(int)))\n",
        "    def _one(p):\n",
        "        if not p.stem.isdigit():\n",
        "            return None\n",
        "        try:\n",
        "            with h5py.File(p, 'r') as f:\n",
        "                return (int(p.stem), f['LFS'].shape[-1])\n",
        "        except Exception:\n",
        "            return None\n",
        "    paths = [p for p in root.glob('*.h5') if p.stem.isdigit()]\n",
        "    out = {}\n",
        "    with ThreadPoolExecutor(max_workers=min(32, len(paths) or 1)) as ex:\n",
        "        for fut in as_completed(ex.submit(_one, p) for p in paths):\n",
        "            res = fut.result()\n",
        "            if res is not None:\n",
        "                out[res[0]] = res[1]\n",
        "    return out\n",
        "dsrpt_lengths = _get_shot_lengths(DECIMATED_ROOT, DECIMATED_ROOT / 'meta.csv') if DECIMATED_ROOT.exists() else {}\n",
        "clear_lengths = _get_shot_lengths(CLEAR_DECIMATED_ROOT, CLEAR_DECIMATED_ROOT / 'meta.csv') if CLEAR_DECIMATED_ROOT.exists() else {}\n",
        "\n",
        "def load_snr_from_shot_list(path: Path) -> dict[int, float]:\n",
        "    \"\"\"Load shot -> SNR min from DisruptCNN-format shot list (tab-sep, SNR min at column index 5).\"\"\"\n",
        "    if path is None or not Path(path).exists():\n",
        "        return {}\n",
        "    data = np.loadtxt(path, skiprows=1)\n",
        "    if data.size == 0:\n",
        "        return {}\n",
        "    if data.ndim == 1:\n",
        "        data = data.reshape(1, -1)\n",
        "    shots = data[:, 0].astype(int)\n",
        "    snr = data[:, 5].astype(float)\n",
        "    return dict(zip(shots, snr))\n",
        "\n",
        "def compute_snr_from_decimated(root: Path, shot: int, T: int, baseline_frac: float = 0.1) -> float:\n",
        "    \"\"\"Simple SNR proxy: min over channels of (std(rest) / std(baseline)). Baseline = first baseline_frac of time.\"\"\"\n",
        "    with h5py.File(root / f'{shot}.h5', 'r') as f:\n",
        "        X = np.asarray(f['LFS'][:], dtype=np.float64)  # (20, 8, T)\n",
        "    X_flat = X.reshape(CHANNELS, -1).T  # (T, 160)\n",
        "    n_base = max(1, int(T * baseline_frac))\n",
        "    baseline = X_flat[:n_base]\n",
        "    rest = X_flat[n_base:]\n",
        "    if rest.shape[0] < 2:\n",
        "        return np.nan\n",
        "    std_base = np.std(baseline, axis=0)\n",
        "    std_rest = np.std(rest, axis=0)\n",
        "    std_base = np.where(std_base > 1e-12, std_base, 1e-12)\n",
        "    ratio = std_rest / std_base\n",
        "    return float(np.min(ratio))\n",
        "\n",
        "# Load or compute snr_min per shot\n",
        "snr_d = load_snr_from_shot_list(SHOT_LIST_DSRPT)\n",
        "snr_c = load_snr_from_shot_list(SHOT_LIST_CLEAR)\n",
        "if not snr_d and DECIMATED_ROOT.exists():\n",
        "    print('Computing SNR from decimated disruptive data...')\n",
        "    for shot in tqdm(list(dsrpt_lengths.keys())):\n",
        "        snr_d[shot] = compute_snr_from_decimated(DECIMATED_ROOT, shot, dsrpt_lengths[shot])\n",
        "if not snr_c and CLEAR_DECIMATED_ROOT.exists():\n",
        "    print('Computing SNR from decimated clear data...')\n",
        "    for shot in tqdm(list(clear_lengths.keys())):\n",
        "        snr_c[shot] = compute_snr_from_decimated(CLEAR_DECIMATED_ROOT, shot, clear_lengths[shot])\n",
        "\n",
        "# Load meta and add snr_min (so downstream and output meta have it)\n",
        "meta_d = pd.read_csv(DECIMATED_ROOT / 'meta.csv') if (DECIMATED_ROOT / 'meta.csv').exists() else pd.DataFrame(columns=['shot', 'split', 't_disruption'])\n",
        "meta_c = pd.read_csv(CLEAR_DECIMATED_ROOT / 'meta.csv') if CLEAR_DECIMATED_ROOT.exists() and (CLEAR_DECIMATED_ROOT / 'meta.csv').exists() else pd.DataFrame(columns=['shot', 'split'])\n",
        "if 'shot' in meta_d.columns:\n",
        "    meta_d['snr_min'] = meta_d['shot'].astype(int).map(snr_d)\n",
        "if 'shot' in meta_c.columns:\n",
        "    meta_c['snr_min'] = meta_c['shot'].astype(int).map(snr_c)\n",
        "\n",
        "# Shots that pass SNR > threshold. Only drop when snr_min is known and <= threshold; keep when NaN (missing) to avoid over-dropping.\n",
        "def _good_shots(meta, snr_col='snr_min'):\n",
        "    if snr_col not in meta.columns or meta.empty:\n",
        "        return set()\n",
        "    s = pd.to_numeric(meta[snr_col], errors='coerce')\n",
        "    # Keep: (snr_min > threshold) OR (snr_min is NaN = no info, keep to be safe)\n",
        "    keep = (s > SNR_MIN_THRESHOLD) | s.isna()\n",
        "    return set(meta.loc[keep, 'shot'].astype(int))\n",
        "\n",
        "good_shots_d = _good_shots(meta_d) if not meta_d.empty else set(dsrpt_lengths.keys())\n",
        "good_shots_c = _good_shots(meta_c) if not meta_c.empty else set(clear_lengths.keys())\n",
        "if meta_d.empty and dsrpt_lengths:\n",
        "    good_shots_d = set(dsrpt_lengths.keys())\n",
        "if meta_c.empty and clear_lengths:\n",
        "    good_shots_c = set(clear_lengths.keys())\n",
        "\n",
        "# Diagnostics: so you can check if computed SNR or NaN is causing big drops\n",
        "if 'snr_min' in meta_d.columns and not meta_d.empty:\n",
        "    s = pd.to_numeric(meta_d['snr_min'], errors='coerce')\n",
        "    n_na = s.isna().sum()\n",
        "    s_ok = s.dropna()\n",
        "    print(f'Disruptive snr_min: n={len(s)}  NaN={n_na}  valid: min={s_ok.min():.3f} median={s_ok.median():.3f} max={s_ok.max():.3f}  n>{SNR_MIN_THRESHOLD}={(s_ok > SNR_MIN_THRESHOLD).sum()}')\n",
        "if 'snr_min' in meta_c.columns and not meta_c.empty:\n",
        "    s = pd.to_numeric(meta_c['snr_min'], errors='coerce')\n",
        "    n_na = s.isna().sum()\n",
        "    s_ok = s.dropna()\n",
        "    print(f'Clear snr_min:       n={len(s)}  NaN={n_na}  valid: min={s_ok.min():.3f} median={s_ok.median():.3f} max={s_ok.max():.3f}  n>{SNR_MIN_THRESHOLD}={(s_ok > SNR_MIN_THRESHOLD).sum()}')\n",
        "\n",
        "# Save meta with snr_min back to decimated dirs (so run_fusion_soen --snr-min-threshold can use it when reading from _pca dirs)\n",
        "if not meta_d.empty and (DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    meta_d.to_csv(DECIMATED_ROOT / 'meta.csv', index=False)\n",
        "if not meta_c.empty and CLEAR_DECIMATED_ROOT.exists() and (CLEAR_DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    meta_c.to_csv(CLEAR_DECIMATED_ROOT / 'meta.csv', index=False)\n",
        "\n",
        "n_drop_d = len(dsrpt_lengths) - len(good_shots_d) if dsrpt_lengths else 0\n",
        "n_drop_c = len(clear_lengths) - len(good_shots_c) if clear_lengths else 0\n",
        "print(f'SNR > {SNR_MIN_THRESHOLD} (or NaN): disruptive {len(good_shots_d)} kept, {n_drop_d} dropped; clear {len(good_shots_c)} kept, {n_drop_c} dropped')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb35c919",
      "metadata": {},
      "source": [
        "## 1. List shots, lengths, and splits (train/val/test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6abcdd48",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_shot_lengths(root: Path) -> dict[int, int]:\n",
        "    \"\"\"Return {shot_id: T} for each .h5 in root.\"\"\"\n",
        "    out = {}\n",
        "    for p in root.glob('*.h5'):\n",
        "        if not p.stem.isdigit():\n",
        "            continue\n",
        "        shot = int(p.stem)\n",
        "        with h5py.File(p, 'r') as f:\n",
        "            T = f['LFS'].shape[-1]\n",
        "        out[shot] = T\n",
        "    return out\n",
        "\n",
        "def get_splits(root: Path, lengths: dict) -> dict[int, str]:\n",
        "    \"\"\"Return {shot_id: 'train'|'val'|'test'} from meta.csv or 80/20 default.\"\"\"\n",
        "    if (root / 'meta.csv').exists():\n",
        "        meta = pd.read_csv(root / 'meta.csv')\n",
        "        if 'split' in meta.columns:\n",
        "            return dict(zip(meta['shot'].astype(int), meta['split'].astype(str)))\n",
        "    # Default: 80% train, 20% test\n",
        "    shots = list(lengths.keys())\n",
        "    n = len(shots)\n",
        "    n_train = int(0.8 * n)\n",
        "    out = {s: 'train' for s in shots[:n_train]}\n",
        "    for s in shots[n_train:]:\n",
        "        out[s] = 'test'\n",
        "    return out\n",
        "\n",
        "dsrpt_lengths = get_shot_lengths(DECIMATED_ROOT) if DECIMATED_ROOT.exists() else {}\n",
        "clear_lengths = get_shot_lengths(CLEAR_DECIMATED_ROOT) if CLEAR_DECIMATED_ROOT.exists() else {}\n",
        "dsrpt_splits = get_splits(DECIMATED_ROOT, dsrpt_lengths) if dsrpt_lengths else {}\n",
        "clear_splits = get_splits(CLEAR_DECIMATED_ROOT, clear_lengths) if clear_lengths else {}\n",
        "\n",
        "train_shots_d = [s for s, sp in dsrpt_splits.items() if sp == 'train']\n",
        "train_shots_c = [s for s, sp in clear_splits.items() if sp == 'train']\n",
        "# Restrict to SNR > threshold (good_shots from section 0) for PCA fit\n",
        "train_shots_d = [s for s in train_shots_d if s in good_shots_d]\n",
        "train_shots_c = [s for s in train_shots_c if s in good_shots_c]\n",
        "print(f'Disruptive: {len(dsrpt_lengths)} shots  (train with SNR>3={len(train_shots_d)})')\n",
        "print(f'Clear:      {len(clear_lengths)} shots  (train with SNR>3={len(train_shots_c)})')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f02f502",
      "metadata": {},
      "source": [
        "## 2. Streaming mean and covariance (training time points only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e450420d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Welford in batches: merge (n, mean, M2) with batch (B, 160). Memory O(160²) + O(B*160).\n",
        "BATCH_SIZE = 10_000   # time points per batch (vectorized update)\n",
        "\n",
        "def welford_merge(n, mean, M2, batch: np.ndarray):\n",
        "    \"\"\"Merge batch (B, 160) into running stats. Returns (n_new, mean_new, M2_new).\"\"\"\n",
        "    B = len(batch)\n",
        "    if B == 0:\n",
        "        return n, mean, M2\n",
        "    batch_mean = batch.mean(axis=0)\n",
        "    batch_M2 = (batch - batch_mean).T @ (batch - batch_mean)\n",
        "    n_new = n + B\n",
        "    mean_new = (n * mean + B * batch_mean) / n_new\n",
        "    delta = mean - batch_mean\n",
        "    M2_new = M2 + batch_M2 + (n * B / n_new) * np.outer(delta, delta)\n",
        "    return n_new, mean_new, M2_new\n",
        "\n",
        "def load_shot_flat(root: Path, shot: int, T: int) -> np.ndarray:\n",
        "    \"\"\"Load one shot as (T, 160) float64.\"\"\"\n",
        "    with h5py.File(root / f'{shot}.h5', 'r') as f:\n",
        "        data = np.asarray(f['LFS'][:], dtype=np.float64)  # (20, 8, T)\n",
        "    return data.reshape(CHANNELS, -1).T  # (T, 160)\n",
        "\n",
        "def transform_shot_global(root: Path, shot: int, T: int, mu: np.ndarray, W_K: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"z_t = (x_t - μ) W_K. Return (K, T) for LFS layout.\"\"\"\n",
        "    X = load_shot_flat(root, shot, T)  # (T, 160)\n",
        "    Z = (X - mu) @ W_K  # (T, K)\n",
        "    return Z.T.astype(np.float32)  # (K, T)\n",
        "\n",
        "# Training shots only: (root, shot, T)\n",
        "train_tuples = [(DECIMATED_ROOT, s, dsrpt_lengths[s]) for s in train_shots_d]\n",
        "train_tuples += [(CLEAR_DECIMATED_ROOT, s, clear_lengths[s]) for s in train_shots_c]\n",
        "\n",
        "n_total = 0\n",
        "mean = np.zeros(CHANNELS, dtype=np.float64)\n",
        "M2 = np.zeros((CHANNELS, CHANNELS), dtype=np.float64)\n",
        "\n",
        "for root, shot, T in tqdm(train_tuples, desc='Streaming μ and M2 (train only)'):\n",
        "    X = load_shot_flat(root, shot, T)  # (T, 160)\n",
        "    for start in range(0, len(X), BATCH_SIZE):\n",
        "        batch = X[start:start + BATCH_SIZE]\n",
        "        n_total, mean, M2 = welford_merge(n_total, mean, M2, batch)\n",
        "\n",
        "print(f'Training time points: {n_total}')\n",
        "if n_total < 2:\n",
        "    raise ValueError('Need at least 2 training time points to fit PCA')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "825cfeac",
      "metadata": {},
      "source": [
        "## 3. Eigendecompose C and take top K eigenvectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5968d86a",
      "metadata": {},
      "outputs": [],
      "source": [
        "C = M2 / (n_total - 1)\n",
        "eigenvalues, eigenvectors = np.linalg.eigh(C)\n",
        "idx = np.argsort(eigenvalues)[::-1]\n",
        "eigenvalues = eigenvalues[idx]\n",
        "eigenvectors = eigenvectors[:, idx]\n",
        "\n",
        "K = min(N_COMPONENTS, CHANNELS, len(eigenvalues))\n",
        "mu = mean.copy()\n",
        "W_K = eigenvectors[:, :K].astype(np.float32)  # (160, K)\n",
        "\n",
        "var_explained = eigenvalues / eigenvalues.sum()\n",
        "print(f'Global PCA: μ ∈ R^{CHANNELS}, W_K ∈ R^{CHANNELS}×{K}')\n",
        "print(f'Cumulative variance (top {K}): {var_explained[:K].sum():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84126840",
      "metadata": {},
      "source": [
        "## 3b. How many components for ≥99% variance? (from global C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c35148eb",
      "metadata": {},
      "outputs": [],
      "source": [
        "target_variance = 0.99\n",
        "cumvar = np.cumsum(var_explained)\n",
        "k_99 = int(np.searchsorted(cumvar, target_variance)) + 1\n",
        "k_99 = min(k_99, len(cumvar))\n",
        "print(f'Components needed for ≥{target_variance*100:.0f}% variance (global PCA): {k_99}')\n",
        "print(f'  → Consider N_COMPONENTS >= {k_99} to retain ≥99% of the information.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff93473",
      "metadata": {},
      "source": [
        "## 3c. Save PCA result (top 16 PCs; write 1/4/8/16 via N_PCA_OUT in 3d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f4ea640",
      "metadata": {},
      "outputs": [],
      "source": [
        "N_PC_SAVE = 16   # save top 16 so we can write _pca1, _pca4, _pca8, _pca16\n",
        "W_full = eigenvectors[:, :N_PC_SAVE].astype(np.float32)  # (160, 16)\n",
        "pca_save_path = Path('pca_global_top16.npz')\n",
        "np.savez(pca_save_path, mu=mu.astype(np.float32), W_K=W_full, var_explained=var_explained[:N_PC_SAVE].astype(np.float32))\n",
        "\n",
        "info_saved = var_explained[:N_PC_SAVE].sum()\n",
        "print(f'Saved top {N_PC_SAVE} PCs to {pca_save_path}')\n",
        "print(f'Variance (information) retained: {info_saved*100:.2f}%')\n",
        "print(f'Per-component: {var_explained[:N_PC_SAVE]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a1325f5",
      "metadata": {},
      "source": [
        "## 3d. Save transformed data (top N_PCA_OUT PCs → *_pca1, *_pca4, *_pca8, *_pca16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efb79785",
      "metadata": {},
      "outputs": [],
      "source": [
        "W_save = W_full[:, :N_PCA_OUT]\n",
        "import shutil\n",
        "\n",
        "# Only write shots with SNR > threshold; write meta with snr_min column\n",
        "OUT_DSRPT_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_d_out = meta_d[meta_d['shot'].astype(int).isin(good_shots_d)] if not meta_d.empty else pd.DataFrame()\n",
        "if meta_d_out.empty and good_shots_d:\n",
        "    meta_d_out = pd.DataFrame({'shot': list(good_shots_d), 'snr_min': [snr_d.get(s, np.nan) for s in good_shots_d], 'split': [dsrpt_splits.get(s, 'train') for s in good_shots_d]})\n",
        "    if not meta_d.empty and 't_disruption' in meta_d.columns:\n",
        "        meta_d_out['t_disruption'] = meta_d_out['shot'].map(meta_d.set_index('shot')['t_disruption'])\n",
        "if not meta_d_out.empty:\n",
        "    meta_d_out.to_csv(OUT_DSRPT_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in dsrpt_lengths if s in good_shots_d], desc=f'Save disruptive {N_PCA_OUT}-PC (SNR>3)'):\n",
        "    T = dsrpt_lengths[shot]\n",
        "    data = transform_shot_global(DECIMATED_ROOT, shot, T, mu, W_save)\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_d)} disruptive shots (SNR>3) to {OUT_DSRPT_PCA} (shape {N_PCA_OUT}×T per shot)')\n",
        "\n",
        "OUT_CLEAR_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_c_out = meta_c[meta_c['shot'].astype(int).isin(good_shots_c)] if not meta_c.empty else pd.DataFrame()\n",
        "if meta_c_out.empty and good_shots_c:\n",
        "    meta_c_out = pd.DataFrame({'shot': list(good_shots_c), 'snr_min': [snr_c.get(s, np.nan) for s in good_shots_c], 'split': [clear_splits.get(s, 'train') for s in good_shots_c]})\n",
        "if not meta_c_out.empty:\n",
        "    meta_c_out.to_csv(OUT_CLEAR_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in clear_lengths if s in good_shots_c], desc=f'Save clear {N_PCA_OUT}-PC (SNR>3)'):\n",
        "    T = clear_lengths[shot]\n",
        "    data = transform_shot_global(CLEAR_DECIMATED_ROOT, shot, T, mu, W_save)\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_c)} clear shots (SNR>3) to {OUT_CLEAR_PCA} (shape {N_PCA_OUT}×T per shot)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transform and save disruptive shots -> dsrpt_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeefead7",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_DSRPT_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_d_out = meta_d[meta_d['shot'].astype(int).isin(good_shots_d)] if not meta_d.empty else meta_d\n",
        "if not meta_d_out.empty:\n",
        "    meta_d_out.to_csv(OUT_DSRPT_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in dsrpt_lengths if s in good_shots_d], desc='Save disruptive PCA (SNR>3)'):\n",
        "    data = transform_shot_global(DECIMATED_ROOT, shot, dsrpt_lengths[shot], mu, W_K)\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_d)} disruptive shots (SNR>3) to {OUT_DSRPT_PCA}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transform and save clear shots -> clear_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ec72cd9",
      "metadata": {},
      "outputs": [],
      "source": [
        "OUT_CLEAR_PCA.mkdir(parents=True, exist_ok=True)\n",
        "meta_c_out = meta_c[meta_c['shot'].astype(int).isin(good_shots_c)] if not meta_c.empty else meta_c\n",
        "if not meta_c_out.empty:\n",
        "    meta_c_out.to_csv(OUT_CLEAR_PCA / 'meta.csv', index=False)\n",
        "for shot in tqdm([s for s in clear_lengths if s in good_shots_c], desc='Save clear PCA (SNR>3)'):\n",
        "    data = transform_shot_global(CLEAR_DECIMATED_ROOT, shot, clear_lengths[shot], mu, W_K)\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "print(f'Saved {len(good_shots_c)} clear shots (SNR>3) to {OUT_CLEAR_PCA}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sanity: shapes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f163e3a",
      "metadata": {},
      "outputs": [],
      "source": [
        "if good_shots_d:\n",
        "    shot0 = next(iter(good_shots_d))\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Disruptive PCA example: shot {shot0} LFS shape = {sh} (N_components, T)')\n",
        "if good_shots_c:\n",
        "    shot0 = next(iter(good_shots_c))\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Clear PCA example:      shot {shot0} LFS shape = {sh} (N_components, T)')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
