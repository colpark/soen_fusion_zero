{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PCA from decimated data (per time point)\n",
        "\n",
        "Starting from the **decimated** disruptive and clear shots, fit **PCA per time index** (across the 160 channels), keep the **top N** components, and save the reduced data as `*_pca` (e.g. `dsrpt_decimated_pca`, `clear_decimated_pca`).\n",
        "\n",
        "- At each time index `t`, we collect the 160-D vector from every shot that has length > t, fit PCA, keep top N.\n",
        "- Each shot is then transformed to shape `(N, T_shot)` and saved.\n",
        "- Requires decimated data to exist (run preprocessing_viz 4b first, or point to existing decimated dirs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "from pathlib import Path\n",
        "from sklearn.decomposition import PCA\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Paths — decimated inputs (disruptive + clear)\n",
        "DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/dsrpt_decimated')\n",
        "CLEAR_DECIMATED_ROOT = Path('/home/idies/workspace/Storage/yhuang2/persistent/ecei/clear_decimated')\n",
        "# Outputs (will be created)\n",
        "OUT_DSRPT_PCA = DECIMATED_ROOT.parent / 'dsrpt_decimated_pca'\n",
        "OUT_CLEAR_PCA = DECIMATED_ROOT.parent / 'clear_decimated_pca'\n",
        "\n",
        "N_COMPONENTS = 16   # top N PCA components per time point (1–160)\n",
        "CHANNELS = 20 * 8   # 160"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. List shots and their lengths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def get_shot_lengths(root: Path) -> dict[int, int]:\n",
        "    \"\"\"Return {shot_id: T} for each .h5 in root.\"\"\"\n",
        "    out = {}\n",
        "    for p in root.glob('*.h5'):\n",
        "        if not p.stem.isdigit():\n",
        "            continue\n",
        "        shot = int(p.stem)\n",
        "        with h5py.File(p, 'r') as f:\n",
        "            T = f['LFS'].shape[-1]\n",
        "        out[shot] = T\n",
        "    return out\n",
        "\n",
        "dsrpt_lengths = get_shot_lengths(DECIMATED_ROOT) if DECIMATED_ROOT.exists() else {}\n",
        "clear_lengths = get_shot_lengths(CLEAR_DECIMATED_ROOT) if CLEAR_DECIMATED_ROOT.exists() else {}\n",
        "print(f'Disruptive: {len(dsrpt_lengths)} shots')\n",
        "print(f'Clear:      {len(clear_lengths)} shots')\n",
        "if dsrpt_lengths:\n",
        "    Ts = list(dsrpt_lengths.values())\n",
        "    print(f'  Disruptive T: min={min(Ts)}, max={max(Ts)}')\n",
        "if clear_lengths:\n",
        "    Ts = list(clear_lengths.values())\n",
        "    print(f'  Clear T:      min={min(Ts)}, max={max(Ts)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Build (time_index -> list of 160-D vectors) for fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_channel_vector(root: Path, shot: int, t: int) -> np.ndarray:\n",
        "    \"\"\"Load the 160-D vector at time t for one shot. Shape (160,).\"\"\"\n",
        "    with h5py.File(root / f'{shot}.h5', 'r') as f:\n",
        "        x = np.asarray(f['LFS'][..., t], dtype=np.float32)\n",
        "    return x.ravel()  # (20,8) -> (160,)\n",
        "\n",
        "def collect_vectors_at_t(roots_with_shots: list[tuple[Path, list[tuple[int, int]]]], t: int) -> np.ndarray:\n",
        "    \"\"\"Collect (n, 160) array of all vectors at time t from given (root, [(shot, T), ...]).\"\"\"\n",
        "    rows = []\n",
        "    for root, shots_with_T in roots_with_shots:\n",
        "        for shot, T in shots_with_T:\n",
        "            if T <= t:\n",
        "                continue\n",
        "            rows.append(load_channel_vector(root, shot, t))\n",
        "    if not rows:\n",
        "        return np.empty((0, CHANNELS), dtype=np.float32)\n",
        "    return np.stack(rows, axis=0)\n",
        "\n",
        "# Pairs (root, [(shot, T), ...]) for fitting\n",
        "dsrpt_list = [(DECIMATED_ROOT, list(dsrpt_lengths.items()))] if dsrpt_lengths else []\n",
        "clear_list = [(CLEAR_DECIMATED_ROOT, list(clear_lengths.items()))] if clear_lengths else []\n",
        "all_roots_shots = dsrpt_list + clear_list\n",
        "max_T = max(\n",
        "    (max((T for _, T in s])) for _, s in all_roots_shots if s),\n",
        "    default=0\n",
        ")\n",
        "print(f'Max T over all shots: {max_T}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fit PCA at each time index (top N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3b. How many components for ≥99% variance?\n",
        "\n",
        "At a sample of time indices, fit PCA with **all** components and find the smallest number of components whose cumulative explained variance ≥ 99%. Use this to choose `N_COMPONENTS` if you want to retain 99% of the information."
      ],
      "id": "84126840"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_variance = 0.99\n",
        "# Sample time indices so we don't fit at every t (expensive)\n",
        "step = max(1, max_T // 50)   # ~50 time points, or every t if max_T < 50\n",
        "t_sample = list(range(0, max_T, step))\n",
        "\n",
        "n_for_99 = []\n",
        "for t in tqdm(t_sample, desc='99% variance check'):\n",
        "    X = collect_vectors_at_t(all_roots_shots, t)\n",
        "    if X.shape[0] < 2:\n",
        "        continue\n",
        "    n_full = min(X.shape[0], X.shape[1])\n",
        "    pca_full = PCA(n_components=n_full)\n",
        "    pca_full.fit(X)\n",
        "    cumvar = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "    k = int(np.searchsorted(cumvar, target_variance)) + 1\n",
        "    k = min(k, len(cumvar))\n",
        "    n_for_99.append(k)\n",
        "\n",
        "if n_for_99:\n",
        "    n_for_99 = np.array(n_for_99)\n",
        "    print(f'At {target_variance*100:.0f}% explained variance (over {len(t_sample)} time samples):')\n",
        "    print(f'  Components needed: min={n_for_99.min()}, max={n_for_99.max()}, mean={n_for_99.mean():.1f}, median={np.median(n_for_99):.0f}')\n",
        "    print(f'  → Consider N_COMPONENTS >= {int(np.ceil(n_for_99.max()))} to retain ≥99% at all sampled t.')\n",
        "    import matplotlib.pyplot as plt\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    ax.hist(n_for_99, bins=min(50, len(np.unique(n_for_99))), color='steelblue', edgecolor='white')\n",
        "    ax.axvline(np.median(n_for_99), color='red', linestyle='--', label=f'median={np.median(n_for_99):.0f}')\n",
        "    ax.set_xlabel('Number of components for ≥99% variance')\n",
        "    ax.set_ylabel('Count (time samples)')\n",
        "    ax.legend()\n",
        "    ax.set_title(f'Per-time-point PCA: components needed for ≥{target_variance*100:.0f}% variance')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('Not enough data to compute (need at least 2 samples at some t).')"
      ],
      "id": "c35148eb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_comp = min(N_COMPONENTS, CHANNELS)\n",
        "pca_per_t = []  # pca_per_t[t] = fitted PCA or None if too few samples\n",
        "\n",
        "for t in tqdm(range(max_T), desc='Fit PCA per t'):\n",
        "    X = collect_vectors_at_t(all_roots_shots, t)\n",
        "    if X.shape[0] < 2:\n",
        "        pca_per_t.append(None)\n",
        "        continue\n",
        "    n = min(n_comp, X.shape[0], X.shape[1])\n",
        "    pca = PCA(n_components=n)\n",
        "    pca.fit(X)\n",
        "    pca_per_t.append(pca)\n",
        "\n",
        "n_fitted = sum(1 for p in pca_per_t if p is not None)\n",
        "print(f'Fitted PCA at {n_fitted}/{max_T} time indices')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transform and save disruptive shots -> dsrpt_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def transform_shot(root: Path, shot: int, T: int, pca_per_t: list) -> np.ndarray:\n",
        "    \"\"\"Return (N, T) array: at each t, project 160-D to top N components.\"\"\"\n",
        "    N = N_COMPONENTS\n",
        "    out = np.zeros((N, T), dtype=np.float32)\n",
        "    for t in range(T):\n",
        "        pca = pca_per_t[t] if t < len(pca_per_t) else None\n",
        "        if pca is None:\n",
        "            out[:, t] = 0\n",
        "            continue\n",
        "        x = load_channel_vector(root, shot, t).reshape(1, -1)\n",
        "        out[:pca.n_components_, t] = pca.transform(x).ravel()\n",
        "        if pca.n_components_ < N:\n",
        "            out[pca.n_components_:, t] = 0\n",
        "    return out\n",
        "\n",
        "OUT_DSRPT_PCA.mkdir(parents=True, exist_ok=True)\n",
        "if (DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    import shutil\n",
        "    shutil.copy(DECIMATED_ROOT / 'meta.csv', OUT_DSRPT_PCA / 'meta.csv')\n",
        "\n",
        "for shot, T in tqdm(list(dsrpt_lengths.items()), desc='Save disruptive PCA'):\n",
        "    data = transform_shot(DECIMATED_ROOT, shot, T, pca_per_t)\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "\n",
        "print(f'Saved {len(dsrpt_lengths)} shots to {OUT_DSRPT_PCA}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transform and save clear shots -> clear_decimated_pca"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "OUT_CLEAR_PCA.mkdir(parents=True, exist_ok=True)\n",
        "if CLEAR_DECIMATED_ROOT.exists() and (CLEAR_DECIMATED_ROOT / 'meta.csv').exists():\n",
        "    import shutil\n",
        "    shutil.copy(CLEAR_DECIMATED_ROOT / 'meta.csv', OUT_CLEAR_PCA / 'meta.csv')\n",
        "\n",
        "for shot, T in tqdm(list(clear_lengths.items()), desc='Save clear PCA'):\n",
        "    data = transform_shot(CLEAR_DECIMATED_ROOT, shot, T, pca_per_t)\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot}.h5', 'w') as f:\n",
        "        f.create_dataset('LFS', data=data, dtype=np.float32)\n",
        "\n",
        "print(f'Saved {len(clear_lengths)} shots to {OUT_CLEAR_PCA}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Sanity: shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if dsrpt_lengths:\n",
        "    shot0 = list(dsrpt_lengths.keys())[0]\n",
        "    with h5py.File(OUT_DSRPT_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Disruptive PCA example: shot {shot0} LFS shape = {sh} (N_components, T)')\n",
        "if clear_lengths:\n",
        "    shot0 = list(clear_lengths.keys())[0]\n",
        "    with h5py.File(OUT_CLEAR_PCA / f'{shot0}.h5', 'r') as f:\n",
        "        sh = f['LFS'].shape\n",
        "    print(f'Clear PCA example:      shot {shot0} LFS shape = {sh} (N_components, T)')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}