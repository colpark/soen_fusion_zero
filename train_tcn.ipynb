{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# TCN Disruption Prediction — Training\n",
    "\n",
    "Per-timestep binary classification using a Temporal Convolutional Network.\n",
    "Adapted from [disruptcnn](https://github.com/rmchurch/disruptcnn) (Churchill et al. 2019).\n",
    "\n",
    "Uses `ECEiTCNDataset` with pre-decimated data for fast I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "from dataset_ecei_tcn import ECEiTCNDataset, create_loaders\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-config",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Data ──────────────────────────────────────────────────────────────\n",
    "ROOT           = '/global/cfs/cdirs/m5187/proj-share/ECEi_excerpt/dsrpt'\n",
    "DECIMATED_ROOT = '/global/cfs/cdirs/m5187/proj-share/ECEi_excerpt/dsrpt_decimated'\n",
    "\n",
    "DATA_STEP       = 10\n",
    "TWARN           = 300_000      # 300 ms at 1 MHz\n",
    "BASELINE_LEN    = 40_000       # 40 ms (matches disruptcnn)\n",
    "NSUB            = 781_250      # ~781 ms (matches disruptcnn)\n",
    "STRIDE          = 481_260      # overlap by receptive field\n",
    "\n",
    "BATCH_SIZE      = 12\n",
    "NUM_WORKERS     = 4\n",
    "\n",
    "# ── Model (matches disruptcnn run.sh) ─────────────────────────────────\n",
    "INPUT_CHANNELS  = 160          # 20 × 8 flattened\n",
    "N_CLASSES       = 1            # binary per-timestep\n",
    "LEVELS          = 4\n",
    "NHID            = 80           # hidden channels per level\n",
    "KERNEL_SIZE     = 15\n",
    "DILATION_BASE   = 10\n",
    "DROPOUT         = 0.1\n",
    "\n",
    "# ── Training ──────────────────────────────────────────────────────────\n",
    "EPOCHS          = 50\n",
    "LR              = 2e-3\n",
    "CLIP            = 0.3\n",
    "WARMUP_EPOCHS   = 3\n",
    "WARMUP_FACTOR   = 8            # start LR = LR / WARMUP_FACTOR\n",
    "\n",
    "CHECKPOINT_DIR  = Path('checkpoints_tcn')\n",
    "CHECKPOINT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-model",
   "metadata": {},
   "source": [
    "## 2. TCN Model\n",
    "\n",
    "Temporal Convolutional Network from [Bai et al. 2018](https://arxiv.org/abs/1803.01271),\n",
    "with modifications for arbitrary dilation factors (from disruptcnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcn-modules",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Remove trailing padding to enforce causality.\"\"\"\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(\n",
    "            n_inputs, n_outputs, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(\n",
    "            n_outputs, n_outputs, kernel_size,\n",
    "            stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "            self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, dilation_size=2,\n",
    "                 kernel_size=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        if np.isscalar(dilation_size):\n",
    "            dilation_size = [dilation_size ** i for i in range(num_levels)]\n",
    "        for i in range(num_levels):\n",
    "            in_ch = num_inputs if i == 0 else num_channels[i - 1]\n",
    "            out_ch = num_channels[i]\n",
    "            layers.append(TemporalBlock(\n",
    "                in_ch, out_ch, kernel_size, stride=1,\n",
    "                padding=(kernel_size - 1) * dilation_size[i],\n",
    "                dilation=dilation_size[i], dropout=dropout))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels,\n",
    "                 kernel_size, dropout, dilation_size):\n",
    "        super().__init__()\n",
    "        self.tcn = TemporalConvNet(\n",
    "            input_size, num_channels,\n",
    "            kernel_size=kernel_size, dropout=dropout,\n",
    "            dilation_size=dilation_size)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (N, C_in, L) → output: (N, L)\"\"\"\n",
    "        y = self.tcn(x)                          # (N, C_hid, L)\n",
    "        o = self.linear(y.permute(0, 2, 1))      # (N, L, 1)\n",
    "        return torch.sigmoid(o.squeeze(-1))       # (N, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-build",
   "metadata": {},
   "source": [
    "## 3. Build model & compute receptive field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_receptive_field(kernel_size, dilation_sizes):\n",
    "    \"\"\"Receptive field length of a TCN.\"\"\"\n",
    "    return 1 + 2 * (kernel_size - 1) * int(np.sum(dilation_sizes))\n",
    "\n",
    "\n",
    "def build_model(input_channels, n_classes, levels, nhid,\n",
    "                kernel_size, dilation_base, dropout, nrecept_target=30_000):\n",
    "    \"\"\"Build TCN and compute actual receptive field (matches disruptcnn logic).\"\"\"\n",
    "    channel_sizes = [nhid] * levels\n",
    "\n",
    "    # adjust last-level dilation so receptive field ≈ nrecept_target\n",
    "    base_dilations = [dilation_base ** i for i in range(levels - 1)]\n",
    "    rf_without_last = calc_receptive_field(kernel_size, base_dilations)\n",
    "    last_dilation = int(np.ceil(\n",
    "        (nrecept_target - rf_without_last) / (2.0 * (kernel_size - 1))))\n",
    "    last_dilation = max(last_dilation, 1)\n",
    "    dilation_sizes = base_dilations + [last_dilation]\n",
    "\n",
    "    nrecept = calc_receptive_field(kernel_size, dilation_sizes)\n",
    "\n",
    "    model = TCN(input_channels, n_classes, channel_sizes,\n",
    "                kernel_size=kernel_size, dropout=dropout,\n",
    "                dilation_size=dilation_sizes)\n",
    "\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'Dilation sizes : {dilation_sizes}')\n",
    "    print(f'Receptive field: {nrecept:,} samples '\n",
    "          f'({nrecept / (1e6 / 10) * 1e3:.1f} ms at 100 kHz)')\n",
    "    print(f'Parameters     : {n_params:,}')\n",
    "    return model, nrecept\n",
    "\n",
    "\n",
    "model, NRECEPT = build_model(\n",
    "    INPUT_CHANNELS, N_CLASSES, LEVELS, NHID,\n",
    "    KERNEL_SIZE, DILATION_BASE, DROPOUT,\n",
    "    nrecept_target=30_000,\n",
    ")\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-data",
   "metadata": {},
   "source": [
    "## 4. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ECEiTCNDataset(\n",
    "    root            = ROOT,\n",
    "    decimated_root  = DECIMATED_ROOT,\n",
    "    Twarn           = TWARN,\n",
    "    baseline_length = BASELINE_LEN,\n",
    "    data_step       = DATA_STEP,\n",
    "    nsub            = NSUB,\n",
    "    stride          = STRIDE,\n",
    "    normalize       = True,\n",
    ")\n",
    "ds.summary()\n",
    "\n",
    "loaders = create_loaders(ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "for name, loader in loaders.items():\n",
    "    print(f'  {name:>5s}: {len(loader.dataset)} subseqs, {len(loader)} batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-train",
   "metadata": {},
   "source": [
    "## 5. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, nrecept, device, clip=0.3):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    for X, target, weight in loader:\n",
    "        # X: (B, 20, 8, T) → (B, 160, T)\n",
    "        B = X.shape[0]\n",
    "        X = X.view(B, -1, X.shape[-1]).to(device)\n",
    "        target = target.to(device)\n",
    "        weight = weight.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)                         # (B, T)\n",
    "\n",
    "        # loss only on valid region (after receptive field)\n",
    "        out_v = output[:, nrecept - 1:]\n",
    "        tgt_v = target[:, nrecept - 1:]\n",
    "        wgt_v = weight[:, nrecept - 1:]\n",
    "\n",
    "        loss = F.binary_cross_entropy(out_v, tgt_v, weight=wgt_v)\n",
    "        loss.backward()\n",
    "\n",
    "        if clip > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return total_loss / max(n_batches, 1)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, nrecept, device, thresholds=None):\n",
    "    \"\"\"Compute loss, accuracy, and F1 over a loader.\"\"\"\n",
    "    model.eval()\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.05, 0.95, 19)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n_batches = 0\n",
    "    total = 0\n",
    "    TPs = np.zeros(len(thresholds))\n",
    "    TNs = np.zeros(len(thresholds))\n",
    "    FPs = np.zeros(len(thresholds))\n",
    "    FNs = np.zeros(len(thresholds))\n",
    "\n",
    "    for X, target, weight in loader:\n",
    "        B = X.shape[0]\n",
    "        X = X.view(B, -1, X.shape[-1]).to(device)\n",
    "        target = target.to(device)\n",
    "        weight = weight.to(device)\n",
    "\n",
    "        output = model(X)\n",
    "        out_v = output[:, nrecept - 1:]\n",
    "        tgt_v = target[:, nrecept - 1:]\n",
    "        wgt_v = weight[:, nrecept - 1:]\n",
    "\n",
    "        loss = F.binary_cross_entropy(out_v, tgt_v, weight=wgt_v)\n",
    "        total_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        total += tgt_v.numel()\n",
    "\n",
    "        for i, th in enumerate(thresholds):\n",
    "            pred = (out_v >= th).float()\n",
    "            TPs[i] += ((pred == 1) & (tgt_v == 1)).sum().item()\n",
    "            TNs[i] += ((pred == 0) & (tgt_v == 0)).sum().item()\n",
    "            FPs[i] += ((pred == 1) & (tgt_v == 0)).sum().item()\n",
    "            FNs[i] += ((pred == 0) & (tgt_v == 1)).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / max(n_batches, 1)\n",
    "\n",
    "    # best F1 across thresholds\n",
    "    precision = TPs / (TPs + FPs + 1e-10)\n",
    "    recall    = TPs / (TPs + FNs + 1e-10)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
    "    best_idx = np.argmax(f1)\n",
    "    accuracy  = (TPs[best_idx] + TNs[best_idx]) / max(total, 1)\n",
    "\n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1[best_idx],\n",
    "        'precision': precision[best_idx],\n",
    "        'recall': recall[best_idx],\n",
    "        'threshold': thresholds[best_idx],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, nesterov=True)\n",
    "\n",
    "# linear warmup then ReduceLROnPlateau\n",
    "warmup_iters = WARMUP_EPOCHS * len(loaders.get('train', list(loaders.values())[0]))\n",
    "warmup_lambda = lambda it: (1 - 1/WARMUP_FACTOR) / max(warmup_iters, 1) * it + 1/WARMUP_FACTOR\n",
    "scheduler_warmup  = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
    "\n",
    "train_split = 'train' if 'train' in loaders else list(loaders.keys())[0]\n",
    "val_split   = 'test'  if 'test'  in loaders else train_split\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': [], 'val_f1': [], 'val_acc': [], 'lr': []}\n",
    "best_f1 = 0.0\n",
    "iteration = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    # ── train ──\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for X, target, weight in loaders[train_split]:\n",
    "        B = X.shape[0]\n",
    "        X = X.view(B, -1, X.shape[-1]).to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        weight = weight.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X)\n",
    "        out_v = output[:, NRECEPT - 1:]\n",
    "        tgt_v = target[:, NRECEPT - 1:]\n",
    "        wgt_v = weight[:, NRECEPT - 1:]\n",
    "        loss = F.binary_cross_entropy(out_v, tgt_v, weight=wgt_v)\n",
    "        loss.backward()\n",
    "        if CLIP > 0:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        n_batches += 1\n",
    "        iteration += 1\n",
    "\n",
    "        # warmup scheduler steps per iteration\n",
    "        if iteration <= warmup_iters:\n",
    "            scheduler_warmup.step(iteration)\n",
    "\n",
    "    train_loss = epoch_loss / max(n_batches, 1)\n",
    "\n",
    "    # ── validate ──\n",
    "    val_metrics = evaluate(model, loaders[val_split], NRECEPT, DEVICE)\n",
    "\n",
    "    # plateau scheduler steps per epoch on val loss\n",
    "    if iteration > warmup_iters:\n",
    "        scheduler_plateau.step(val_metrics['loss'])\n",
    "\n",
    "    lr_now = optimizer.param_groups[0]['lr']\n",
    "    elapsed = time.perf_counter() - t0\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_metrics['loss'])\n",
    "    history['val_f1'].append(val_metrics['f1'])\n",
    "    history['val_acc'].append(val_metrics['accuracy'])\n",
    "    history['lr'].append(lr_now)\n",
    "\n",
    "    # ── checkpoint ──\n",
    "    is_best = val_metrics['f1'] > best_f1\n",
    "    if is_best:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'best_f1': best_f1,\n",
    "            'threshold': val_metrics['threshold'],\n",
    "            'nrecept': NRECEPT,\n",
    "        }, CHECKPOINT_DIR / 'best.pt')\n",
    "\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'best_f1': best_f1,\n",
    "        'nrecept': NRECEPT,\n",
    "    }, CHECKPOINT_DIR / 'last.pt')\n",
    "\n",
    "    star = ' *' if is_best else ''\n",
    "    print(f'Epoch {epoch:3d}/{EPOCHS}  '\n",
    "          f'train_loss={train_loss:.4e}  '\n",
    "          f'val_loss={val_metrics[\"loss\"]:.4e}  '\n",
    "          f'acc={val_metrics[\"accuracy\"]:.4f}  '\n",
    "          f'F1={val_metrics[\"f1\"]:.4f} (th={val_metrics[\"threshold\"]:.2f})  '\n",
    "          f'lr={lr_now:.2e}  '\n",
    "          f'{elapsed:.1f}s{star}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-curves",
   "metadata": {},
   "source": [
    "## 6. Training curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curves",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "epochs_range = np.arange(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(epochs_range, history['train_loss'], label='Train')\n",
    "axes[0].plot(epochs_range, history['val_loss'], label='Val')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('BCE Loss')\n",
    "axes[0].set_title('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# F1 & Accuracy\n",
    "axes[1].plot(epochs_range, history['val_f1'], label='F1', color='firebrick')\n",
    "axes[1].plot(epochs_range, history['val_acc'], label='Accuracy', color='steelblue')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_title('Validation F1 & Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0, 1)\n",
    "\n",
    "# Learning rate\n",
    "axes[2].plot(epochs_range, history['lr'], color='darkorange')\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('LR')\n",
    "axes[2].set_title('Learning Rate')\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Best val F1 = {best_f1:.4f}', fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-viz",
   "metadata": {},
   "source": [
    "## 7. Visualise predictions on validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "viz-pred",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "ckpt = torch.load(CHECKPOINT_DIR / 'best.pt', map_location=DEVICE)\n",
    "model.load_state_dict(ckpt['state_dict'])\n",
    "best_th = ckpt.get('threshold', 0.5)\n",
    "print(f'Loaded best checkpoint (epoch {ckpt[\"epoch\"]}, F1={ckpt[\"best_f1\"]:.4f}, th={best_th:.2f})')\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# grab a batch from validation\n",
    "val_loader = loaders[val_split]\n",
    "X_b, target_b, weight_b = next(iter(val_loader))\n",
    "\n",
    "B = X_b.shape[0]\n",
    "X_flat = X_b.view(B, -1, X_b.shape[-1]).to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_b = model(X_flat).cpu().numpy()\n",
    "\n",
    "target_np = target_b.numpy()\n",
    "T_sub = pred_b.shape[-1]\n",
    "t_ax_ms = np.arange(T_sub) / (1e6 / DATA_STEP / 1000)  # ms\n",
    "\n",
    "n_show = min(4, B)\n",
    "fig, axes = plt.subplots(n_show, 1, figsize=(16, 3 * n_show), sharex=True)\n",
    "if n_show == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(t_ax_ms, pred_b[i], color='steelblue', linewidth=1, label='Prediction')\n",
    "    ax.plot(t_ax_ms, target_np[i], color='firebrick', linewidth=1, linestyle='--', label='Target')\n",
    "    ax.axhline(best_th, color='gray', linestyle=':', alpha=0.5, label=f'Threshold={best_th:.2f}')\n",
    "    ax.axvspan(0, t_ax_ms[NRECEPT - 1], alpha=0.08, color='gray')\n",
    "    ax.set_ylabel(f'Sample {i}')\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    if i == 0:\n",
    "        ax.legend(loc='upper left', fontsize=9)\n",
    "        ax.set_title('TCN predictions vs targets (gray = receptive field, not in loss)')\n",
    "\n",
    "axes[-1].set_xlabel('Time (ms) within subsequence')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
