{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TCN Disruption Prediction — Training\n",
        "\n",
        "Per-timestep binary classification using a Temporal Convolutional Network.\n",
        "Adapted from [disruptcnn](https://github.com/rmchurch/disruptcnn) (Churchill et al. 2019).\n",
        "\n",
        "Uses `ECEiTCNDataset` with pre-decimated data for fast I/O."
      ],
      "id": "title"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "from dataset_ecei_tcn import ECEiTCNDataset, create_loaders\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {DEVICE}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "imports"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ],
      "id": "sec-config"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ── Data ──────────────────────────────────────────────────────────────\n",
        "ROOT           = '/global/cfs/cdirs/m5187/proj-share/ECEi_excerpt/dsrpt'\n",
        "DECIMATED_ROOT = '/global/cfs/cdirs/m5187/proj-share/ECEi_excerpt/dsrpt_decimated'\n",
        "\n",
        "DATA_STEP       = 10\n",
        "TWARN           = 300_000      # 300 ms at 1 MHz\n",
        "BASELINE_LEN    = 40_000       # 40 ms (matches disruptcnn)\n",
        "NSUB            = 781_250      # ~781 ms (matches disruptcnn)\n",
        "STRIDE          = 481_260      # overlap by receptive field\n",
        "\n",
        "BATCH_SIZE      = 12\n",
        "NUM_WORKERS     = 4\n",
        "\n",
        "# ── Model (matches disruptcnn run.sh) ─────────────────────────────────\n",
        "INPUT_CHANNELS  = 160          # 20 × 8 flattened\n",
        "N_CLASSES       = 1            # binary per-timestep\n",
        "LEVELS          = 4\n",
        "NHID            = 80           # hidden channels per level\n",
        "KERNEL_SIZE     = 15\n",
        "DILATION_BASE   = 10\n",
        "DROPOUT         = 0.1\n",
        "\n",
        "# ── Training ──────────────────────────────────────────────────────────\n",
        "EPOCHS          = 50\n",
        "LR              = 2e-3\n",
        "CLIP            = 0.3\n",
        "WARMUP_EPOCHS   = 3\n",
        "WARMUP_FACTOR   = 8            # start LR = LR / WARMUP_FACTOR\n",
        "\n",
        "CHECKPOINT_DIR  = Path('checkpoints_tcn')\n",
        "CHECKPOINT_DIR.mkdir(exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "config"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. TCN Model\n",
        "\n",
        "Temporal Convolutional Network from [Bai et al. 2018](https://arxiv.org/abs/1803.01271),\n",
        "with modifications for arbitrary dilation factors (from disruptcnn)."
      ],
      "id": "sec-model"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Chomp1d(nn.Module):\n",
        "    \"\"\"Remove trailing padding to enforce causality.\"\"\"\n",
        "    def __init__(self, chomp_size):\n",
        "        super().__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(\n",
        "            n_inputs, n_outputs, kernel_size,\n",
        "            stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(\n",
        "            n_outputs, n_outputs, kernel_size,\n",
        "            stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "            self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        self.conv1.weight.data.normal_(0, 0.01)\n",
        "        self.conv2.weight.data.normal_(0, 0.01)\n",
        "        if self.downsample is not None:\n",
        "            self.downsample.weight.data.normal_(0, 0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, dilation_size=2,\n",
        "                 kernel_size=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        if np.isscalar(dilation_size):\n",
        "            dilation_size = [dilation_size ** i for i in range(num_levels)]\n",
        "        for i in range(num_levels):\n",
        "            in_ch = num_inputs if i == 0 else num_channels[i - 1]\n",
        "            out_ch = num_channels[i]\n",
        "            layers.append(TemporalBlock(\n",
        "                in_ch, out_ch, kernel_size, stride=1,\n",
        "                padding=(kernel_size - 1) * dilation_size[i],\n",
        "                dilation=dilation_size[i], dropout=dropout))\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels,\n",
        "                 kernel_size, dropout, dilation_size):\n",
        "        super().__init__()\n",
        "        self.tcn = TemporalConvNet(\n",
        "            input_size, num_channels,\n",
        "            kernel_size=kernel_size, dropout=dropout,\n",
        "            dilation_size=dilation_size)\n",
        "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"x: (N, C_in, L) → output: (N, L)\"\"\"\n",
        "        y = self.tcn(x)                          # (N, C_hid, L)\n",
        "        o = self.linear(y.permute(0, 2, 1))      # (N, L, 1)\n",
        "        return torch.sigmoid(o.squeeze(-1))       # (N, L)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "tcn-modules"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Build model & compute receptive field"
      ],
      "id": "sec-build"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def calc_receptive_field(kernel_size, dilation_sizes):\n",
        "    \"\"\"Receptive field length of a TCN.\"\"\"\n",
        "    return 1 + 2 * (kernel_size - 1) * int(np.sum(dilation_sizes))\n",
        "\n",
        "\n",
        "def build_model(input_channels, n_classes, levels, nhid,\n",
        "                kernel_size, dilation_base, dropout, nrecept_target=30_000):\n",
        "    \"\"\"Build TCN and compute actual receptive field (matches disruptcnn logic).\"\"\"\n",
        "    channel_sizes = [nhid] * levels\n",
        "\n",
        "    # adjust last-level dilation so receptive field ≈ nrecept_target\n",
        "    base_dilations = [dilation_base ** i for i in range(levels - 1)]\n",
        "    rf_without_last = calc_receptive_field(kernel_size, base_dilations)\n",
        "    last_dilation = int(np.ceil(\n",
        "        (nrecept_target - rf_without_last) / (2.0 * (kernel_size - 1))))\n",
        "    last_dilation = max(last_dilation, 1)\n",
        "    dilation_sizes = base_dilations + [last_dilation]\n",
        "\n",
        "    nrecept = calc_receptive_field(kernel_size, dilation_sizes)\n",
        "\n",
        "    model = TCN(input_channels, n_classes, channel_sizes,\n",
        "                kernel_size=kernel_size, dropout=dropout,\n",
        "                dilation_size=dilation_sizes)\n",
        "\n",
        "    n_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f'Dilation sizes : {dilation_sizes}')\n",
        "    print(f'Receptive field: {nrecept:,} samples '\n",
        "          f'({nrecept / (1e6 / 10) * 1e3:.1f} ms at 100 kHz)')\n",
        "    print(f'Parameters     : {n_params:,}')\n",
        "    return model, nrecept\n",
        "\n",
        "\n",
        "model, NRECEPT = build_model(\n",
        "    INPUT_CHANNELS, N_CLASSES, LEVELS, NHID,\n",
        "    KERNEL_SIZE, DILATION_BASE, DROPOUT,\n",
        "    nrecept_target=30_000,\n",
        ")\n",
        "model = model.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "build-model"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data"
      ],
      "id": "sec-data"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ds = ECEiTCNDataset(\n",
        "    root            = ROOT,\n",
        "    decimated_root  = DECIMATED_ROOT,\n",
        "    Twarn           = TWARN,\n",
        "    baseline_length = BASELINE_LEN,\n",
        "    data_step       = DATA_STEP,\n",
        "    nsub            = NSUB,\n",
        "    stride          = STRIDE,\n",
        "    normalize       = True,\n",
        ")\n",
        "ds.summary()\n",
        "\n",
        "loaders = create_loaders(ds, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
        "for name, loader in loaders.items():\n",
        "    print(f'  {name:>5s}: {len(loader.dataset)} subseqs, {len(loader)} batches')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training loop"
      ],
      "id": "sec-train"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LOG_EVERY_N_BATCHES = 5   # print a line every N batches during training\n",
        "\n",
        "\n",
        "def _grad_norm(model):\n",
        "    \"\"\"Total L2 norm of all gradients.\"\"\"\n",
        "    total = 0.0\n",
        "    for p in model.parameters():\n",
        "        if p.grad is not None:\n",
        "            total += p.grad.data.norm(2).item() ** 2\n",
        "    return total ** 0.5\n",
        "\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, nrecept, device, epoch,\n",
        "                    n_epochs, clip=0.3, log_every=LOG_EVERY_N_BATCHES):\n",
        "    \"\"\"Train for one epoch with verbose per-batch logging.\"\"\"\n",
        "    model.train()\n",
        "    n_batches = len(loader)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    running_pos = 0\n",
        "    batch_losses = []\n",
        "\n",
        "    pbar = tqdm(enumerate(loader), total=n_batches,\n",
        "                desc=f'Epoch {epoch}/{n_epochs} [train]',\n",
        "                leave=True, dynamic_ncols=True)\n",
        "\n",
        "    for batch_idx, (X, target, weight) in pbar:\n",
        "        B = X.shape[0]\n",
        "        X = X.view(B, -1, X.shape[-1]).to(device)\n",
        "        target = target.to(device)\n",
        "        weight = weight.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X)\n",
        "\n",
        "        # loss only on valid region (after receptive field)\n",
        "        out_v = output[:, nrecept - 1:]\n",
        "        tgt_v = target[:, nrecept - 1:]\n",
        "        wgt_v = weight[:, nrecept - 1:]\n",
        "\n",
        "        loss = F.binary_cross_entropy(out_v, tgt_v, weight=wgt_v)\n",
        "        loss.backward()\n",
        "\n",
        "        grad_norm_before = _grad_norm(model)\n",
        "        if clip > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        grad_norm_after = _grad_norm(model)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # ── accumulate stats ──\n",
        "        batch_loss = loss.item()\n",
        "        batch_losses.append(batch_loss)\n",
        "        running_loss += batch_loss\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = (out_v >= 0.5).float()\n",
        "            running_correct += (pred == tgt_v).sum().item()\n",
        "            running_total += tgt_v.numel()\n",
        "            running_pos += tgt_v.sum().item()\n",
        "\n",
        "        # ── update progress bar ──\n",
        "        avg_loss = running_loss / (batch_idx + 1)\n",
        "        avg_acc = running_correct / max(running_total, 1)\n",
        "        pos_frac = running_pos / max(running_total, 1)\n",
        "        lr_now = optimizer.param_groups[0]['lr']\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{avg_loss:.4e}',\n",
        "            'acc': f'{avg_acc:.3f}',\n",
        "            'pos%': f'{pos_frac:.3f}',\n",
        "            'lr': f'{lr_now:.2e}',\n",
        "        })\n",
        "\n",
        "        # ── detailed log every N batches ──\n",
        "        if (batch_idx + 1) % log_every == 0 or (batch_idx + 1) == n_batches:\n",
        "            tqdm.write(\n",
        "                f'  [{epoch}/{n_epochs}] '\n",
        "                f'batch {batch_idx+1:>4d}/{n_batches}  '\n",
        "                f'loss={batch_loss:.4e}  '\n",
        "                f'avg_loss={avg_loss:.4e}  '\n",
        "                f'acc={avg_acc:.4f}  '\n",
        "                f'pos%={pos_frac:.3f}  '\n",
        "                f'|grad|={grad_norm_before:.3f}->{grad_norm_after:.3f}  '\n",
        "                f'lr={lr_now:.2e}'\n",
        "            )\n",
        "\n",
        "    epoch_loss = running_loss / max(n_batches, 1)\n",
        "    epoch_acc = running_correct / max(running_total, 1)\n",
        "    return {\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': epoch_acc,\n",
        "        'pos_frac': running_pos / max(running_total, 1),\n",
        "        'batch_losses': batch_losses,\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, nrecept, device, epoch=0, n_epochs=0,\n",
        "             split_name='val', thresholds=None):\n",
        "    \"\"\"Compute loss, accuracy, and F1 over a loader with verbose output.\"\"\"\n",
        "    model.eval()\n",
        "    if thresholds is None:\n",
        "        thresholds = np.linspace(0.05, 0.95, 19)\n",
        "\n",
        "    total_loss = 0.0\n",
        "    n_batches = len(loader)\n",
        "    total = 0\n",
        "    correct_50 = 0\n",
        "    total_pos = 0\n",
        "    TPs = np.zeros(len(thresholds))\n",
        "    TNs = np.zeros(len(thresholds))\n",
        "    FPs = np.zeros(len(thresholds))\n",
        "    FNs = np.zeros(len(thresholds))\n",
        "\n",
        "    pbar = tqdm(enumerate(loader), total=n_batches,\n",
        "                desc=f'Epoch {epoch}/{n_epochs} [{split_name}]',\n",
        "                leave=True, dynamic_ncols=True)\n",
        "\n",
        "    for batch_idx, (X, target, weight) in pbar:\n",
        "        B = X.shape[0]\n",
        "        X = X.view(B, -1, X.shape[-1]).to(device)\n",
        "        target = target.to(device)\n",
        "        weight = weight.to(device)\n",
        "\n",
        "        output = model(X)\n",
        "        out_v = output[:, nrecept - 1:]\n",
        "        tgt_v = target[:, nrecept - 1:]\n",
        "        wgt_v = weight[:, nrecept - 1:]\n",
        "\n",
        "        loss = F.binary_cross_entropy(out_v, tgt_v, weight=wgt_v)\n",
        "        total_loss += loss.item()\n",
        "        total += tgt_v.numel()\n",
        "        total_pos += tgt_v.sum().item()\n",
        "\n",
        "        pred_50 = (out_v >= 0.5).float()\n",
        "        correct_50 += (pred_50 == tgt_v).sum().item()\n",
        "\n",
        "        for i, th in enumerate(thresholds):\n",
        "            pred = (out_v >= th).float()\n",
        "            TPs[i] += ((pred == 1) & (tgt_v == 1)).sum().item()\n",
        "            TNs[i] += ((pred == 0) & (tgt_v == 0)).sum().item()\n",
        "            FPs[i] += ((pred == 1) & (tgt_v == 0)).sum().item()\n",
        "            FNs[i] += ((pred == 0) & (tgt_v == 1)).sum().item()\n",
        "\n",
        "        avg_loss = total_loss / (batch_idx + 1)\n",
        "        avg_acc  = correct_50 / max(total, 1)\n",
        "        pbar.set_postfix({'loss': f'{avg_loss:.4e}', 'acc@0.5': f'{avg_acc:.3f}'})\n",
        "\n",
        "    avg_loss = total_loss / max(n_batches, 1)\n",
        "\n",
        "    # best F1 across thresholds\n",
        "    precision = TPs / (TPs + FPs + 1e-10)\n",
        "    recall    = TPs / (TPs + FNs + 1e-10)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-10)\n",
        "    best_idx = np.argmax(f1)\n",
        "    accuracy  = (TPs[best_idx] + TNs[best_idx]) / max(total, 1)\n",
        "\n",
        "    metrics = {\n",
        "        'loss': avg_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'acc_at_50': correct_50 / max(total, 1),\n",
        "        'f1': f1[best_idx],\n",
        "        'precision': precision[best_idx],\n",
        "        'recall': recall[best_idx],\n",
        "        'threshold': thresholds[best_idx],\n",
        "        'pos_frac': total_pos / max(total, 1),\n",
        "        'n_timesteps': total,\n",
        "    }\n",
        "\n",
        "    tqdm.write(\n",
        "        f'  [{split_name}] '\n",
        "        f'loss={avg_loss:.4e}  '\n",
        "        f'acc@best_th={accuracy:.4f}  acc@0.5={metrics[\"acc_at_50\"]:.4f}  '\n",
        "        f'F1={f1[best_idx]:.4f}  P={precision[best_idx]:.4f}  R={recall[best_idx]:.4f}  '\n",
        "        f'th={thresholds[best_idx]:.2f}  '\n",
        "        f'pos%={metrics[\"pos_frac\"]:.3f}  '\n",
        "        f'n_ts={total:,}'\n",
        "    )\n",
        "    return metrics"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "train-helpers"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0.9, nesterov=True)\n",
        "\n",
        "# linear warmup then ReduceLROnPlateau\n",
        "train_split = 'train' if 'train' in loaders else list(loaders.keys())[0]\n",
        "val_split   = 'test'  if 'test'  in loaders else train_split\n",
        "\n",
        "warmup_iters = WARMUP_EPOCHS * len(loaders[train_split])\n",
        "warmup_lambda = lambda it: (1 - 1/WARMUP_FACTOR) / max(warmup_iters, 1) * it + 1/WARMUP_FACTOR\n",
        "scheduler_warmup  = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
        "scheduler_plateau = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5)\n",
        "\n",
        "# ── history dict ──\n",
        "history = {\n",
        "    'train_loss': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_f1': [], 'val_acc': [], 'val_precision': [],\n",
        "    'val_recall': [], 'val_threshold': [], 'lr': [],\n",
        "}\n",
        "best_f1 = 0.0\n",
        "global_step = 0\n",
        "\n",
        "# ── pretty header ──\n",
        "print('=' * 110)\n",
        "print(f'  Training config:  {EPOCHS} epochs | batch={BATCH_SIZE} | '\n",
        "      f'lr={LR} | clip={CLIP} | warmup={WARMUP_EPOCHS} ep')\n",
        "print(f'  Splits:  train={len(loaders[train_split].dataset)} subseqs '\n",
        "      f'({len(loaders[train_split])} batches) | '\n",
        "      f'val={len(loaders[val_split].dataset)} subseqs '\n",
        "      f'({len(loaders[val_split])} batches)')\n",
        "print(f'  Receptive field:  {NRECEPT:,} samples')\n",
        "print(f'  Device: {DEVICE}')\n",
        "print('=' * 110)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    t0 = time.perf_counter()\n",
        "    print(f'\\n{\"─\" * 110}')\n",
        "    print(f'  EPOCH {epoch}/{EPOCHS}   (global step = {global_step:,},'\n",
        "          f'  lr = {optimizer.param_groups[0][\"lr\"]:.2e})')\n",
        "    print(f'{\"─\" * 110}')\n",
        "\n",
        "    # ═══════════════════════════════════ TRAIN ═══════════════════════════\n",
        "    train_metrics = train_one_epoch(\n",
        "        model, loaders[train_split], optimizer, NRECEPT, DEVICE,\n",
        "        epoch=epoch, n_epochs=EPOCHS, clip=CLIP,\n",
        "    )\n",
        "    global_step += len(loaders[train_split])\n",
        "\n",
        "    # warmup scheduler steps (per iteration inside train_one_epoch above\n",
        "    # is the typical approach but we keep it simple: step once per batch)\n",
        "    if global_step <= warmup_iters:\n",
        "        scheduler_warmup.step(global_step)\n",
        "\n",
        "    # ═══════════════════════════════════ VALIDATE ═══════════════════════\n",
        "    val_metrics = evaluate(\n",
        "        model, loaders[val_split], NRECEPT, DEVICE,\n",
        "        epoch=epoch, n_epochs=EPOCHS, split_name='val',\n",
        "    )\n",
        "\n",
        "    if global_step > warmup_iters:\n",
        "        scheduler_plateau.step(val_metrics['loss'])\n",
        "\n",
        "    lr_now = optimizer.param_groups[0]['lr']\n",
        "    elapsed = time.perf_counter() - t0\n",
        "\n",
        "    # ── record history ──\n",
        "    history['train_loss'].append(train_metrics['loss'])\n",
        "    history['train_acc'].append(train_metrics['accuracy'])\n",
        "    history['val_loss'].append(val_metrics['loss'])\n",
        "    history['val_f1'].append(val_metrics['f1'])\n",
        "    history['val_acc'].append(val_metrics['accuracy'])\n",
        "    history['val_precision'].append(val_metrics['precision'])\n",
        "    history['val_recall'].append(val_metrics['recall'])\n",
        "    history['val_threshold'].append(val_metrics['threshold'])\n",
        "    history['lr'].append(lr_now)\n",
        "\n",
        "    # ── checkpoint ──\n",
        "    is_best = val_metrics['f1'] > best_f1\n",
        "    if is_best:\n",
        "        best_f1 = val_metrics['f1']\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "            'best_f1': best_f1,\n",
        "            'threshold': val_metrics['threshold'],\n",
        "            'nrecept': NRECEPT,\n",
        "        }, CHECKPOINT_DIR / 'best.pt')\n",
        "\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "        'best_f1': best_f1,\n",
        "        'nrecept': NRECEPT,\n",
        "    }, CHECKPOINT_DIR / 'last.pt')\n",
        "\n",
        "    # ── epoch summary ──\n",
        "    star = '  ** NEW BEST **' if is_best else ''\n",
        "    print(f'\\n  EPOCH {epoch}/{EPOCHS} SUMMARY  ({elapsed:.1f}s){star}')\n",
        "    print(f'  ┌─────────────────────────────────────────────────────────────┐')\n",
        "    print(f'  │  Train loss    : {train_metrics[\"loss\"]:.6e}                │')\n",
        "    print(f'  │  Train acc@0.5 : {train_metrics[\"accuracy\"]:.4f}  '\n",
        "          f'  pos%: {train_metrics[\"pos_frac\"]:.3f}           │')\n",
        "    print(f'  │  ─────────────────────────────────────────────────────────  │')\n",
        "    print(f'  │  Val   loss    : {val_metrics[\"loss\"]:.6e}                │')\n",
        "    print(f'  │  Val   acc@th  : {val_metrics[\"accuracy\"]:.4f}  '\n",
        "          f'  acc@0.5: {val_metrics[\"acc_at_50\"]:.4f}          │')\n",
        "    print(f'  │  Val   F1      : {val_metrics[\"f1\"]:.4f}  '\n",
        "          f'  P={val_metrics[\"precision\"]:.4f}  R={val_metrics[\"recall\"]:.4f}  '\n",
        "          f'th={val_metrics[\"threshold\"]:.2f}  │')\n",
        "    print(f'  │  LR            : {lr_now:.2e}                           │')\n",
        "    print(f'  │  Best F1 so far: {best_f1:.4f}                              │')\n",
        "    print(f'  └─────────────────────────────────────────────────────────────┘')\n",
        "\n",
        "print(f'\\n{\"═\" * 110}')\n",
        "print(f'  TRAINING COMPLETE — {EPOCHS} epochs, best val F1 = {best_f1:.4f}')\n",
        "print(f'{\"═\" * 110}')"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "train-loop"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training curves"
      ],
      "id": "sec-curves"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(20, 8))\n",
        "epochs_range = np.arange(1, len(history['train_loss']) + 1)\n",
        "\n",
        "# ── Row 1 ──\n",
        "\n",
        "# (0,0) Loss\n",
        "axes[0, 0].plot(epochs_range, history['train_loss'], label='Train', linewidth=1.5)\n",
        "axes[0, 0].plot(epochs_range, history['val_loss'], label='Val', linewidth=1.5)\n",
        "axes[0, 0].set_ylabel('BCE Loss')\n",
        "axes[0, 0].set_title('Loss')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# (0,1) Accuracy\n",
        "axes[0, 1].plot(epochs_range, history['train_acc'], label='Train', linewidth=1.5)\n",
        "axes[0, 1].plot(epochs_range, history['val_acc'], label='Val (best th)', linewidth=1.5)\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Accuracy')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_ylim(0, 1)\n",
        "\n",
        "# (0,2) F1\n",
        "axes[0, 2].plot(epochs_range, history['val_f1'], color='firebrick', linewidth=2,\n",
        "                label='Val F1')\n",
        "axes[0, 2].set_ylabel('F1 Score')\n",
        "axes[0, 2].set_title(f'Validation F1  (best = {best_f1:.4f})')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "axes[0, 2].set_ylim(0, 1)\n",
        "\n",
        "# ── Row 2 ──\n",
        "\n",
        "# (1,0) Precision & Recall\n",
        "axes[1, 0].plot(epochs_range, history['val_precision'], label='Precision',\n",
        "                linewidth=1.5, color='teal')\n",
        "axes[1, 0].plot(epochs_range, history['val_recall'], label='Recall',\n",
        "                linewidth=1.5, color='darkorange')\n",
        "axes[1, 0].set_xlabel('Epoch')\n",
        "axes[1, 0].set_ylabel('Score')\n",
        "axes[1, 0].set_title('Val Precision & Recall')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_ylim(0, 1)\n",
        "\n",
        "# (1,1) Best threshold per epoch\n",
        "axes[1, 1].plot(epochs_range, history['val_threshold'], color='purple',\n",
        "                linewidth=1.5, marker='.', markersize=4)\n",
        "axes[1, 1].set_xlabel('Epoch')\n",
        "axes[1, 1].set_ylabel('Threshold')\n",
        "axes[1, 1].set_title('Best threshold per epoch')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].set_ylim(0, 1)\n",
        "\n",
        "# (1,2) Learning rate\n",
        "axes[1, 2].plot(epochs_range, history['lr'], color='darkorange', linewidth=1.5)\n",
        "axes[1, 2].set_xlabel('Epoch')\n",
        "axes[1, 2].set_ylabel('LR')\n",
        "axes[1, 2].set_title('Learning Rate')\n",
        "axes[1, 2].set_yscale('log')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(f'Training Summary — Best val F1 = {best_f1:.4f}',\n",
        "             fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "curves"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Visualise predictions on validation samples"
      ],
      "id": "sec-viz"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load best checkpoint\n",
        "ckpt = torch.load(CHECKPOINT_DIR / 'best.pt', map_location=DEVICE)\n",
        "model.load_state_dict(ckpt['state_dict'])\n",
        "best_th = ckpt.get('threshold', 0.5)\n",
        "print(f'Loaded best checkpoint (epoch {ckpt[\"epoch\"]}, F1={ckpt[\"best_f1\"]:.4f}, th={best_th:.2f})')\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# grab a batch from validation\n",
        "val_loader = loaders[val_split]\n",
        "X_b, target_b, weight_b = next(iter(val_loader))\n",
        "\n",
        "B = X_b.shape[0]\n",
        "X_flat = X_b.view(B, -1, X_b.shape[-1]).to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_b = model(X_flat).cpu().numpy()\n",
        "\n",
        "target_np = target_b.numpy()\n",
        "T_sub = pred_b.shape[-1]\n",
        "t_ax_ms = np.arange(T_sub) / (1e6 / DATA_STEP / 1000)  # ms\n",
        "\n",
        "n_show = min(4, B)\n",
        "fig, axes = plt.subplots(n_show, 1, figsize=(16, 3 * n_show), sharex=True)\n",
        "if n_show == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.plot(t_ax_ms, pred_b[i], color='steelblue', linewidth=1, label='Prediction')\n",
        "    ax.plot(t_ax_ms, target_np[i], color='firebrick', linewidth=1, linestyle='--', label='Target')\n",
        "    ax.axhline(best_th, color='gray', linestyle=':', alpha=0.5, label=f'Threshold={best_th:.2f}')\n",
        "    ax.axvspan(0, t_ax_ms[NRECEPT - 1], alpha=0.08, color='gray')\n",
        "    ax.set_ylabel(f'Sample {i}')\n",
        "    ax.set_ylim(-0.05, 1.05)\n",
        "    ax.grid(True, alpha=0.2)\n",
        "    if i == 0:\n",
        "        ax.legend(loc='upper left', fontsize=9)\n",
        "        ax.set_title('TCN predictions vs targets (gray = receptive field, not in loss)')\n",
        "\n",
        "axes[-1].set_xlabel('Time (ms) within subsequence')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "viz-pred"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}