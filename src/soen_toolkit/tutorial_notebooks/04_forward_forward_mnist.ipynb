{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 04 â€” Forward-Forward Learning for MNIST with SOEN\n",
    "\n",
    "This notebook demonstrates the **Forward-Forward (FF) algorithm** (Hinton, 2022) for training SOEN networks on MNIST, as an alternative to backpropagation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Š NOISE CONFIGURATION: ENABLED (Default)\n",
    "\n",
    "> **This tutorial runs with NOISE INJECTION (documented defaults).**\n",
    ">\n",
    "> | Parameter | Default | Description |\n",
    "> |-----------|---------|-------------|\n",
    "> | `phi` | **0.01** | Noise on input flux |\n",
    "> | `s` | **0.005** | Noise on state |\n",
    "> | `relative` | **False** | Absolute scaling |\n",
    ">\n",
    "> **To toggle noise on/off:** Use the `NOISE_ENABLED` variable below.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Forward-Forward for SOEN?\n",
    "\n",
    "| Aspect | Backpropagation | Forward-Forward |\n",
    "|--------|-----------------|------------------|\n",
    "| **Computation** | Requires backward pass | Forward only |\n",
    "| **Memory** | Store all activations | No caching needed |\n",
    "| **Weight transport** | Symmetric weights required | Local weights only |\n",
    "| **Hardware fit** | Needs external computation | Matches SOEN physics |\n",
    "| **Noise tolerance** | Gradients compound noise | Local learning is robust |\n",
    "\n",
    "## The Forward-Forward Algorithm\n",
    "\n",
    "Instead of propagating errors backward, each layer learns locally:\n",
    "\n",
    "1. **Positive pass**: Real data with correct label â†’ maximize \"goodness\"\n",
    "2. **Negative pass**: Real data with wrong label â†’ minimize \"goodness\"\n",
    "3. **Goodness**: Sum of squared activations (or mean squared state)\n",
    "4. **Threshold**: Each layer pushes positive goodness above Î¸, negative below Î¸\n",
    "\n",
    "```\n",
    "Positive data: Image + correct_label_embedding â†’ High goodness âœ“\n",
    "Negative data: Image + wrong_label_embedding   â†’ Low goodness  âœ—\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Disable tqdm notebook widgets BEFORE any imports\nimport os\nos.environ[\"TQDM_DISABLE\"] = \"0\"  # Don't disable, but force text mode\nos.environ[\"TQDM_MININTERVAL\"] = \"1\"\n\n# Setup: Ensure soen_toolkit is importable\nimport sys\nfrom pathlib import Path\n\n# Add src directory to path if running from notebook location\nnotebook_dir = Path.cwd()\nfor parent in [notebook_dir] + list(notebook_dir.parents):\n    candidate = parent / \"src\"\n    if (candidate / \"soen_toolkit\").exists():\n        sys.path.insert(0, str(candidate))\n        break\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport gzip\nimport urllib.request\nimport struct\nfrom typing import Optional, Tuple, List, Dict\nfrom dataclasses import dataclass\n\n# Use standard tqdm (not notebook version to avoid widget errors)\ntry:\n    from tqdm import tqdm\nexcept ImportError:\n    def tqdm(iterable, **kwargs):\n        return iterable\n\n# Set torch precision\ntorch.set_float32_matmul_precision('high')\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n    DEVICE = torch.device('cuda')\nelse:\n    DEVICE = torch.device('cpu')\nprint(f\"Using device: {DEVICE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# NOISE CONFIGURATION TOGGLE\n",
    "# ==============================================================================\n",
    "# Set NOISE_ENABLED = False to run with ideal conditions (no noise)\n",
    "# Set NOISE_ENABLED = True for noise injection (default)\n",
    "\n",
    "NOISE_ENABLED = True  # Toggle this to enable/disable noise\n",
    "\n",
    "# Default noise parameters (documented defaults)\n",
    "NOISE_DEFAULTS = {\n",
    "    \"phi\": 0.01,           # Noise on input flux\n",
    "    \"s\": 0.005,            # Noise on state\n",
    "    \"g\": 0.0,              # Source function noise\n",
    "    \"bias_current\": 0.0,   # Bias current noise\n",
    "    \"j\": 0.0,              # Connection weight noise\n",
    "    \"relative\": False,     # Absolute scaling\n",
    "}\n",
    "\n",
    "def set_model_noise(model, enabled=True, noise_values=None):\n",
    "    \"\"\"\n",
    "    Toggle noise injection on/off for a SOEN model.\n",
    "    \n",
    "    Args:\n",
    "        model: SOENModelCore instance\n",
    "        enabled: If True, apply noise; if False, set all noise to 0\n",
    "        noise_values: Dict of noise parameters (uses NOISE_DEFAULTS if None)\n",
    "    \n",
    "    Returns:\n",
    "        model: The modified model (for chaining)\n",
    "    \"\"\"\n",
    "    from soen_toolkit.core.configs import NoiseConfig\n",
    "    \n",
    "    if noise_values is None:\n",
    "        noise_values = NOISE_DEFAULTS\n",
    "    \n",
    "    # Update layer noise configurations\n",
    "    for cfg in model.layers_config:\n",
    "        if enabled:\n",
    "            cfg.noise = NoiseConfig(\n",
    "                phi=noise_values.get(\"phi\", 0.01),\n",
    "                s=noise_values.get(\"s\", 0.005),\n",
    "                g=noise_values.get(\"g\", 0.0),\n",
    "                bias_current=noise_values.get(\"bias_current\", 0.0),\n",
    "                j=noise_values.get(\"j\", 0.0),\n",
    "                relative=noise_values.get(\"relative\", False),\n",
    "                extras=getattr(cfg.noise, \"extras\", {}),\n",
    "            )\n",
    "        else:\n",
    "            cfg.noise = NoiseConfig(\n",
    "                phi=0.0, s=0.0, g=0.0, bias_current=0.0, j=0.0,\n",
    "                relative=False,\n",
    "                extras=getattr(cfg.noise, \"extras\", {}),\n",
    "            )\n",
    "    \n",
    "    # Update connection noise configurations\n",
    "    for conn_cfg in model.connections_config:\n",
    "        if enabled:\n",
    "            conn_cfg.noise = NoiseConfig(\n",
    "                phi=0.0, g=0.0, s=0.0, bias_current=0.0,\n",
    "                j=noise_values.get(\"j\", 0.0),\n",
    "                relative=noise_values.get(\"relative\", False),\n",
    "                extras={},\n",
    "            )\n",
    "        else:\n",
    "            conn_cfg.noise = NoiseConfig(\n",
    "                phi=0.0, g=0.0, s=0.0, bias_current=0.0, j=0.0,\n",
    "                relative=False, extras={},\n",
    "            )\n",
    "    \n",
    "    status = \"ENABLED\" if enabled else \"DISABLED\"\n",
    "    print(f\"âœ“ Noise injection {status}\")\n",
    "    if enabled:\n",
    "        print(f\"  phi={noise_values['phi']}, s={noise_values['s']}, \"\n",
    "              f\"relative={noise_values['relative']}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(f\"Noise injection: {'ENABLED' if NOISE_ENABLED else 'DISABLED'}\")\n",
    "if NOISE_ENABLED:\n",
    "    print(f\"  Default values: phi={NOISE_DEFAULTS['phi']}, s={NOISE_DEFAULTS['s']}, \"\n",
    "          f\"relative={NOISE_DEFAULTS['relative']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Forward-Forward Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class FFConfig:\n",
    "    \"\"\"Configuration for Forward-Forward training.\"\"\"\n",
    "    \n",
    "    # Architecture\n",
    "    input_dim: int = 28 * 28 + 10  # Flattened MNIST + one-hot label\n",
    "    hidden_dims: List[int] = None  # Will default to [500, 500]\n",
    "    num_classes: int = 10\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 128\n",
    "    num_epochs: int = 60\n",
    "    learning_rate: float = 0.03\n",
    "    \n",
    "    # Forward-Forward specific\n",
    "    threshold: float = 2.0  # Goodness threshold\n",
    "    negative_label_method: str = \"random\"  # \"random\" or \"next\"\n",
    "    \n",
    "    # SOEN specific\n",
    "    use_soen_layers: bool = True  # Use SOEN dynamics vs standard layers\n",
    "    dt: float = 100.0  # SOEN timestep\n",
    "    num_timesteps: int = 28  # Process as sequence (one row at a time)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.hidden_dims is None:\n",
    "            self.hidden_dims = [500, 500]\n",
    "\n",
    "\n",
    "# Default configuration\n",
    "config = FFConfig()\n",
    "print(\"Forward-Forward Configuration:\")\n",
    "print(f\"  Input dim: {config.input_dim}\")\n",
    "print(f\"  Hidden dims: {config.hidden_dims}\")\n",
    "print(f\"  Threshold: {config.threshold}\")\n",
    "print(f\"  Use SOEN layers: {config.use_soen_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 2. Prepare MNIST Dataset with Label Embedding\n",
    "\n",
    "For Forward-Forward, we embed the label directly into the input:\n",
    "- **Positive data**: Image pixels + one-hot encoding of the **correct** label\n",
    "- **Negative data**: Image pixels + one-hot encoding of a **wrong** label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist_file(filename, base_url=\"https://ossci-datasets.s3.amazonaws.com/mnist/\"):\n",
    "    \"\"\"Download a single MNIST file if not already present.\"\"\"\n",
    "    data_dir = Path(\"./data/mnist\")\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    filepath = data_dir / filename\n",
    "    if not filepath.exists():\n",
    "        url = base_url + filename\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        urllib.request.urlretrieve(url, filepath)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def read_mnist_images(filepath):\n",
    "    \"\"\"Read MNIST image file (idx3-ubyte format).\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(num_images, rows, cols)\n",
    "    return images\n",
    "\n",
    "\n",
    "def read_mnist_labels(filepath):\n",
    "    \"\"\"Read MNIST label file (idx1-ubyte format).\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "    \"\"\"Load MNIST dataset.\"\"\"\n",
    "    # Download files\n",
    "    train_images_file = download_mnist_file(\"train-images-idx3-ubyte.gz\")\n",
    "    train_labels_file = download_mnist_file(\"train-labels-idx1-ubyte.gz\")\n",
    "    test_images_file = download_mnist_file(\"t10k-images-idx3-ubyte.gz\")\n",
    "    test_labels_file = download_mnist_file(\"t10k-labels-idx1-ubyte.gz\")\n",
    "    \n",
    "    # Read data\n",
    "    train_images = read_mnist_images(train_images_file).astype(np.float32) / 255.0\n",
    "    train_labels = read_mnist_labels(train_labels_file)\n",
    "    test_images = read_mnist_images(test_images_file).astype(np.float32) / 255.0\n",
    "    test_labels = read_mnist_labels(test_labels_file)\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_images, train_labels, test_images, test_labels = load_mnist()\n",
    "print(f\"Train: {train_images.shape}, Test: {test_images.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTForwardForwardDataset(Dataset):\n",
    "    \"\"\"MNIST dataset prepared for Forward-Forward learning.\n",
    "    \n",
    "    Each sample returns both positive and negative versions:\n",
    "    - Positive: image + correct label one-hot\n",
    "    - Negative: image + wrong label one-hot\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels, num_classes=10, negative_method=\"random\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: (N, 28, 28) image array\n",
    "            labels: (N,) label array\n",
    "            num_classes: Number of classes (10 for MNIST)\n",
    "            negative_method: \"random\" for random wrong label, \"next\" for (label+1)%10\n",
    "        \"\"\"\n",
    "        self.images = torch.tensor(images, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.num_classes = num_classes\n",
    "        self.negative_method = negative_method\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _embed_label(self, image, label):\n",
    "        \"\"\"Embed label into image.\n",
    "        \n",
    "        We replace the first 10 pixels of the flattened image with the one-hot label.\n",
    "        This is Hinton's approach from the original paper.\n",
    "        \"\"\"\n",
    "        flat_image = image.flatten()  # (784,)\n",
    "        one_hot = F.one_hot(label, num_classes=self.num_classes).float()  # (10,)\n",
    "        \n",
    "        # Replace first 10 pixels with label\n",
    "        embedded = flat_image.clone()\n",
    "        embedded[:self.num_classes] = one_hot\n",
    "        \n",
    "        return embedded\n",
    "    \n",
    "    def _get_negative_label(self, true_label):\n",
    "        \"\"\"Generate a wrong label for negative data.\"\"\"\n",
    "        if self.negative_method == \"next\":\n",
    "            return (true_label + 1) % self.num_classes\n",
    "        else:  # random\n",
    "            wrong_label = torch.randint(0, self.num_classes - 1, (1,)).item()\n",
    "            if wrong_label >= true_label:\n",
    "                wrong_label += 1\n",
    "            return wrong_label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        true_label = self.labels[idx]\n",
    "        \n",
    "        # Positive: correct label\n",
    "        positive_data = self._embed_label(image, true_label)\n",
    "        \n",
    "        # Negative: wrong label\n",
    "        wrong_label = self._get_negative_label(true_label.item())\n",
    "        negative_data = self._embed_label(image, torch.tensor(wrong_label))\n",
    "        \n",
    "        return {\n",
    "            \"positive\": positive_data,\n",
    "            \"negative\": negative_data,\n",
    "            \"label\": true_label,\n",
    "            \"image\": image,\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = MNISTForwardForwardDataset(\n",
    "    train_images, train_labels, \n",
    "    negative_method=config.negative_label_method\n",
    ")\n",
    "test_dataset = MNISTForwardForwardDataset(\n",
    "    test_images, test_labels,\n",
    "    negative_method=config.negative_label_method\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Verify\n",
    "sample = train_dataset[0]\n",
    "print(f\"Positive data shape: {sample['positive'].shape}\")\n",
    "print(f\"Negative data shape: {sample['negative'].shape}\")\n",
    "print(f\"True label: {sample['label']}\")\n",
    "print(f\"First 10 pixels of positive (should be one-hot): {sample['positive'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Visualize Positive vs Negative Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ff_data(dataset, n_samples=5):\n",
    "    \"\"\"Visualize positive and negative data pairs.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(3, n_samples, figsize=(3*n_samples, 9))\n",
    "    fig.suptitle('Forward-Forward Data: Positive vs Negative', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        sample = dataset[i]\n",
    "        \n",
    "        # Original image\n",
    "        axes[0, i].imshow(sample['image'], cmap='gray')\n",
    "        axes[0, i].set_title(f\"True label: {sample['label'].item()}\")\n",
    "        axes[0, i].axis('off')\n",
    "        \n",
    "        # Positive embedding (first 10 pixels show one-hot)\n",
    "        pos_reshaped = sample['positive'].reshape(28, 28)\n",
    "        axes[1, i].imshow(pos_reshaped, cmap='gray')\n",
    "        embedded_label = sample['positive'][:10].argmax().item()\n",
    "        axes[1, i].set_title(f\"Positive (label={embedded_label})\")\n",
    "        axes[1, i].axis('off')\n",
    "        \n",
    "        # Negative embedding\n",
    "        neg_reshaped = sample['negative'].reshape(28, 28)\n",
    "        axes[2, i].imshow(neg_reshaped, cmap='gray')\n",
    "        wrong_label = sample['negative'][:10].argmax().item()\n",
    "        axes[2, i].set_title(f\"Negative (label={wrong_label})\", color='red')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "    axes[1, 0].set_ylabel('Positive\\n(correct label)', fontsize=12)\n",
    "    axes[2, 0].set_ylabel('Negative\\n(wrong label)', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_ff_data(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Forward-Forward Layer\n",
    "\n",
    "Each layer in Forward-Forward has its own local objective:\n",
    "- **Goodness** = sum of squared activations\n",
    "- **Goal**: goodness > threshold for positive data, goodness < threshold for negative data\n",
    "\n",
    "We'll create both a standard FF layer and a SOEN-based FF layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFLayer(nn.Module):\n",
    "    \"\"\"Standard Forward-Forward layer with local learning objective.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, threshold: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.threshold = threshold\n",
    "        self.layer_norm = nn.LayerNorm(out_features)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass with layer normalization.\"\"\"\n",
    "        x = self.linear(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "    def compute_goodness(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute goodness (sum of squared activations).\"\"\"\n",
    "        return (x ** 2).mean(dim=1)  # Mean over features, shape: (batch,)\n",
    "    \n",
    "    def ff_loss(self, pos_goodness: torch.Tensor, neg_goodness: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward-Forward loss: push positive above threshold, negative below.\n",
    "        \n",
    "        Loss = log(1 + exp(-(pos_goodness - threshold))) + log(1 + exp(neg_goodness - threshold))\n",
    "        \"\"\"\n",
    "        pos_loss = torch.log(1 + torch.exp(-(pos_goodness - self.threshold)))\n",
    "        neg_loss = torch.log(1 + torch.exp(neg_goodness - self.threshold))\n",
    "        return (pos_loss + neg_loss).mean()\n",
    "\n",
    "\n",
    "# Test\n",
    "test_layer = FFLayer(784, 500)\n",
    "test_input = torch.randn(32, 784)\n",
    "test_output = test_layer(test_input)\n",
    "test_goodness = test_layer.compute_goodness(test_output)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Goodness shape: {test_goodness.shape}\")\n",
    "print(f\"Mean goodness: {test_goodness.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. SOEN Forward-Forward Layer\n",
    "\n",
    "This layer uses SOEN SingleDendrite dynamics for the forward pass, making it compatible with neuromorphic hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "\n",
    "class SOENFFLayer(nn.Module):\n",
    "    \"\"\"SOEN-based Forward-Forward layer using SingleDendrite dynamics.\n",
    "    \n",
    "    The input is processed as a sequence (28 timesteps for MNIST rows),\n",
    "    and goodness is computed from the final states.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int, \n",
    "        threshold: float = 2.0,\n",
    "        dt: float = 100.0,\n",
    "        num_timesteps: int = 28,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "        self.num_timesteps = num_timesteps\n",
    "        \n",
    "        # Build SOEN model\n",
    "        sim_cfg = SimulationConfig(\n",
    "            dt=dt,\n",
    "            input_type=\"state\",\n",
    "            track_phi=False,\n",
    "            track_power=False,\n",
    "        )\n",
    "        \n",
    "        # Input layer\n",
    "        layer0 = LayerConfig(\n",
    "            layer_id=0,\n",
    "            layer_type=\"Input\",\n",
    "            params={\"dim\": in_features // num_timesteps},  # Features per timestep\n",
    "        )\n",
    "        \n",
    "        # SOEN hidden layer with SingleDendrite dynamics\n",
    "        layer1 = LayerConfig(\n",
    "            layer_id=1,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": out_features,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": {\"distribution\": \"uniform\", \"params\": {\"min\": 1.8, \"max\": 2.1}},\n",
    "                \"gamma_plus\": {\"distribution\": \"constant\", \"params\": {\"value\": 0.001}},\n",
    "                \"gamma_minus\": {\"distribution\": \"constant\", \"params\": {\"value\": 0.0001}},\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        # Connection\n",
    "        conn = ConnectionConfig(\n",
    "            from_layer=0,\n",
    "            to_layer=1,\n",
    "            connection_type=\"dense\",\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "            learnable=True,\n",
    "        )\n",
    "        \n",
    "        self.soen_model = SOENModelCore(\n",
    "            sim_config=sim_cfg,\n",
    "            layers_config=[layer0, layer1],\n",
    "            connections_config=[conn],\n",
    "        )\n",
    "        \n",
    "        # Apply noise configuration\n",
    "        if NOISE_ENABLED:\n",
    "            set_model_noise(self.soen_model, enabled=True, noise_values=NOISE_DEFAULTS)\n",
    "        \n",
    "        # Output projection (optional, for matching dimensions)\n",
    "        self.layer_norm = nn.LayerNorm(out_features)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Process input through SOEN dynamics.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, features) where features = 784\n",
    "        \n",
    "        Returns:\n",
    "            Output tensor of shape (batch, out_features)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Reshape to sequence: (batch, timesteps, features_per_step)\n",
    "        # For MNIST: (batch, 28, 28) treating each row as a timestep\n",
    "        x_seq = x.view(batch_size, self.num_timesteps, -1)\n",
    "        \n",
    "        # Process through SOEN\n",
    "        output, all_states = self.soen_model(x_seq)\n",
    "        \n",
    "        # Use final state (or max-pooled state)\n",
    "        # output shape: (batch, timesteps, out_features)\n",
    "        if output.dim() == 3:\n",
    "            # Max pool over time to get most salient features\n",
    "            output = output.max(dim=1)[0]  # (batch, out_features)\n",
    "        \n",
    "        # Normalize\n",
    "        output = self.layer_norm(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def compute_goodness(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute goodness (mean squared activation).\"\"\"\n",
    "        return (x ** 2).mean(dim=1)\n",
    "    \n",
    "    def ff_loss(self, pos_goodness: torch.Tensor, neg_goodness: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward-Forward loss.\"\"\"\n",
    "        pos_loss = torch.log(1 + torch.exp(-(pos_goodness - self.threshold)))\n",
    "        neg_loss = torch.log(1 + torch.exp(neg_goodness - self.threshold))\n",
    "        return (pos_loss + neg_loss).mean()\n",
    "\n",
    "\n",
    "# Test SOEN FF layer\n",
    "print(\"Testing SOEN FF Layer...\")\n",
    "soen_layer = SOENFFLayer(784, 256, threshold=2.0)\n",
    "test_input = torch.randn(4, 784)\n",
    "test_output = soen_layer(test_input)\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Goodness: {soen_layer.compute_goodness(test_output).mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 6. Forward-Forward Network\n",
    "\n",
    "Stack multiple FF layers, each trained with its own local objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardForwardNet(nn.Module):\n",
    "    \"\"\"Multi-layer Forward-Forward network.\n",
    "    \n",
    "    Each layer is trained independently with its own optimizer.\n",
    "    For inference, we evaluate goodness across all possible labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim: int = 784,\n",
    "        hidden_dims: List[int] = [500, 500],\n",
    "        num_classes: int = 10,\n",
    "        threshold: float = 2.0,\n",
    "        use_soen: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.use_soen = use_soen\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        dims = [input_dim] + hidden_dims\n",
    "        for i in range(len(hidden_dims)):\n",
    "            if use_soen and i == 0:  # Only first layer uses SOEN for now\n",
    "                layer = SOENFFLayer(dims[i], dims[i+1], threshold=threshold)\n",
    "            else:\n",
    "                layer = FFLayer(dims[i], dims[i+1], threshold=threshold)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        print(f\"Created FF Network with {len(self.layers)} layers:\")\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            layer_type = \"SOEN\" if isinstance(layer, SOENFFLayer) else \"Standard\"\n",
    "            print(f\"  Layer {i}: {dims[i]} -> {dims[i+1]} ({layer_type})\")\n",
    "    \n",
    "    def forward_one_layer(self, x: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through layers up to and including layer_idx.\"\"\"\n",
    "        for i in range(layer_idx + 1):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "    \n",
    "    def compute_layer_goodness(self, x: torch.Tensor, layer_idx: int) -> torch.Tensor:\n",
    "        \"\"\"Compute goodness at a specific layer.\"\"\"\n",
    "        output = self.forward_one_layer(x, layer_idx)\n",
    "        return self.layers[layer_idx].compute_goodness(output)\n",
    "    \n",
    "    def predict(self, images: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict labels by finding which label embedding maximizes total goodness.\n",
    "        \n",
    "        For each image, we try all 10 possible labels and pick the one\n",
    "        that results in highest summed goodness across all layers.\n",
    "        \"\"\"\n",
    "        batch_size = images.shape[0]\n",
    "        device = images.device\n",
    "        \n",
    "        # Flatten images\n",
    "        flat_images = images.view(batch_size, -1)  # (batch, 784)\n",
    "        \n",
    "        # Store goodness for each label\n",
    "        all_goodness = torch.zeros(batch_size, self.num_classes, device=device)\n",
    "        \n",
    "        for label in range(self.num_classes):\n",
    "            # Create one-hot label\n",
    "            one_hot = F.one_hot(torch.tensor([label], device=device), self.num_classes)\n",
    "            one_hot = one_hot.float().expand(batch_size, -1)  # (batch, 10)\n",
    "            \n",
    "            # Embed label into image (replace first 10 pixels)\n",
    "            embedded = flat_images.clone()\n",
    "            embedded[:, :self.num_classes] = one_hot\n",
    "            \n",
    "            # Compute total goodness across all layers\n",
    "            total_goodness = torch.zeros(batch_size, device=device)\n",
    "            x = embedded\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "                total_goodness += layer.compute_goodness(x)\n",
    "            \n",
    "            all_goodness[:, label] = total_goodness\n",
    "        \n",
    "        # Return predicted labels (highest goodness)\n",
    "        return all_goodness.argmax(dim=1)\n",
    "    \n",
    "    def get_layer_optimizer(self, layer_idx: int, lr: float = 0.03):\n",
    "        \"\"\"Get optimizer for a specific layer.\"\"\"\n",
    "        return torch.optim.Adam(self.layers[layer_idx].parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# Create network\n",
    "ff_net = ForwardForwardNet(\n",
    "    input_dim=784,\n",
    "    hidden_dims=config.hidden_dims,\n",
    "    num_classes=config.num_classes,\n",
    "    threshold=config.threshold,\n",
    "    use_soen=config.use_soen_layers,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 7. Training Loop\n",
    "\n",
    "The key difference from backprop: **each layer is trained independently** using its own forward passes and local loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forward_forward(\n",
    "    model: ForwardForwardNet,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    num_epochs: int = 60,\n",
    "    lr: float = 0.03,\n",
    "    device: torch.device = DEVICE,\n",
    "):\n",
    "    \"\"\"Train using Forward-Forward algorithm.\n",
    "    \n",
    "    Key insight: Each layer is trained independently!\n",
    "    No backward pass through the entire network.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    # Create separate optimizer for each layer\n",
    "    optimizers = [model.get_layer_optimizer(i, lr) for i in range(len(model.layers))]\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"layer_losses\": [[] for _ in range(len(model.layers))],\n",
    "        \"layer_pos_goodness\": [[] for _ in range(len(model.layers))],\n",
    "        \"layer_neg_goodness\": [[] for _ in range(len(model.layers))],\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nStarting Forward-Forward Training\")\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"Epochs: {num_epochs}, LR: {lr}, Threshold: {model.layers[0].threshold}\")\n",
    "    print(f\"Layers: {len(model.layers)}, Device: {device}\")\n",
    "    print(f\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_losses = [0.0 for _ in range(len(model.layers))]\n",
    "        epoch_pos_goodness = [0.0 for _ in range(len(model.layers))]\n",
    "        epoch_neg_goodness = [0.0 for _ in range(len(model.layers))]\n",
    "        num_batches = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for batch in pbar:\n",
    "            pos_data = batch[\"positive\"].to(device)\n",
    "            neg_data = batch[\"negative\"].to(device)\n",
    "            \n",
    "            # Train each layer independently (this is the key FF insight!)\n",
    "            pos_input = pos_data\n",
    "            neg_input = neg_data\n",
    "            \n",
    "            batch_loss = 0.0\n",
    "            for layer_idx, (layer, optimizer) in enumerate(zip(model.layers, optimizers)):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass for this layer\n",
    "                pos_output = layer(pos_input)\n",
    "                neg_output = layer(neg_input)\n",
    "                \n",
    "                # Compute goodness\n",
    "                pos_goodness = layer.compute_goodness(pos_output)\n",
    "                neg_goodness = layer.compute_goodness(neg_output)\n",
    "                \n",
    "                # Local loss for this layer\n",
    "                loss = layer.ff_loss(pos_goodness, neg_goodness)\n",
    "                \n",
    "                # Backward (only for this layer's parameters!)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track metrics\n",
    "                epoch_losses[layer_idx] += loss.item()\n",
    "                epoch_pos_goodness[layer_idx] += pos_goodness.mean().item()\n",
    "                epoch_neg_goodness[layer_idx] += neg_goodness.mean().item()\n",
    "                batch_loss += loss.item()\n",
    "                \n",
    "                # Detach for next layer (no gradient flow between layers!)\n",
    "                pos_input = pos_output.detach()\n",
    "                neg_input = neg_output.detach()\n",
    "            \n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{batch_loss/len(model.layers):.4f}\"})\n",
    "        \n",
    "        # Average metrics\n",
    "        for i in range(len(model.layers)):\n",
    "            history[\"layer_losses\"][i].append(epoch_losses[i] / num_batches)\n",
    "            history[\"layer_pos_goodness\"][i].append(epoch_pos_goodness[i] / num_batches)\n",
    "            history[\"layer_neg_goodness\"][i].append(epoch_neg_goodness[i] / num_batches)\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / (num_batches * len(model.layers))\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            test_acc = evaluate_ff(model, test_loader, device)\n",
    "            history[\"test_accuracy\"].append(test_acc)\n",
    "            print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Test Acc={test_acc:.2%}\")\n",
    "            \n",
    "            # Print layer-wise goodness\n",
    "            for i in range(len(model.layers)):\n",
    "                pos_g = history[\"layer_pos_goodness\"][i][-1]\n",
    "                neg_g = history[\"layer_neg_goodness\"][i][-1]\n",
    "                print(f\"  Layer {i}: pos_goodness={pos_g:.3f}, neg_goodness={neg_g:.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_ff(model: ForwardForwardNet, test_loader: DataLoader, device: torch.device):\n",
    "    \"\"\"Evaluate Forward-Forward model.\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            predictions = model.predict(images)\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train_forward_forward(\n",
    "    model=ff_net,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    num_epochs=config.num_epochs,\n",
    "    lr=config.learning_rate,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 8. Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ff_training_history(history: Dict):\n",
    "    \"\"\"Plot Forward-Forward training metrics.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Forward-Forward Training Progress', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Overall loss\n",
    "    axes[0, 0].plot(history[\"train_loss\"], 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Test accuracy\n",
    "    epochs_with_acc = list(range(0, len(history[\"train_loss\"]), 5))\n",
    "    if 0 not in epochs_with_acc:\n",
    "        epochs_with_acc = [0] + epochs_with_acc\n",
    "    axes[0, 1].plot(epochs_with_acc[:len(history[\"test_accuracy\"])], \n",
    "                    history[\"test_accuracy\"], 'g-o', linewidth=2, markersize=6)\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Test Accuracy')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([0, 1])\n",
    "    \n",
    "    # 3. Layer-wise goodness (positive)\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(history[\"layer_pos_goodness\"])))\n",
    "    for i, (pos_g, neg_g) in enumerate(zip(history[\"layer_pos_goodness\"], \n",
    "                                           history[\"layer_neg_goodness\"])):\n",
    "        axes[1, 0].plot(pos_g, color=colors[i], linestyle='-', \n",
    "                        linewidth=2, label=f'Layer {i} (pos)')\n",
    "        axes[1, 0].plot(neg_g, color=colors[i], linestyle='--', \n",
    "                        linewidth=2, alpha=0.7, label=f'Layer {i} (neg)')\n",
    "    \n",
    "    # Add threshold line\n",
    "    threshold = config.threshold\n",
    "    axes[1, 0].axhline(y=threshold, color='red', linestyle=':', linewidth=2, label=f'Threshold ({threshold})')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Goodness')\n",
    "    axes[1, 0].set_title('Layer-wise Goodness (solid=positive, dashed=negative)')\n",
    "    axes[1, 0].legend(loc='best', fontsize=8)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Layer-wise loss\n",
    "    for i, layer_loss in enumerate(history[\"layer_losses\"]):\n",
    "        axes[1, 1].plot(layer_loss, color=colors[i], linewidth=2, label=f'Layer {i}')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Layer-wise FF Loss')\n",
    "    axes[1, 1].legend(loc='best')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Final test accuracy: {history['test_accuracy'][-1]:.2%}\")\n",
    "    print(f\"Best test accuracy: {max(history['test_accuracy']):.2%}\")\n",
    "    print(f\"\\nFinal layer-wise goodness:\")\n",
    "    for i in range(len(history['layer_pos_goodness'])):\n",
    "        pos = history['layer_pos_goodness'][i][-1]\n",
    "        neg = history['layer_neg_goodness'][i][-1]\n",
    "        separation = pos - neg\n",
    "        print(f\"  Layer {i}: pos={pos:.3f}, neg={neg:.3f}, separation={separation:.3f}\")\n",
    "\n",
    "\n",
    "plot_ff_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 9. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_ff_predictions(model: ForwardForwardNet, test_dataset, n_samples=20):\n",
    "    \"\"\"Visualize Forward-Forward predictions with goodness values.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Random samples\n",
    "    indices = np.random.choice(len(test_dataset), n_samples, replace=False)\n",
    "    \n",
    "    n_cols = 5\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 3.5*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    fig.suptitle('Forward-Forward Predictions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    correct = 0\n",
    "    for i, idx in enumerate(indices):\n",
    "        sample = test_dataset[idx]\n",
    "        image = sample['image'].unsqueeze(0).to(DEVICE)\n",
    "        true_label = sample['label'].item()\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            pred = model.predict(image).item()\n",
    "        \n",
    "        is_correct = pred == true_label\n",
    "        correct += is_correct\n",
    "        \n",
    "        # Plot\n",
    "        ax = axes[i]\n",
    "        ax.imshow(sample['image'], cmap='gray')\n",
    "        \n",
    "        color = 'green' if is_correct else 'red'\n",
    "        symbol = 'âœ“' if is_correct else 'âœ—'\n",
    "        ax.set_title(f\"{symbol} Pred: {pred}\\nTrue: {true_label}\", \n",
    "                     color=color, fontsize=10, fontweight='bold' if not is_correct else 'normal')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSample accuracy: {correct}/{n_samples} ({correct/n_samples:.1%})\")\n",
    "\n",
    "\n",
    "visualize_ff_predictions(ff_net, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## 10. Analyze Goodness Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_goodness_separation(model: ForwardForwardNet, test_loader: DataLoader):\n",
    "    \"\"\"Analyze how well positive and negative goodness are separated.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_pos_goodness = [[] for _ in range(len(model.layers))]\n",
    "    all_neg_goodness = [[] for _ in range(len(model.layers))]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Analyzing goodness\"):\n",
    "            pos_data = batch[\"positive\"].to(DEVICE)\n",
    "            neg_data = batch[\"negative\"].to(DEVICE)\n",
    "            \n",
    "            pos_input = pos_data\n",
    "            neg_input = neg_data\n",
    "            \n",
    "            for layer_idx, layer in enumerate(model.layers):\n",
    "                pos_output = layer(pos_input)\n",
    "                neg_output = layer(neg_input)\n",
    "                \n",
    "                pos_g = layer.compute_goodness(pos_output)\n",
    "                neg_g = layer.compute_goodness(neg_output)\n",
    "                \n",
    "                all_pos_goodness[layer_idx].extend(pos_g.cpu().numpy())\n",
    "                all_neg_goodness[layer_idx].extend(neg_g.cpu().numpy())\n",
    "                \n",
    "                pos_input = pos_output\n",
    "                neg_input = neg_output\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt.subplots(1, len(model.layers), figsize=(6*len(model.layers), 5))\n",
    "    if len(model.layers) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle('Goodness Distributions by Layer', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    threshold = config.threshold\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        pos_g = np.array(all_pos_goodness[i])\n",
    "        neg_g = np.array(all_neg_goodness[i])\n",
    "        \n",
    "        # Histograms\n",
    "        ax.hist(pos_g, bins=50, alpha=0.6, color='green', label=f'Positive (Î¼={pos_g.mean():.2f})', density=True)\n",
    "        ax.hist(neg_g, bins=50, alpha=0.6, color='red', label=f'Negative (Î¼={neg_g.mean():.2f})', density=True)\n",
    "        \n",
    "        # Threshold\n",
    "        ax.axvline(x=threshold, color='black', linestyle='--', linewidth=2, label=f'Threshold ({threshold})')\n",
    "        \n",
    "        ax.set_xlabel('Goodness', fontsize=12)\n",
    "        ax.set_ylabel('Density', fontsize=12)\n",
    "        ax.set_title(f'Layer {i}', fontsize=12)\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print separation stats\n",
    "        separation = pos_g.mean() - neg_g.mean()\n",
    "        overlap = np.sum((pos_g < threshold) | (neg_g > threshold)) / (len(pos_g) + len(neg_g))\n",
    "        print(f\"Layer {i}: separation={separation:.3f}, ~overlap={overlap:.1%}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_goodness_separation(ff_net, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 11. Compare with Backpropagation (Optional)\n",
    "\n",
    "For reference, let's see how a similar architecture performs with standard backprop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackpropNet(nn.Module):\n",
    "    \"\"\"Standard backprop network for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dims=[500, 500], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        dims = [input_dim] + hidden_dims + [num_classes]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:  # No activation after last layer\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.LayerNorm(dims[i+1]))\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_backprop(model, train_loader, test_loader, num_epochs=20, lr=0.001):\n",
    "    \"\"\"Train with standard backpropagation.\"\"\"\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {\"train_loss\": [], \"test_accuracy\": []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Backprop Epoch {epoch+1}\"):\n",
    "            images = batch[\"image\"].to(DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()  # This is the key difference from FF!\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                images = batch[\"image\"].to(DEVICE)\n",
    "                labels = batch[\"label\"].to(DEVICE)\n",
    "                outputs = model(images)\n",
    "                _, predicted = outputs.max(1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        acc = correct / total\n",
    "        history[\"test_accuracy\"].append(acc)\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, Test Acc={acc:.2%}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Uncomment to train backprop comparison\n",
    "# bp_net = BackpropNet(hidden_dims=config.hidden_dims)\n",
    "# bp_history = train_backprop(bp_net, train_loader, test_loader, num_epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Forward-Forward Algorithm**:\n",
    "   - Each layer learns **independently** using local goodness\n",
    "   - No backward pass required â†’ natural fit for neuromorphic hardware\n",
    "   - Uses positive/negative contrastive data with embedded labels\n",
    "\n",
    "2. **Key Differences from Backprop**:\n",
    "   | Aspect | Backprop | Forward-Forward |\n",
    "   |--------|----------|------------------|\n",
    "   | Gradient flow | Global (through all layers) | Local (per layer) |\n",
    "   | Memory | Store all activations | No storage needed |\n",
    "   | Data | Standard (image, label) | Contrastive (pos/neg pairs) |\n",
    "   | Hardware | Requires external compute | On-chip learning possible |\n",
    "\n",
    "3. **SOEN Advantages**:\n",
    "   - FF aligns with SOEN's forward-only physics\n",
    "   - Local learning is noise-tolerant\n",
    "   - Potential for on-chip training without external GPUs\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "- Forward-Forward typically achieves ~98% on MNIST (vs ~99% for backprop)\n",
    "- The slight accuracy gap is offset by hardware advantages\n",
    "- Goodness separation should show clear positive/negative clusters\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different threshold values\n",
    "- Experiment with more SOEN layers\n",
    "- Implement symmetric negative data generation\n",
    "- Test on more complex datasets (Fashion-MNIST, CIFAR-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}