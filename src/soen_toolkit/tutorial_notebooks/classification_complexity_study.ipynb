{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Classification Complexity Study: Circle vs Ring\n",
    "\n",
    "A nonlinear binary classification task where the SingleDendrite's nonlinear dynamics are actually **useful**.\n",
    "\n",
    "## Task: Circle Inside Ring\n",
    "\n",
    "```\n",
    "         ████████████\n",
    "       ██            ██\n",
    "      █    ░░░░░░░░    █\n",
    "     █   ░░░░░░░░░░░░   █\n",
    "     █   ░░░░░░░░░░░░   █     ░ = Class 0 (inner circle)\n",
    "     █   ░░░░░░░░░░░░   █     █ = Class 1 (outer ring)\n",
    "      █    ░░░░░░░░    █\n",
    "       ██            ██\n",
    "         ████████████\n",
    "```\n",
    "\n",
    "This is a **nonlinear** classification problem - a single linear classifier cannot solve it!\n",
    "\n",
    "## Why This is Good for SOEN\n",
    "\n",
    "Unlike linear regression, this task **benefits** from the SingleDendrite's nonlinear dynamics.\n",
    "The question: How many neurons do we need to solve this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Generate Circle-in-Ring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circle_ring_data(n_samples=500, inner_radius=0.3, outer_radius_min=0.5, \n",
    "                               outer_radius_max=0.8, noise=0.05):\n",
    "    \"\"\"\n",
    "    Generate 2D classification data: circle inside a ring.\n",
    "    \n",
    "    Class 0: Points inside inner circle (r < inner_radius)\n",
    "    Class 1: Points in outer ring (outer_radius_min < r < outer_radius_max)\n",
    "    \n",
    "    Returns:\n",
    "        X: [n_samples, 2] - (x, y) coordinates\n",
    "        y: [n_samples] - class labels (0 or 1)\n",
    "    \"\"\"\n",
    "    n_each = n_samples // 2\n",
    "    \n",
    "    # Class 0: Inner circle\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_inner = np.random.uniform(0, inner_radius, n_each)\n",
    "    x_inner = r_inner * np.cos(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    y_inner = r_inner * np.sin(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    # Class 1: Outer ring\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_outer = np.random.uniform(outer_radius_min, outer_radius_max, n_each)\n",
    "    x_outer = r_outer * np.cos(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    y_outer = r_outer * np.sin(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([\n",
    "        np.column_stack([x_inner, y_inner]),\n",
    "        np.column_stack([x_outer, y_outer])\n",
    "    ])\n",
    "    y = np.array([0] * n_each + [1] * n_each)\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(len(y))\n",
    "    X, y = X[idx], y[idx]\n",
    "    \n",
    "    # Scale to SOEN operating range [0, 0.3]\n",
    "    X = (X + 1) / 2 * 0.25 + 0.025  # Map [-1, 1] to [0.025, 0.275]\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "# Generate data\n",
    "N_SAMPLES = 500\n",
    "X_data, y_data = generate_circle_ring_data(N_SAMPLES)\n",
    "\n",
    "print(f\"Dataset shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "print(f\"Class distribution: {(y_data == 0).sum().item()} inner, {(y_data == 1).sum().item()} outer\")\n",
    "print(f\"X range: [{X_data.min():.3f}, {X_data.max():.3f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = ['blue', 'red']\n",
    "for c in [0, 1]:\n",
    "    mask = y_data == c\n",
    "    label = 'Inner circle (class 0)' if c == 0 else 'Outer ring (class 1)'\n",
    "    plt.scatter(X_data[mask, 0], X_data[mask, 1], c=colors[c], \n",
    "                alpha=0.6, s=30, label=label)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Binary Classification: Circle vs Ring')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for SOEN\n",
    "\n",
    "SOEN expects temporal sequences. We'll use constant inputs over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50  # Time steps for SOEN dynamics to settle\n",
    "\n",
    "# Expand to sequence: [N, T, 2]\n",
    "X_seq = X_data.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "y_labels = y_data.unsqueeze(1)  # [N, 1]\n",
    "\n",
    "print(f\"SOEN input shape: {X_seq.shape}\")\n",
    "print(f\"Labels shape: {y_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Model Builder for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(hidden_dims, input_dim=2, dt=50.0):\n",
    "    \"\"\"\n",
    "    Build a SOEN classifier.\n",
    "    \n",
    "    Architecture: 2 (input) → hidden → 1 (output)\n",
    "    \n",
    "    Args:\n",
    "        hidden_dims: List of hidden layer sizes\n",
    "        input_dim: Input dimension (2 for x,y coordinates)\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=dt,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = []\n",
    "    connections = []\n",
    "    \n",
    "    # Input layer (dim=2 for x, y)\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": input_dim},\n",
    "    ))\n",
    "    \n",
    "    # Hidden layers\n",
    "    prev_dim = input_dim\n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        \n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "                \"learnable_params\": {\n",
    "                    \"phi_offset\": False,\n",
    "                    \"bias_current\": False,\n",
    "                    \"gamma_plus\": False,\n",
    "                    \"gamma_minus\": False,\n",
    "                },\n",
    "            },\n",
    "        ))\n",
    "        \n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "        \n",
    "        prev_dim = hidden_dim\n",
    "    \n",
    "    # Output layer (dim=1 for binary classification)\n",
    "    output_layer_id = len(hidden_dims) + 1\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=output_layer_id,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    ))\n",
    "    \n",
    "    connections.append(ConnectionConfig(\n",
    "        from_layer=output_layer_id - 1,\n",
    "        to_layer=output_layer_id,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"xavier_uniform\"},\n",
    "    ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Test builder\n",
    "print(\"Testing classifier builder...\")\n",
    "test_configs = [\n",
    "    ([2], \"2→2→1\"),\n",
    "    ([4], \"2→4→1\"),\n",
    "    ([8], \"2→8→1\"),\n",
    "    ([4, 4], \"2→4→4→1\"),\n",
    "]\n",
    "\n",
    "for hidden_dims, name in test_configs:\n",
    "    model = build_classifier(hidden_dims)\n",
    "    n_params = count_params(model)\n",
    "    layer_dims = [l.dim for l in model.layers]\n",
    "    print(f\"  {name}: layers={layer_dims}, params={n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Training Function for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, X_train, y_train, n_epochs=300, lr=0.02, verbose=False):\n",
    "    \"\"\"\n",
    "    Train SOEN classifier with BCE loss.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        final_hist, _ = model(X_train)\n",
    "        logits = final_hist[:, -1, :]  # [N, 1]\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(logits, y_train)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            acc = (preds == y_train).float().mean().item()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(acc)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"  Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc:.4f}\")\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "\n",
    "def evaluate_classifier(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate classifier and return predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_hist, _ = model(X_test)\n",
    "        logits = final_hist[:, -1, :]\n",
    "        probs = torch.sigmoid(logits).squeeze().numpy()\n",
    "        preds = (probs > 0.5).astype(float)\n",
    "    \n",
    "    y_true = y_test.squeeze().numpy()\n",
    "    accuracy = (preds == y_true).mean()\n",
    "    \n",
    "    return preds, probs, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Train All Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architectures to compare (input_dim=2 for all)\n",
    "ARCHITECTURES = {\n",
    "    \"2 neurons\": [2],\n",
    "    \"4 neurons\": [4],\n",
    "    \"8 neurons\": [8],\n",
    "    \"16 neurons\": [16],\n",
    "    \"32 neurons\": [32],\n",
    "    \"4→4 (deep)\": [4, 4],\n",
    "    \"8→8 (deep)\": [8, 8],\n",
    "    \"4→4→4 (3 layers)\": [4, 4, 4],\n",
    "}\n",
    "\n",
    "N_EPOCHS = 400\n",
    "LR = 0.02\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training all architectures...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, hidden_dims in ARCHITECTURES.items():\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    \n",
    "    model = build_classifier(hidden_dims)\n",
    "    n_params = count_params(model)\n",
    "    \n",
    "    losses, accuracies = train_classifier(\n",
    "        model, X_seq, y_labels, n_epochs=N_EPOCHS, lr=LR, verbose=False\n",
    "    )\n",
    "    \n",
    "    preds, probs, final_acc = evaluate_classifier(model, X_seq, y_labels)\n",
    "    \n",
    "    results[name] = {\n",
    "        'hidden_dims': hidden_dims,\n",
    "        'n_params': n_params,\n",
    "        'losses': losses,\n",
    "        'accuracies': accuracies,\n",
    "        'final_acc': final_acc,\n",
    "        'final_loss': losses[-1],\n",
    "        'preds': preds,\n",
    "        'probs': probs,\n",
    "        'model': model,\n",
    "    }\n",
    "    \n",
    "    print(f\"  Params: {n_params}, Accuracy: {final_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Compare Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(results)))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    ax1.plot(res['losses'], label=f\"{name}\", color=color, lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('BCE Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2 = axes[1]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    ax2.plot(res['accuracies'], label=f\"{name}\", color=color, lw=2)\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect')\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Decision Boundary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X_data, y_data, ax, title, resolution=100):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for a 2D classifier.\n",
    "    \"\"\"\n",
    "    # Create grid\n",
    "    x_min, x_max = X_data[:, 0].min() - 0.02, X_data[:, 0].max() + 0.02\n",
    "    y_min, y_max = X_data[:, 1].min() - 0.02, X_data[:, 1].max() + 0.02\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution),\n",
    "        np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "    \n",
    "    # Prepare grid as SOEN input\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.FloatTensor(grid_points)\n",
    "    grid_seq = grid_tensor.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_hist, _ = model(grid_seq)\n",
    "        logits = final_hist[:, -1, :]\n",
    "        probs = torch.sigmoid(logits).squeeze().numpy()\n",
    "    \n",
    "    Z = probs.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.7)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    # Plot data points\n",
    "    for c, color in enumerate(['blue', 'red']):\n",
    "        mask = y_data.squeeze() == c\n",
    "        ax.scatter(X_data[mask, 0], X_data[mask, 1], c=color, \n",
    "                   s=20, alpha=0.6, edgecolors='white', linewidths=0.5)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "# Plot decision boundaries for all architectures\n",
    "n_models = len(results)\n",
    "cols = 4\n",
    "rows = (n_models + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "X_np = X_data.numpy()\n",
    "y_np = y_data.numpy()\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    plot_decision_boundary(\n",
    "        res['model'], X_np, y_np, ax,\n",
    "        f\"{name}\\nAcc={res['final_acc']:.3f}, Params={res['n_params']}\"\n",
    "    )\n",
    "\n",
    "# Hide unused\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Performance vs Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = list(results.keys())\n",
    "n_params = [results[n]['n_params'] for n in names]\n",
    "accuracies = [results[n]['final_acc'] for n in names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(results)))\n",
    "\n",
    "# Bar chart\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(range(len(names)), accuracies, color=colors)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', label='Random (50%)')\n",
    "ax1.axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect (100%)')\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Final Accuracy by Architecture')\n",
    "ax1.set_ylim(0.4, 1.05)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Scatter: params vs accuracy\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(n_params, accuracies, s=150, c=colors, edgecolors='black', zorder=5)\n",
    "for i, name in enumerate(names):\n",
    "    ax2.annotate(name, (n_params[i], accuracies[i]), \n",
    "                 textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Number of Trainable Parameters')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Accuracy vs Model Complexity')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Best vs Worst Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best and worst\n",
    "best_name = max(results, key=lambda x: results[x]['final_acc'])\n",
    "worst_name = min(results, key=lambda x: results[x]['final_acc'])\n",
    "\n",
    "print(f\"Best: {best_name} (Accuracy: {results[best_name]['final_acc']:.4f})\")\n",
    "print(f\"Worst: {worst_name} (Accuracy: {results[worst_name]['final_acc']:.4f})\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for ax, name, title in zip(axes, [worst_name, best_name], ['Worst (Simplest)', 'Best']):\n",
    "    res = results[name]\n",
    "    plot_decision_boundary(\n",
    "        res['model'], X_np, y_np, ax,\n",
    "        f\"{title}: {name}\\nAccuracy={res['final_acc']:.4f}, Params={res['n_params']}\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for name, res in results.items():\n",
    "    n_hidden = sum(res['hidden_dims'])\n",
    "    n_layers = len(res['hidden_dims'])\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Architecture': name,\n",
    "        'Hidden Neurons': n_hidden,\n",
    "        'Hidden Layers': n_layers,\n",
    "        'Trainable Params': res['n_params'],\n",
    "        'Final Accuracy': f\"{res['final_acc']:.4f}\",\n",
    "        'Final Loss': f\"{res['final_loss']:.4f}\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df = df.sort_values('Trainable Params')\n",
    "\n",
    "print(\"=\" * 90)\n",
    "print(\"CLASSIFICATION COMPLEXITY STUDY: SUMMARY\")\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nTask: Binary classification (Circle vs Ring)\")\n",
    "print(f\"Input dimension: 2 (x, y coordinates)\")\n",
    "print(f\"Training samples: {N_SAMPLES}\")\n",
    "print(f\"Training epochs: {N_EPOCHS}\")\n",
    "print()\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze trends\n",
    "wide_models = ['2 neurons', '4 neurons', '8 neurons', '16 neurons', '32 neurons']\n",
    "wide_accs = [results[n]['final_acc'] for n in wide_models if n in results]\n",
    "\n",
    "print(\"\\n1. EFFECT OF WIDTH (more neurons in hidden layer):\")\n",
    "for name in wide_models:\n",
    "    if name in results:\n",
    "        print(f\"   {name}: Accuracy = {results[name]['final_acc']:.4f}\")\n",
    "\n",
    "if wide_accs[-1] > wide_accs[0]:\n",
    "    improvement = (wide_accs[-1] - wide_accs[0]) * 100\n",
    "    print(f\"   → More neurons HELPS (+{improvement:.1f}% accuracy)\")\n",
    "\n",
    "print(\"\\n2. EFFECT OF DEPTH:\")\n",
    "if '4→4 (deep)' in results and '8 neurons' in results:\n",
    "    shallow_8 = results['8 neurons']['final_acc']\n",
    "    deep_4x4 = results['4→4 (deep)']['final_acc']\n",
    "    print(f\"   8 neurons (1 layer):  Acc = {shallow_8:.4f}, Params = {results['8 neurons']['n_params']}\")\n",
    "    print(f\"   4→4 (2 layers):       Acc = {deep_4x4:.4f}, Params = {results['4→4 (deep)']['n_params']}\")\n",
    "    if deep_4x4 > shallow_8:\n",
    "        print(f\"   → Depth helps with similar parameter count!\")\n",
    "\n",
    "print(\"\\n3. COMPARISON TO LINEAR REGRESSION:\")\n",
    "print(\"   Unlike linear regression, this nonlinear classification task\")\n",
    "print(\"   BENEFITS from the SingleDendrite's nonlinear dynamics.\")\n",
    "print(f\"   Best accuracy achieved: {results[best_name]['final_acc']:.4f}\")\n",
    "\n",
    "print(\"\\n4. HARDWARE REQUIREMENTS FOR BEST MODEL:\")\n",
    "best_res = results[best_name]\n",
    "print(f\"   Architecture: {best_name}\")\n",
    "print(f\"   SingleDendrite neurons: {sum(best_res['hidden_dims'])}\")\n",
    "print(f\"   Trainable connections: {best_res['n_params']}\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHT:\")\n",
    "if results[best_name]['final_acc'] > 0.9:\n",
    "    print(\"   ✓ SOEN can effectively solve nonlinear classification!\")\n",
    "    print(\"   ✓ The nonlinear dynamics are an ASSET for this task.\")\n",
    "else:\n",
    "    print(\"   More neurons or training may be needed for higher accuracy.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
