{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Optimizer Comparison\n",
    "\n",
    "## Simple 2-Parameter Linear Regression\n",
    "\n",
    "This notebook compares different PyTorch optimizers on a minimal regression task:\n",
    "\n",
    "$$y = w \\cdot x + b + \\epsilon$$\n",
    "\n",
    "Where:\n",
    "- `w` (weight) and `b` (bias) are the 2 learnable parameters\n",
    "- We can visualize the optimization trajectory in 2D parameter space\n",
    "\n",
    "### Optimizers Compared\n",
    "1. **SGD** - Vanilla Stochastic Gradient Descent\n",
    "2. **SGD + Momentum** - SGD with momentum\n",
    "3. **Adam** - Adaptive Moment Estimation\n",
    "4. **AdamW** - Adam with decoupled weight decay\n",
    "5. **RMSprop** - Root Mean Square Propagation\n",
    "6. **Adagrad** - Adaptive Gradient\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import copy\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "Create a simple linear dataset with known ground truth parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth parameters\n",
    "TRUE_W = 2.5\n",
    "TRUE_B = -1.0\n",
    "\n",
    "# Generate data\n",
    "N_SAMPLES = 100\n",
    "NOISE_STD = 0.3\n",
    "\n",
    "x_data = torch.linspace(-2, 2, N_SAMPLES).reshape(-1, 1)\n",
    "y_data = TRUE_W * x_data + TRUE_B + NOISE_STD * torch.randn(N_SAMPLES, 1)\n",
    "\n",
    "print(f\"Data shape: x={x_data.shape}, y={y_data.shape}\")\n",
    "print(f\"Ground truth: w={TRUE_W}, b={TRUE_B}\")\n",
    "\n",
    "# Visualize data\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_data.numpy(), y_data.numpy(), alpha=0.6, label='Data points')\n",
    "plt.plot(x_data.numpy(), TRUE_W * x_data.numpy() + TRUE_B, 'r-', linewidth=2, label=f'True: y = {TRUE_W}x + {TRUE_B}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Linear Regression Data')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Simple 2-Parameter Model\n",
    "\n",
    "A minimal linear model with just weight `w` and bias `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearModel(nn.Module):\n",
    "    \"\"\"Minimal 2-parameter linear model: y = w*x + b\"\"\"\n",
    "    \n",
    "    def __init__(self, init_w=0.0, init_b=0.0):\n",
    "        super().__init__()\n",
    "        # Single weight and bias\n",
    "        self.w = nn.Parameter(torch.tensor([init_w]))\n",
    "        self.b = nn.Parameter(torch.tensor([init_b]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.w * x + self.b\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"Return current (w, b) as tuple\"\"\"\n",
    "        return self.w.item(), self.b.item()\n",
    "\n",
    "# Test the model\n",
    "test_model = SimpleLinearModel(init_w=0.5, init_b=0.5)\n",
    "print(f\"Model parameters: w={test_model.w.item():.3f}, b={test_model.b.item():.3f}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in test_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Loss Landscape\n",
    "\n",
    "Plot the MSE loss surface in (w, b) parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_landscape(x_data, y_data, w_range, b_range, resolution=100):\n",
    "    \"\"\"Compute MSE loss for a grid of (w, b) values.\"\"\"\n",
    "    w_vals = np.linspace(w_range[0], w_range[1], resolution)\n",
    "    b_vals = np.linspace(b_range[0], b_range[1], resolution)\n",
    "    W, B = np.meshgrid(w_vals, b_vals)\n",
    "    \n",
    "    loss_surface = np.zeros_like(W)\n",
    "    \n",
    "    for i in range(resolution):\n",
    "        for j in range(resolution):\n",
    "            w, b = W[i, j], B[i, j]\n",
    "            y_pred = w * x_data.numpy() + b\n",
    "            loss = np.mean((y_pred - y_data.numpy()) ** 2)\n",
    "            loss_surface[i, j] = loss\n",
    "    \n",
    "    return W, B, loss_surface\n",
    "\n",
    "# Compute loss landscape\n",
    "W_RANGE = (-1, 5)\n",
    "B_RANGE = (-4, 2)\n",
    "W, B, loss_surface = compute_loss_landscape(x_data, y_data, W_RANGE, B_RANGE)\n",
    "\n",
    "# Plot loss landscape\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(W, B, loss_surface, cmap='viridis', alpha=0.8)\n",
    "ax1.scatter([TRUE_W], [TRUE_B], [0], color='red', s=100, marker='*', label='Optimum')\n",
    "ax1.set_xlabel('w (weight)')\n",
    "ax1.set_ylabel('b (bias)')\n",
    "ax1.set_zlabel('MSE Loss')\n",
    "ax1.set_title('Loss Landscape (3D)')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = axes[1]\n",
    "contour = ax2.contour(W, B, loss_surface, levels=30, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.scatter([TRUE_W], [TRUE_B], color='red', s=100, marker='*', zorder=5, label=f'Optimum ({TRUE_W}, {TRUE_B})')\n",
    "ax2.set_xlabel('w (weight)')\n",
    "ax2.set_ylabel('b (bias)')\n",
    "ax2.set_title('Loss Landscape (Contour)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function with Trajectory Tracking\n",
    "\n",
    "Train the model and record parameter trajectory for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(optimizer_class, optimizer_kwargs, x_data, y_data, \n",
    "                init_w=0.0, init_b=0.0, n_epochs=100, batch_size=None):\n",
    "    \"\"\"\n",
    "    Train a simple linear model and track the optimization trajectory.\n",
    "    \n",
    "    Args:\n",
    "        optimizer_class: PyTorch optimizer class\n",
    "        optimizer_kwargs: Dict of optimizer arguments (lr, momentum, etc.)\n",
    "        x_data, y_data: Training data\n",
    "        init_w, init_b: Initial parameter values\n",
    "        n_epochs: Number of training epochs\n",
    "        batch_size: If None, use full batch; otherwise mini-batch\n",
    "    \n",
    "    Returns:\n",
    "        Dict with losses, trajectory, final model\n",
    "    \"\"\"\n",
    "    # Create fresh model\n",
    "    model = SimpleLinearModel(init_w=init_w, init_b=init_b)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), **optimizer_kwargs)\n",
    "    \n",
    "    # Track metrics\n",
    "    losses = []\n",
    "    trajectory = [model.get_params()]  # Starting point\n",
    "    \n",
    "    n_samples = len(x_data)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        if batch_size is None:\n",
    "            # Full batch gradient descent\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x_data)\n",
    "            loss = criterion(y_pred, y_data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            trajectory.append(model.get_params())\n",
    "        else:\n",
    "            # Mini-batch SGD\n",
    "            indices = torch.randperm(n_samples)\n",
    "            epoch_loss = 0.0\n",
    "            n_batches = 0\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_idx = indices[i:i+batch_size]\n",
    "                x_batch = x_data[batch_idx]\n",
    "                y_batch = y_data[batch_idx]\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x_batch)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "                n_batches += 1\n",
    "                trajectory.append(model.get_params())\n",
    "            \n",
    "            losses.append(epoch_loss / n_batches)\n",
    "    \n",
    "    return {\n",
    "        'losses': losses,\n",
    "        'trajectory': trajectory,\n",
    "        'final_w': model.w.item(),\n",
    "        'final_b': model.b.item(),\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Optimizers\n",
    "\n",
    "Train with different optimizers and compare their behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer configurations\n",
    "OPTIMIZERS = {\n",
    "    'SGD (lr=0.01)': (torch.optim.SGD, {'lr': 0.01}),\n",
    "    'SGD (lr=0.1)': (torch.optim.SGD, {'lr': 0.1}),\n",
    "    'SGD + Momentum': (torch.optim.SGD, {'lr': 0.01, 'momentum': 0.9}),\n",
    "    'Adam (lr=0.1)': (torch.optim.Adam, {'lr': 0.1}),\n",
    "    'Adam (lr=0.01)': (torch.optim.Adam, {'lr': 0.01}),\n",
    "    'AdamW': (torch.optim.AdamW, {'lr': 0.1}),\n",
    "    'RMSprop': (torch.optim.RMSprop, {'lr': 0.01}),\n",
    "    'Adagrad': (torch.optim.Adagrad, {'lr': 0.1}),\n",
    "}\n",
    "\n",
    "# Common starting point (away from optimum)\n",
    "INIT_W = 0.0\n",
    "INIT_B = 0.0\n",
    "N_EPOCHS = 50\n",
    "\n",
    "# Train with each optimizer\n",
    "results = {}\n",
    "for name, (opt_class, opt_kwargs) in OPTIMIZERS.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    results[name] = train_model(\n",
    "        opt_class, opt_kwargs, x_data, y_data,\n",
    "        init_w=INIT_W, init_b=INIT_B, n_epochs=N_EPOCHS\n",
    "    )\n",
    "    print(f\"  Final: w={results[name]['final_w']:.4f}, b={results[name]['final_b']:.4f}, \"\n",
    "          f\"loss={results[name]['losses'][-1]:.6f}\")\n",
    "\n",
    "print(f\"\\nGround truth: w={TRUE_W}, b={TRUE_B}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear scale\n",
    "ax1 = axes[0]\n",
    "for name, res in results.items():\n",
    "    ax1.plot(res['losses'], label=name, linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Loss Curves (Linear Scale)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "ax2 = axes[1]\n",
    "for name, res in results.items():\n",
    "    ax2.plot(res['losses'], label=name, linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MSE Loss (log scale)')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title('Loss Curves (Log Scale)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Optimization Trajectories\n",
    "\n",
    "Show how each optimizer navigates the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trajectories on contour\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Background contour\n",
    "contour = ax.contour(W, B, loss_surface, levels=30, cmap='gray', alpha=0.5)\n",
    "ax.contourf(W, B, loss_surface, levels=30, cmap='viridis', alpha=0.3)\n",
    "\n",
    "# Color map for optimizers\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "\n",
    "# Plot each trajectory\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    traj = np.array(res['trajectory'])\n",
    "    ax.plot(traj[:, 0], traj[:, 1], '-', color=color, linewidth=2, label=name, alpha=0.8)\n",
    "    ax.scatter(traj[0, 0], traj[0, 1], color=color, s=100, marker='o', edgecolor='black', zorder=5)  # Start\n",
    "    ax.scatter(traj[-1, 0], traj[-1, 1], color=color, s=100, marker='s', edgecolor='black', zorder=5)  # End\n",
    "\n",
    "# Mark optimum\n",
    "ax.scatter([TRUE_W], [TRUE_B], color='red', s=200, marker='*', zorder=10, label=f'Optimum ({TRUE_W}, {TRUE_B})')\n",
    "\n",
    "# Mark start\n",
    "ax.scatter([INIT_W], [INIT_B], color='black', s=150, marker='X', zorder=10, label=f'Start ({INIT_W}, {INIT_B})')\n",
    "\n",
    "ax.set_xlabel('w (weight)', fontsize=12)\n",
    "ax.set_ylabel('b (bias)', fontsize=12)\n",
    "ax.set_title('Optimization Trajectories in Parameter Space', fontsize=14)\n",
    "ax.legend(loc='upper left', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(W_RANGE)\n",
    "ax.set_ylim(B_RANGE)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Trajectory Comparison (Zoomed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoomed view near optimum\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Zoom range\n",
    "zoom_w = (TRUE_W - 1.5, TRUE_W + 1.0)\n",
    "zoom_b = (TRUE_B - 1.5, TRUE_B + 1.5)\n",
    "\n",
    "# Recompute loss landscape for zoomed view\n",
    "W_zoom, B_zoom, loss_zoom = compute_loss_landscape(x_data, y_data, zoom_w, zoom_b, resolution=50)\n",
    "\n",
    "for idx, ((name, res), color) in enumerate(zip(results.items(), colors)):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Background\n",
    "    ax.contourf(W_zoom, B_zoom, loss_zoom, levels=20, cmap='viridis', alpha=0.4)\n",
    "    ax.contour(W_zoom, B_zoom, loss_zoom, levels=20, cmap='gray', alpha=0.3)\n",
    "    \n",
    "    # Trajectory\n",
    "    traj = np.array(res['trajectory'])\n",
    "    ax.plot(traj[:, 0], traj[:, 1], 'o-', color=color, linewidth=2, markersize=3, alpha=0.8)\n",
    "    ax.scatter(traj[0, 0], traj[0, 1], color='black', s=80, marker='o', zorder=5, label='Start')\n",
    "    ax.scatter(traj[-1, 0], traj[-1, 1], color=color, s=80, marker='s', edgecolor='black', zorder=5, label='End')\n",
    "    \n",
    "    # Optimum\n",
    "    ax.scatter([TRUE_W], [TRUE_B], color='red', s=100, marker='*', zorder=10)\n",
    "    \n",
    "    ax.set_title(f'{name}\\nFinal loss: {res[\"losses\"][-1]:.6f}', fontsize=10)\n",
    "    ax.set_xlabel('w')\n",
    "    ax.set_ylabel('b')\n",
    "    ax.set_xlim(zoom_w)\n",
    "    ax.set_ylim(zoom_b)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Zoomed Trajectories Near Optimum', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Parameter Evolution Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Weight evolution\n",
    "ax1 = axes[0]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    traj = np.array(res['trajectory'])\n",
    "    ax1.plot(traj[:, 0], label=name, color=color, linewidth=2)\n",
    "ax1.axhline(y=TRUE_W, color='red', linestyle='--', linewidth=2, label=f'True w={TRUE_W}')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('w (weight)')\n",
    "ax1.set_title('Weight (w) Evolution')\n",
    "ax1.legend(loc='lower right', fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Bias evolution\n",
    "ax2 = axes[1]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    traj = np.array(res['trajectory'])\n",
    "    ax2.plot(traj[:, 1], label=name, color=color, linewidth=2)\n",
    "ax2.axhline(y=TRUE_B, color='red', linestyle='--', linewidth=2, label=f'True b={TRUE_B}')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('b (bias)')\n",
    "ax2.set_title('Bias (b) Evolution')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for name, res in results.items():\n",
    "    w_error = abs(res['final_w'] - TRUE_W)\n",
    "    b_error = abs(res['final_b'] - TRUE_B)\n",
    "    param_dist = np.sqrt(w_error**2 + b_error**2)\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Optimizer': name,\n",
    "        'Final Loss': f\"{res['losses'][-1]:.6f}\",\n",
    "        'Final w': f\"{res['final_w']:.4f}\",\n",
    "        'Final b': f\"{res['final_b']:.4f}\",\n",
    "        'w Error': f\"{w_error:.4f}\",\n",
    "        'b Error': f\"{b_error:.4f}\",\n",
    "        'Distance to Optimum': f\"{param_dist:.4f}\",\n",
    "        'Steps': len(res['trajectory'])\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"=\" * 100)\n",
    "print(\"OPTIMIZER COMPARISON SUMMARY\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nGround Truth: w = {TRUE_W}, b = {TRUE_B}\")\n",
    "print(f\"Starting Point: w = {INIT_W}, b = {INIT_B}\")\n",
    "print(f\"Epochs: {N_EPOCHS}\\n\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Convergence Speed Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochs_to_threshold(losses, threshold):\n",
    "    \"\"\"Return number of epochs to reach loss threshold, or None if not reached.\"\"\"\n",
    "    for i, loss in enumerate(losses):\n",
    "        if loss <= threshold:\n",
    "            return i + 1\n",
    "    return None\n",
    "\n",
    "# Analyze convergence speed\n",
    "thresholds = [1.0, 0.5, 0.2, 0.1, 0.15]\n",
    "\n",
    "print(\"Epochs to reach loss threshold:\")\n",
    "print(\"-\" * 80)\n",
    "header = f\"{'Optimizer':<25}\" + \"\".join([f\"Loss<{t:<6}\" for t in thresholds])\n",
    "print(header)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, res in results.items():\n",
    "    row = f\"{name:<25}\"\n",
    "    for thresh in thresholds:\n",
    "        epochs = epochs_to_threshold(res['losses'], thresh)\n",
    "        row += f\"{str(epochs) if epochs else 'N/A':<10}\"\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Mini-Batch vs Full-Batch Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare full-batch vs mini-batch for Adam\n",
    "batch_sizes = [None, 50, 20, 10, 5]  # None = full batch\n",
    "batch_results = {}\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    name = f\"Batch={bs if bs else 'Full'}\"\n",
    "    batch_results[name] = train_model(\n",
    "        torch.optim.Adam, {'lr': 0.1}, x_data, y_data,\n",
    "        init_w=INIT_W, init_b=INIT_B, n_epochs=20, batch_size=bs\n",
    "    )\n",
    "\n",
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "for name, res in batch_results.items():\n",
    "    ax1.plot(res['losses'], label=name, linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Adam: Batch Size Comparison (Loss)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "colors_batch = plt.cm.viridis(np.linspace(0, 1, len(batch_results)))\n",
    "for (name, res), color in zip(batch_results.items(), colors_batch):\n",
    "    traj = np.array(res['trajectory'])\n",
    "    ax2.plot(traj[:, 0], traj[:, 1], '-', color=color, linewidth=1.5, label=name, alpha=0.7)\n",
    "\n",
    "ax2.contour(W, B, loss_surface, levels=20, cmap='gray', alpha=0.3)\n",
    "ax2.scatter([TRUE_W], [TRUE_B], color='red', s=100, marker='*', zorder=10)\n",
    "ax2.scatter([INIT_W], [INIT_B], color='black', s=100, marker='X', zorder=10)\n",
    "ax2.set_xlabel('w (weight)')\n",
    "ax2.set_ylabel('b (bias)')\n",
    "ax2.set_title('Adam: Batch Size Comparison (Trajectory)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(W_RANGE)\n",
    "ax2.set_ylim(B_RANGE)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Key Observations\n",
    "\n",
    "### Summary of Optimizer Behaviors:\n",
    "\n",
    "| Optimizer | Characteristics |\n",
    "|-----------|----------------|\n",
    "| **SGD (low lr)** | Slow, steady convergence; may not reach optimum in limited epochs |\n",
    "| **SGD (high lr)** | Faster but can overshoot; may oscillate |\n",
    "| **SGD + Momentum** | Accelerates convergence; smooths trajectory |\n",
    "| **Adam** | Adaptive learning rate; fast convergence; good default choice |\n",
    "| **AdamW** | Adam with proper weight decay; similar to Adam for this simple case |\n",
    "| **RMSprop** | Adaptive; predecessor to Adam; good for non-stationary objectives |\n",
    "| **Adagrad** | Accumulates gradients; learning rate decreases over time |\n",
    "\n",
    "### For SOEN Training:\n",
    "- **Adam** is typically a good default choice\n",
    "- **Learning rate** is crucial - too high causes instability, too low is slow\n",
    "- **Momentum** helps escape local minima and smooth noisy gradients\n",
    "- The loss landscape shape affects which optimizer works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
