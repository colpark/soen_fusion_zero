{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Tutorial 02 — Train a SOEN Model (ORIGINAL - Control Experiment)\n\n**This notebook uses the ORIGINAL model spec with `J_1_to_2.learnable: false`**\n\n---\n\n## ⚠️ NOISE CONFIGURATION: DISABLED\n\n> **This tutorial runs with NO NOISE INJECTION (ideal conditions).**\n>\n> | Parameter | Value | Description |\n> |-----------|-------|-------------|\n> | `phi` | **0.0** | Flux noise (thermal fluctuations) |\n> | `s` | **0.0** | State noise (integration errors) |\n> | `g` | **0.0** | Source function noise |\n> | `bias_current` | **0.0** | Bias current noise |\n> | `j` | **0.0** | Connection weight noise |\n>\n> To enable hardware-realistic noise, modify the `noise` sections in:\n> `training/test_models/model_specs/1D_5D_2D_PulseNetSpec.yaml`\n\n---\n\nThis serves as a control experiment to demonstrate that even with:\n- Higher learning rate (0.001 vs 0.0001)\n- More epochs (50 vs 10)\n- Same optimizer and scheduler settings\n\nThe model **will NOT learn** because gradient flow is blocked at the output connection.\n\n### Expected Result\n- Accuracy should remain ~50% (random guessing)\n- Loss should stay near ln(2) ≈ 0.693\n\n### Compare With\nRun `02_train_a_model.ipynb` which uses the FIXED config with `J_1_to_2.learnable: true`\nto see proper training.\n\n### ML Task Overview\nBinary classification on time-series inputs:\n- **Class 0**: Single pulse\n- **Class 1**: Two distinct pulses"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Ensure soen_toolkit is importable\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path if running from notebook location\n",
    "notebook_dir = Path.cwd()\n",
    "for parent in [notebook_dir] + list(notebook_dir.parents):\n",
    "    candidate = parent / \"src\"\n",
    "    if (candidate / \"soen_toolkit\").exists():\n",
    "        sys.path.insert(0, str(candidate))\n",
    "        break\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "from soen_toolkit.training.trainers.experiment import run_from_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "**Visualize the Dataset**\n",
    "\n",
    "Let's look at examples from each class to understand the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE DATASET: One-pulse vs Two-pulse classification\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_dataset(data_path=\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\", n_examples=4):\n",
    "    \"\"\"Visualize examples from each class in the dataset.\"\"\"\n",
    "    \n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        data = np.array(f['train']['data'])\n",
    "        labels = np.array(f['train']['labels'])\n",
    "    \n",
    "    print(f\"Dataset shape: {data.shape} (N samples, T timesteps, D features)\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
    "    \n",
    "    # Find examples of each class\n",
    "    class_0_idx = np.where(labels == 0)[0][:n_examples]\n",
    "    class_1_idx = np.where(labels == 1)[0][:n_examples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 5))\n",
    "    fig.suptitle(\"Input Signals: One-Pulse (Class 0) vs Two-Pulse (Class 1)\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot Class 0 (single pulse)\n",
    "    for i, idx in enumerate(class_0_idx):\n",
    "        axes[0, i].plot(data[idx, :, 0], 'b-', linewidth=1.5)\n",
    "        axes[0, i].set_title(f\"Sample {idx}\", fontsize=10)\n",
    "        axes[0, i].set_ylim(-0.1, 1.1)\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel(\"Class 0\\n(One Pulse)\", fontsize=10)\n",
    "    \n",
    "    # Plot Class 1 (two pulses)\n",
    "    for i, idx in enumerate(class_1_idx):\n",
    "        axes[1, i].plot(data[idx, :, 0], 'r-', linewidth=1.5)\n",
    "        axes[1, i].set_title(f\"Sample {idx}\", fontsize=10)\n",
    "        axes[1, i].set_ylim(-0.1, 1.1)\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel(\"Class 1\\n(Two Pulses)\", fontsize=10)\n",
    "        axes[1, i].set_xlabel(\"Time step\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Visualize the dataset\n",
    "train_data, train_labels = visualize_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "**Training (ORIGINAL CONFIG - Control Experiment)**\n",
    "\n",
    "This uses the ORIGINAL model spec with `J_1_to_2.learnable: false`.\n",
    "Even with 50 epochs and higher LR, accuracy should stay ~50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training with ORIGINAL (broken) config\n",
    "# Uses: 1D_5D_2D_PulseNetSpec.yaml with J_1_to_2.learnable = FALSE\n",
    "# Expected: ~50% accuracy (random guessing) due to blocked gradients\n",
    "run_from_config(\"training/training_configs/pulse_net_orig_long.yaml\", script_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "**Visualize Predictions**\n",
    "\n",
    "After training, let's see how the model performs. Expected: ~50% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZE PREDICTIONS: Show model predictions on test samples\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_predictions(n_samples=8):\n",
    "    \"\"\"Load trained model and visualize predictions on test data.\"\"\"\n",
    "    \n",
    "    from soen_toolkit.core.model_yaml import build_model_from_yaml\n",
    "    \n",
    "    # Find the latest checkpoint\n",
    "    ckpt_patterns = [\n",
    "        \"training/temp/**/checkpoints/**/*.ckpt\",\n",
    "        \"training/temp/**/*.ckpt\",\n",
    "    ]\n",
    "    \n",
    "    all_ckpts = []\n",
    "    for pattern in ckpt_patterns:\n",
    "        all_ckpts.extend(glob.glob(pattern, recursive=True))\n",
    "    \n",
    "    if not all_ckpts:\n",
    "        print(\"No checkpoint found. Run training first.\")\n",
    "        return\n",
    "    \n",
    "    latest_ckpt = max(all_ckpts, key=lambda x: Path(x).stat().st_mtime)\n",
    "    print(f\"Loading checkpoint: {latest_ckpt}\")\n",
    "    \n",
    "    # Load model architecture and weights - using ORIGINAL spec\n",
    "    model_path = Path(\"training/test_models/model_specs/1D_5D_2D_PulseNetSpec.yaml\")\n",
    "    model = build_model_from_yaml(model_path)\n",
    "    \n",
    "    # Load trained weights\n",
    "    ckpt = torch.load(latest_ckpt, map_location='cpu')\n",
    "    state_dict = ckpt.get('state_dict', ckpt)\n",
    "    \n",
    "    # Remove 'model.' prefix if present (from Lightning wrapper)\n",
    "    clean_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith('model.'):\n",
    "            clean_state_dict[k[6:]] = v\n",
    "        else:\n",
    "            clean_state_dict[k] = v\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(clean_state_dict, strict=False)\n",
    "        print(\"Model weights loaded successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning loading weights: {e}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Load test data\n",
    "    data_path = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        # Use validation set if available, otherwise train\n",
    "        if 'val' in f:\n",
    "            test_data = np.array(f['val']['data'][:n_samples])\n",
    "            test_labels = np.array(f['val']['labels'][:n_samples])\n",
    "        else:\n",
    "            test_data = np.array(f['train']['data'][:n_samples])\n",
    "            test_labels = np.array(f['train']['labels'][:n_samples])\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(test_data, dtype=torch.float32)\n",
    "        output, all_states = model(x)\n",
    "        \n",
    "        # Apply max pooling over time (like training)\n",
    "        if output.dim() == 3:\n",
    "            pooled = output.max(dim=1)[0]  # [batch, num_classes]\n",
    "        else:\n",
    "            pooled = output\n",
    "        \n",
    "        # Get predictions\n",
    "        probs = torch.softmax(pooled, dim=1)\n",
    "        predictions = torch.argmax(probs, dim=1).numpy()\n",
    "        confidence = probs.max(dim=1)[0].numpy()\n",
    "    \n",
    "    # Visualize\n",
    "    n_cols = min(4, n_samples)\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5*n_cols, 3*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle(\"Model Predictions (ORIGINAL - Expected ~50% Accuracy)\", fontsize=14, fontweight='bold')\n",
    "    \n",
    "    class_names = [\"One Pulse\", \"Two Pulses\"]\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        row, col = i // n_cols, i % n_cols\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Plot input signal\n",
    "        ax.plot(test_data[i, :, 0], 'b-', linewidth=1.5, alpha=0.8)\n",
    "        ax.set_ylim(-0.1, 1.1)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Determine if prediction is correct\n",
    "        is_correct = predictions[i] == test_labels[i]\n",
    "        color = 'green' if is_correct else 'red'\n",
    "        symbol = '✓' if is_correct else '✗'\n",
    "        \n",
    "        # Title with prediction info\n",
    "        true_label = class_names[test_labels[i]]\n",
    "        pred_label = class_names[predictions[i]]\n",
    "        \n",
    "        ax.set_title(\n",
    "            f\"{symbol} Pred: {pred_label} ({confidence[i]:.1%})\\nTrue: {true_label}\",\n",
    "            fontsize=9,\n",
    "            color=color,\n",
    "            fontweight='bold' if not is_correct else 'normal'\n",
    "        )\n",
    "        \n",
    "        if col == 0:\n",
    "            ax.set_ylabel(\"Signal\")\n",
    "        if row == n_rows - 1:\n",
    "            ax.set_xlabel(\"Time step\")\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_samples, n_rows * n_cols):\n",
    "        row, col = i // n_cols, i % n_cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    accuracy = (predictions == test_labels).mean()\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"PREDICTION SUMMARY (ORIGINAL CONFIG)\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy on {n_samples} samples: {accuracy:.1%}\")\n",
    "    print(f\"Correct: {(predictions == test_labels).sum()}/{n_samples}\")\n",
    "    \n",
    "    if accuracy < 0.6:\n",
    "        print(f\"\\n⚠️  As expected, accuracy is near random (~50%)\")\n",
    "        print(f\"   This confirms gradient flow is blocked.\")\n",
    "        print(f\"   Run 02_train_a_model.ipynb for the fixed version.\")\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_predictions(n_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "**Gradient Flow Diagnostic**\n",
    "\n",
    "Run this to confirm gradients are blocked at `J_1_to_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DIAGNOSTIC: Check gradient flow through the ORIGINAL model\n",
    "# ============================================================================\n",
    "\n",
    "def check_gradient_flow():\n",
    "    \"\"\"Test if gradients flow through the ORIGINAL SOEN model.\"\"\"\n",
    "    \n",
    "    from soen_toolkit.core.model_yaml import build_model_from_yaml\n",
    "    \n",
    "    # Load ORIGINAL model spec\n",
    "    model_path = Path(\"training/test_models/model_specs/1D_5D_2D_PulseNetSpec.yaml\")\n",
    "    model = build_model_from_yaml(model_path)\n",
    "    model.train()\n",
    "    \n",
    "    # Load a small batch of data\n",
    "    data_path = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        x = torch.tensor(f['train']['data'][:8], dtype=torch.float32)\n",
    "        y = torch.tensor(f['train']['labels'][:8], dtype=torch.long)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"GRADIENT FLOW DIAGNOSTIC (ORIGINAL MODEL)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Forward pass\n",
    "    x.requires_grad_(True)\n",
    "    output, all_states = model(x)\n",
    "    \n",
    "    # Apply time pooling\n",
    "    if output.dim() == 3:\n",
    "        pooled = output.max(dim=1)[0]\n",
    "    else:\n",
    "        pooled = output\n",
    "    \n",
    "    # Compute loss and backward\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    loss = loss_fn(pooled, y)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Check gradients\n",
    "    print(f\"\\n{'Parameter':<45} {'Grad Norm':<15} {'Has Grad':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    blocked_params = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = param.grad.norm().item()\n",
    "            has_grad = \"✓\" if grad_norm > 1e-10 else \"✗ (zero)\"\n",
    "            print(f\"{name:<45} {grad_norm:<15.8f} {has_grad}\")\n",
    "        else:\n",
    "            print(f\"{name:<45} {'None':<15} ✗ (no grad)\")\n",
    "            blocked_params.append(name)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"DIAGNOSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if blocked_params:\n",
    "        print(f\"⚠️  {len(blocked_params)} parameters have NO gradients:\")\n",
    "        for name in blocked_params:\n",
    "            print(f\"   - {name}\")\n",
    "        print(f\"\\n→ This confirms gradient flow is BLOCKED!\")\n",
    "        print(f\"→ The model cannot learn because J_1_to_2.learnable = false\")\n",
    "    else:\n",
    "        print(\"✓ All parameters have gradients (unexpected for original model)\")\n",
    "\n",
    "# Run diagnostic\n",
    "check_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This control experiment demonstrates that:\n",
    "\n",
    "1. **More epochs don't help** when gradients are blocked\n",
    "2. **Higher learning rate doesn't help** when there's nothing to learn\n",
    "3. **The issue is architectural** (frozen output connection), not hyperparameters\n",
    "\n",
    "To see proper training, run `02_train_a_model.ipynb` which uses `J_1_to_2.learnable: true`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}