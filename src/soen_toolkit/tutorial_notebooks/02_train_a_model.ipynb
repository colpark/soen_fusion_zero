{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 02 — Train a SOEN Model\n",
    "\n",
    "In this tutorial, we’ll walk through training a pre-built SOEN model using the training configuration file located at:\n",
    "`tutorial_notebooks/training/training_configs/pulse_net.yaml`.\n",
    "\n",
    "We’ll use the `run_from_config` function to launch training. This function makes it easy to set up an experiment — once all training settings are defined in your YAML file, you can start training with a single command.\n",
    "\n",
    "You can run it either in a script or directly from the command line.\n",
    "Python:\n",
    "`run_from_config(str(BASE_CONFIG), script_dir=Path.cwd())`\n",
    "CLI:\n",
    "`python -m soen_toolkit.training --config path/to/training_config.yaml`\n",
    "\n",
    "### ML Task Overview\n",
    "\n",
    "This example tackles a binary classification problem on time-series inputs:\n",
    "- Class 1: Input contains a single pulse.\n",
    "- Class 2: Input contains two distinct pulses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Ensure soen_toolkit is importable\nimport sys\nfrom pathlib import Path\n\n# Add src directory to path if running from notebook location\nnotebook_dir = Path.cwd()\nfor parent in [notebook_dir] + list(notebook_dir.parents):\n    candidate = parent / \"src\"\n    if (candidate / \"soen_toolkit\").exists():\n        sys.path.insert(0, str(candidate))\n        break\n\nfrom soen_toolkit.training.trainers.experiment import run_from_config"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "We’ll use the example model and dataset to launch a local test training run. You can experiment by modifying the training YAML file as needed. For more detailed configurations, see: `src/soen_toolkit/training/examples/training_configs`.\n",
    "\n",
    "Additional information about the training process can be found in: `src/soen_toolkit/training/README.md`.\n",
    "\n",
    "If you wish to construct your own datasets, please use hdf5 file format. All instructions can be found at: `docs/DATASETS.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch training via Python API\n",
    "run_from_config(\"training/training_configs/pulse_net.yaml\", script_dir=Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# VISUALIZATION: Plot training results using matplotlib (no tensorboard needed)\n# ============================================================================\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport glob\n\ndef find_latest_log_dir(base_path=\"training_logs\"):\n    \"\"\"Find the most recent training log directory.\"\"\"\n    # Look for tensorboard event files\n    pattern = f\"{base_path}/**/events.out.tfevents*\"\n    event_files = glob.glob(pattern, recursive=True)\n    \n    if not event_files:\n        # Try lightning_logs\n        pattern = \"lightning_logs/**/events.out.tfevents*\"\n        event_files = glob.glob(pattern, recursive=True)\n    \n    if event_files:\n        # Get the most recent one\n        latest = max(event_files, key=lambda x: Path(x).stat().st_mtime)\n        return Path(latest).parent\n    return None\n\ndef parse_tensorboard_logs(log_dir):\n    \"\"\"Parse tensorboard logs using tbparse.\"\"\"\n    try:\n        from tbparse import SummaryReader\n        reader = SummaryReader(str(log_dir))\n        df = reader.scalars\n        return df\n    except ImportError:\n        print(\"tbparse not available, trying manual parsing...\")\n        return None\n    except Exception as e:\n        print(f\"Error parsing logs: {e}\")\n        return None\n\n# Find and parse logs\nlog_dir = find_latest_log_dir()\nif log_dir:\n    print(f\"Found logs at: {log_dir}\")\n    df = parse_tensorboard_logs(log_dir)\n    \n    if df is not None and len(df) > 0:\n        # Get unique tags (metrics)\n        tags = df['tag'].unique()\n        print(f\"Available metrics: {list(tags)}\")\n        \n        # Create subplots\n        n_metrics = min(len(tags), 6)  # Show up to 6 metrics\n        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n        axes = axes.flatten()\n        \n        for i, tag in enumerate(tags[:6]):\n            ax = axes[i]\n            metric_data = df[df['tag'] == tag].sort_values('step')\n            ax.plot(metric_data['step'], metric_data['value'], 'b-', linewidth=1.5)\n            ax.set_xlabel('Step')\n            ax.set_ylabel(tag.split('/')[-1])\n            ax.set_title(tag)\n            ax.grid(True, alpha=0.3)\n        \n        # Hide unused subplots\n        for i in range(len(tags), 6):\n            axes[i].set_visible(False)\n        \n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"No scalar data found in logs.\")\nelse:\n    print(\"No training logs found. Run the training cell first.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Quick Notes on Datasets\n",
    "\n",
    "soen_toolkit.training models expect datasets in **HDF5 format** with the following structure:\n",
    "\n",
    "- **Inputs** (`data`): `[N, T, D]`  \n",
    "  - `N`: number of samples  \n",
    "  - `T`: sequence length  \n",
    "  - `D`: feature dimension (should be equal to the number of units in the input layer - ID=0)\n",
    "\n",
    "- **Labels** (`labels`): shape depends on the task  \n",
    "  - Classification (seq2static): `[N]` (int64 class indices)  \n",
    "  - Classification (seq2seq): `[N, T]` (int64 per-timestep classes)  \n",
    "  - Regression (seq2static): `[N, K]` (float32)  \n",
    "  - Regression (seq2seq): `[N, T, K]` (float32)  \n",
    "  - Unsupervised (seq2seq): labels optional; inputs are used as targets  \n",
    "\n",
    "**Recommended layout:**\n",
    "\n",
    "root/\n",
    "train/{data, labels}\n",
    "val/{data, labels}\n",
    "test/{data, labels}\n",
    "\n",
    "**Key config notes:**\n",
    "- Set `training.paradigm` and `training.mapping` in your YAML (e.g., `supervised` + `seq2static`).  \n",
    "- Use `data.target_seq_len` to align input/output sequence lengths.  \n",
    "- Pooling for seq2static tasks is controlled via `model.time_pooling`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Manual Evaluation and Visualization\n\nIf the above log parsing doesn't work, you can manually evaluate the trained model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# MANUAL EVALUATION: Load trained model and evaluate on test data\n# ============================================================================\n\nimport torch\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\n\n# Find the latest trained model\ndef find_latest_model(base_path=\"training_logs\"):\n    \"\"\"Find the most recent trained model checkpoint.\"\"\"\n    patterns = [\n        f\"{base_path}/**/*.soen\",\n        f\"{base_path}/**/*.ckpt\", \n        \"lightning_logs/**/*.ckpt\",\n    ]\n    \n    all_models = []\n    for pattern in patterns:\n        all_models.extend(glob.glob(pattern, recursive=True))\n    \n    if all_models:\n        return max(all_models, key=lambda x: Path(x).stat().st_mtime)\n    return None\n\n# Load model\nmodel_path = find_latest_model()\nif model_path:\n    print(f\"Found trained model: {model_path}\")\n    \n    # Load based on extension\n    if model_path.endswith('.soen'):\n        from soen_toolkit.core import SOENModelCore\n        model = SOENModelCore.load(model_path)\n    else:\n        # Load from checkpoint\n        from soen_toolkit.training.models import SOENLightningModule\n        model = SOENLightningModule.load_from_checkpoint(model_path)\n        model = model.model  # Get the underlying SOEN model\n    \n    model.eval()\n    print(\"Model loaded successfully!\")\n    \n    # Load test data\n    data_path = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n    if data_path.exists():\n        with h5py.File(data_path, 'r') as f:\n            # Try test split, fall back to val\n            split = 'test' if 'test' in f else 'val'\n            test_data = torch.tensor(f[split]['data'][:], dtype=torch.float32)\n            test_labels = torch.tensor(f[split]['labels'][:], dtype=torch.long)\n        \n        print(f\"Loaded {split} data: {test_data.shape}\")\n        \n        # Run inference\n        with torch.no_grad():\n            outputs, _ = model(test_data[:100])  # First 100 samples\n            \n            # Get predictions (assuming last timestep, argmax for classification)\n            if outputs.dim() == 3:\n                outputs = outputs[:, -1, :]  # Take last timestep\n            predictions = outputs.argmax(dim=-1)\n        \n        # Calculate accuracy\n        correct = (predictions == test_labels[:100]).sum().item()\n        accuracy = correct / len(predictions) * 100\n        print(f\"\\nTest Accuracy: {accuracy:.1f}% ({correct}/{len(predictions)})\")\n        \n        # Visualize some predictions\n        fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n        \n        for i, ax in enumerate(axes.flatten()):\n            if i >= len(test_data):\n                break\n            \n            # Plot input signal\n            signal = test_data[i, :, 0].numpy()\n            ax.plot(signal, 'b-', linewidth=1.5)\n            \n            true_label = test_labels[i].item()\n            pred_label = predictions[i].item()\n            \n            color = 'green' if true_label == pred_label else 'red'\n            ax.set_title(f\"True: {true_label}, Pred: {pred_label}\", color=color)\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Input\")\n            ax.grid(True, alpha=0.3)\n        \n        plt.suptitle(f\"Sample Predictions (Accuracy: {accuracy:.1f}%)\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n        # Confusion matrix\n        from sklearn.metrics import confusion_matrix\n        cm = confusion_matrix(test_labels[:100].numpy(), predictions.numpy())\n        \n        fig, ax = plt.subplots(figsize=(6, 5))\n        im = ax.imshow(cm, cmap='Blues')\n        ax.set_xlabel('Predicted')\n        ax.set_ylabel('True')\n        ax.set_title('Confusion Matrix')\n        ax.set_xticks([0, 1])\n        ax.set_yticks([0, 1])\n        \n        # Add text annotations\n        for i in range(2):\n            for j in range(2):\n                ax.text(j, i, str(cm[i, j]), ha='center', va='center', \n                       color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=14)\n        \n        plt.colorbar(im)\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(f\"Dataset not found at {data_path}\")\nelse:\n    print(\"No trained model found. Run training first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (soen-toolkit)",
   "language": "python",
   "name": "soen_toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}