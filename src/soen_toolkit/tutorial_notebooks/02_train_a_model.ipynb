{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Tutorial 02 — Train a SOEN Model\n\nIn this tutorial, we'll walk through training a pre-built SOEN model using the training configuration file located at:\n`tutorial_notebooks/training/training_configs/pulse_net_trainable.yaml`.\n\nWe'll use the `run_from_config` function to launch training. This function makes it easy to set up an experiment — once all training settings are defined in your YAML file, you can start training with a single command.\n\nYou can run it either in a script or directly from the command line.\nPython:\n`run_from_config(str(BASE_CONFIG), script_dir=Path.cwd())`\nCLI:\n`python -m soen_toolkit.training --config path/to/training_config.yaml`\n\n### ML Task Overview\n\nThis example tackles a binary classification problem on time-series inputs:\n- **Class 0**: Input contains a single pulse.\n- **Class 1**: Input contains two distinct pulses.\n\n### Key Fix Applied\nThe original `pulse_net.yaml` had `J_1_to_2.learnable: false` which blocked gradient flow.\nThe new `pulse_net_trainable.yaml` fixes this with `learnable: true`."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Ensure soen_toolkit is importable\nimport sys\nfrom pathlib import Path\n\n# Add src directory to path if running from notebook location\nnotebook_dir = Path.cwd()\nfor parent in [notebook_dir] + list(notebook_dir.parents):\n    candidate = parent / \"src\"\n    if (candidate / \"soen_toolkit\").exists():\n        sys.path.insert(0, str(candidate))\n        break\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport torch\nimport glob\n\nfrom soen_toolkit.training.trainers.experiment import run_from_config"
  },
  {
   "cell_type": "markdown",
   "source": "**Visualize the Dataset**\n\nLet's look at examples from each class to understand the task.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# VISUALIZE DATASET: One-pulse vs Two-pulse classification\n# ============================================================================\n\ndef visualize_dataset(data_path=\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\", n_examples=4):\n    \"\"\"Visualize examples from each class in the dataset.\"\"\"\n    \n    with h5py.File(data_path, 'r') as f:\n        data = np.array(f['train']['data'])\n        labels = np.array(f['train']['labels'])\n    \n    print(f\"Dataset shape: {data.shape} (N samples, T timesteps, D features)\")\n    print(f\"Labels shape: {labels.shape}\")\n    print(f\"Class distribution: {np.bincount(labels)}\")\n    \n    # Find examples of each class\n    class_0_idx = np.where(labels == 0)[0][:n_examples]\n    class_1_idx = np.where(labels == 1)[0][:n_examples]\n    \n    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 5))\n    fig.suptitle(\"Input Signals: One-Pulse (Class 0) vs Two-Pulse (Class 1)\", fontsize=12, fontweight='bold')\n    \n    # Plot Class 0 (single pulse)\n    for i, idx in enumerate(class_0_idx):\n        axes[0, i].plot(data[idx, :, 0], 'b-', linewidth=1.5)\n        axes[0, i].set_title(f\"Sample {idx}\", fontsize=10)\n        axes[0, i].set_ylim(-0.1, 1.1)\n        axes[0, i].grid(True, alpha=0.3)\n        if i == 0:\n            axes[0, i].set_ylabel(\"Class 0\\n(One Pulse)\", fontsize=10)\n    \n    # Plot Class 1 (two pulses)\n    for i, idx in enumerate(class_1_idx):\n        axes[1, i].plot(data[idx, :, 0], 'r-', linewidth=1.5)\n        axes[1, i].set_title(f\"Sample {idx}\", fontsize=10)\n        axes[1, i].set_ylim(-0.1, 1.1)\n        axes[1, i].grid(True, alpha=0.3)\n        if i == 0:\n            axes[1, i].set_ylabel(\"Class 1\\n(Two Pulses)\", fontsize=10)\n        axes[1, i].set_xlabel(\"Time step\")\n    \n    plt.tight_layout()\n    plt.show()\n    \n    return data, labels\n\n# Visualize the dataset\ntrain_data, train_labels = visualize_dataset()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training**\n",
    "\n",
    "We’ll use the example model and dataset to launch a local test training run. You can experiment by modifying the training YAML file as needed. For more detailed configurations, see: `src/soen_toolkit/training/examples/training_configs`.\n",
    "\n",
    "Additional information about the training process can be found in: `src/soen_toolkit/training/README.md`.\n",
    "\n",
    "If you wish to construct your own datasets, please use hdf5 file format. All instructions can be found at: `docs/DATASETS.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Launch training via Python API using the FIXED trainable config\n# Key change: J_1_to_2.learnable = true (was false in original)\nrun_from_config(\"training/training_configs/pulse_net_trainable.yaml\", script_dir=Path.cwd())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ============================================================================\n# VISUALIZATION: Plot training results using matplotlib (no tensorboard needed)\n# ============================================================================\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\nimport glob\n\ndef find_latest_log_dir():\n    \"\"\"Find the most recent training log directory.\"\"\"\n    # Search multiple possible locations\n    search_paths = [\n        \"training/temp/**/events.out.tfevents*\",\n        \"training_logs/**/events.out.tfevents*\",\n        \"lightning_logs/**/events.out.tfevents*\",\n        \"**/logs/**/events.out.tfevents*\",\n    ]\n    \n    all_event_files = []\n    for pattern in search_paths:\n        all_event_files.extend(glob.glob(pattern, recursive=True))\n    \n    if all_event_files:\n        # Get the most recent one\n        latest = max(all_event_files, key=lambda x: Path(x).stat().st_mtime)\n        return Path(latest).parent\n    return None\n\ndef parse_tensorboard_logs(log_dir):\n    \"\"\"Parse tensorboard logs using tbparse.\"\"\"\n    try:\n        from tbparse import SummaryReader\n        reader = SummaryReader(str(log_dir))\n        df = reader.scalars\n        return df\n    except ImportError:\n        print(\"tbparse not available. Install with: pip install tbparse\")\n        return None\n    except Exception as e:\n        print(f\"Error parsing logs: {e}\")\n        return None\n\n# Find and parse logs\nlog_dir = find_latest_log_dir()\nif log_dir:\n    print(f\"Found logs at: {log_dir}\")\n    df = parse_tensorboard_logs(log_dir)\n    \n    if df is not None and len(df) > 0:\n        # Get unique tags (metrics)\n        tags = df['tag'].unique()\n        print(f\"Available metrics: {list(tags)}\")\n        \n        # Filter for important metrics\n        important_tags = [t for t in tags if any(k in t.lower() for k in ['loss', 'accuracy', 'lr'])]\n        if not important_tags:\n            important_tags = list(tags)[:6]\n        \n        # Create subplots\n        n_plots = min(len(important_tags), 6)\n        fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n        axes = axes.flatten()\n        \n        for i, tag in enumerate(important_tags[:6]):\n            ax = axes[i]\n            metric_data = df[df['tag'] == tag].sort_values('step')\n            ax.plot(metric_data['step'], metric_data['value'], 'b-', linewidth=1.5)\n            ax.set_xlabel('Step')\n            ax.set_ylabel(tag.split('/')[-1])\n            ax.set_title(tag, fontsize=10)\n            ax.grid(True, alpha=0.3)\n        \n        # Hide unused subplots\n        for i in range(len(important_tags), 6):\n            axes[i].set_visible(False)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Print final metrics\n        print(\"\\n\" + \"=\"*60)\n        print(\"FINAL METRICS\")\n        print(\"=\"*60)\n        for tag in important_tags:\n            metric_data = df[df['tag'] == tag].sort_values('step')\n            if len(metric_data) > 0:\n                print(f\"{tag}: {metric_data['value'].iloc[-1]:.4f}\")\n    else:\n        print(\"No scalar data found in logs.\")\nelse:\n    print(\"No training logs found. Run the training cell first.\")\n    print(\"Searched in: training/temp/, training_logs/, lightning_logs/\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "**Visualize Predictions**\n\nAfter training, let's see how the model performs on test samples with input visualization.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# VISUALIZE PREDICTIONS: Show model predictions on test samples\n# ============================================================================\n\ndef visualize_predictions(n_samples=8):\n    \"\"\"Load trained model and visualize predictions on test data.\"\"\"\n    \n    from soen_toolkit.core.model_yaml import build_model_from_yaml\n    \n    # Find the latest checkpoint\n    ckpt_patterns = [\n        \"training/temp/**/checkpoints/**/*.ckpt\",\n        \"training/temp/**/*.ckpt\",\n    ]\n    \n    all_ckpts = []\n    for pattern in ckpt_patterns:\n        all_ckpts.extend(glob.glob(pattern, recursive=True))\n    \n    if not all_ckpts:\n        print(\"No checkpoint found. Run training first.\")\n        return\n    \n    latest_ckpt = max(all_ckpts, key=lambda x: Path(x).stat().st_mtime)\n    print(f\"Loading checkpoint: {latest_ckpt}\")\n    \n    # Load model architecture and weights\n    model_path = Path(\"training/test_models/model_specs/1D_5D_2D_PulseNetSpec_trainable.yaml\")\n    model = build_model_from_yaml(model_path)\n    \n    # Load trained weights\n    ckpt = torch.load(latest_ckpt, map_location='cpu')\n    state_dict = ckpt.get('state_dict', ckpt)\n    \n    # Remove 'model.' prefix if present (from Lightning wrapper)\n    clean_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith('model.'):\n            clean_state_dict[k[6:]] = v\n        else:\n            clean_state_dict[k] = v\n    \n    try:\n        model.load_state_dict(clean_state_dict, strict=False)\n        print(\"Model weights loaded successfully\")\n    except Exception as e:\n        print(f\"Warning loading weights: {e}\")\n    \n    model.eval()\n    \n    # Load test data\n    data_path = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n    with h5py.File(data_path, 'r') as f:\n        # Use validation set if available, otherwise train\n        if 'val' in f:\n            test_data = np.array(f['val']['data'][:n_samples])\n            test_labels = np.array(f['val']['labels'][:n_samples])\n        else:\n            test_data = np.array(f['train']['data'][:n_samples])\n            test_labels = np.array(f['train']['labels'][:n_samples])\n    \n    # Run inference\n    with torch.no_grad():\n        x = torch.tensor(test_data, dtype=torch.float32)\n        output, all_states = model(x)\n        \n        # Apply max pooling over time (like training)\n        if output.dim() == 3:\n            pooled = output.max(dim=1)[0]  # [batch, num_classes]\n        else:\n            pooled = output\n        \n        # Get predictions\n        probs = torch.softmax(pooled, dim=1)\n        predictions = torch.argmax(probs, dim=1).numpy()\n        confidence = probs.max(dim=1)[0].numpy()\n    \n    # Visualize\n    n_cols = min(4, n_samples)\n    n_rows = (n_samples + n_cols - 1) // n_cols\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5*n_cols, 3*n_rows))\n    if n_rows == 1:\n        axes = axes.reshape(1, -1)\n    \n    fig.suptitle(\"Model Predictions on Test Samples\", fontsize=14, fontweight='bold')\n    \n    class_names = [\"One Pulse\", \"Two Pulses\"]\n    \n    for i in range(n_samples):\n        row, col = i // n_cols, i % n_cols\n        ax = axes[row, col]\n        \n        # Plot input signal\n        ax.plot(test_data[i, :, 0], 'b-', linewidth=1.5, alpha=0.8)\n        ax.set_ylim(-0.1, 1.1)\n        ax.grid(True, alpha=0.3)\n        \n        # Determine if prediction is correct\n        is_correct = predictions[i] == test_labels[i]\n        color = 'green' if is_correct else 'red'\n        symbol = '✓' if is_correct else '✗'\n        \n        # Title with prediction info\n        true_label = class_names[test_labels[i]]\n        pred_label = class_names[predictions[i]]\n        \n        ax.set_title(\n            f\"{symbol} Pred: {pred_label} ({confidence[i]:.1%})\\nTrue: {true_label}\",\n            fontsize=9,\n            color=color,\n            fontweight='bold' if not is_correct else 'normal'\n        )\n        \n        if col == 0:\n            ax.set_ylabel(\"Signal\")\n        if row == n_rows - 1:\n            ax.set_xlabel(\"Time step\")\n    \n    # Hide empty subplots\n    for i in range(n_samples, n_rows * n_cols):\n        row, col = i // n_cols, i % n_cols\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print summary\n    accuracy = (predictions == test_labels).mean()\n    print(f\"\\n{'='*50}\")\n    print(f\"PREDICTION SUMMARY\")\n    print(f\"{'='*50}\")\n    print(f\"Accuracy on {n_samples} samples: {accuracy:.1%}\")\n    print(f\"Correct: {(predictions == test_labels).sum()}/{n_samples}\")\n    \n    # Show confusion matrix style breakdown\n    for true_class in [0, 1]:\n        for pred_class in [0, 1]:\n            count = ((test_labels == true_class) & (predictions == pred_class)).sum()\n            if count > 0:\n                print(f\"  True {class_names[true_class]} -> Pred {class_names[pred_class]}: {count}\")\n\n# Visualize predictions\nvisualize_predictions(n_samples=8)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Quick Notes on Datasets\n",
    "\n",
    "soen_toolkit.training models expect datasets in **HDF5 format** with the following structure:\n",
    "\n",
    "- **Inputs** (`data`): `[N, T, D]`  \n",
    "  - `N`: number of samples  \n",
    "  - `T`: sequence length  \n",
    "  - `D`: feature dimension (should be equal to the number of units in the input layer - ID=0)\n",
    "\n",
    "- **Labels** (`labels`): shape depends on the task  \n",
    "  - Classification (seq2static): `[N]` (int64 class indices)  \n",
    "  - Classification (seq2seq): `[N, T]` (int64 per-timestep classes)  \n",
    "  - Regression (seq2static): `[N, K]` (float32)  \n",
    "  - Regression (seq2seq): `[N, T, K]` (float32)  \n",
    "  - Unsupervised (seq2seq): labels optional; inputs are used as targets  \n",
    "\n",
    "**Recommended layout:**\n",
    "\n",
    "root/\n",
    "train/{data, labels}\n",
    "val/{data, labels}\n",
    "test/{data, labels}\n",
    "\n",
    "**Key config notes:**\n",
    "- Set `training.paradigm` and `training.mapping` in your YAML (e.g., `supervised` + `seq2static`).  \n",
    "- Use `data.target_seq_len` to align input/output sequence lengths.  \n",
    "- Pooling for seq2static tasks is controlled via `model.time_pooling`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Manual Evaluation and Visualization\n\nIf the above log parsing doesn't work, you can manually evaluate the trained model:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ============================================================================\n# MANUAL EVALUATION: Load trained model and evaluate on test data\n# ============================================================================\n\nimport torch\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom pathlib import Path\n\n# Find the latest trained model\ndef find_latest_model(base_path=\"training_logs\"):\n    \"\"\"Find the most recent trained model checkpoint.\"\"\"\n    patterns = [\n        f\"{base_path}/**/*.soen\",\n        f\"{base_path}/**/*.ckpt\", \n        \"lightning_logs/**/*.ckpt\",\n    ]\n    \n    all_models = []\n    for pattern in patterns:\n        all_models.extend(glob.glob(pattern, recursive=True))\n    \n    if all_models:\n        return max(all_models, key=lambda x: Path(x).stat().st_mtime)\n    return None\n\n# Load model\nmodel_path = find_latest_model()\nif model_path:\n    print(f\"Found trained model: {model_path}\")\n    \n    # Load based on extension\n    if model_path.endswith('.soen'):\n        from soen_toolkit.core import SOENModelCore\n        model = SOENModelCore.load(model_path)\n    else:\n        # Load from checkpoint\n        from soen_toolkit.training.models import SOENLightningModule\n        model = SOENLightningModule.load_from_checkpoint(model_path)\n        model = model.model  # Get the underlying SOEN model\n    \n    model.eval()\n    print(\"Model loaded successfully!\")\n    \n    # Load test data\n    data_path = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n    if data_path.exists():\n        with h5py.File(data_path, 'r') as f:\n            # Try test split, fall back to val\n            split = 'test' if 'test' in f else 'val'\n            test_data = torch.tensor(f[split]['data'][:], dtype=torch.float32)\n            test_labels = torch.tensor(f[split]['labels'][:], dtype=torch.long)\n        \n        print(f\"Loaded {split} data: {test_data.shape}\")\n        \n        # Run inference\n        with torch.no_grad():\n            outputs, _ = model(test_data[:100])  # First 100 samples\n            \n            # Get predictions (assuming last timestep, argmax for classification)\n            if outputs.dim() == 3:\n                outputs = outputs[:, -1, :]  # Take last timestep\n            predictions = outputs.argmax(dim=-1)\n        \n        # Calculate accuracy\n        correct = (predictions == test_labels[:100]).sum().item()\n        accuracy = correct / len(predictions) * 100\n        print(f\"\\nTest Accuracy: {accuracy:.1f}% ({correct}/{len(predictions)})\")\n        \n        # Visualize some predictions\n        fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n        \n        for i, ax in enumerate(axes.flatten()):\n            if i >= len(test_data):\n                break\n            \n            # Plot input signal\n            signal = test_data[i, :, 0].numpy()\n            ax.plot(signal, 'b-', linewidth=1.5)\n            \n            true_label = test_labels[i].item()\n            pred_label = predictions[i].item()\n            \n            color = 'green' if true_label == pred_label else 'red'\n            ax.set_title(f\"True: {true_label}, Pred: {pred_label}\", color=color)\n            ax.set_xlabel(\"Time\")\n            ax.set_ylabel(\"Input\")\n            ax.grid(True, alpha=0.3)\n        \n        plt.suptitle(f\"Sample Predictions (Accuracy: {accuracy:.1f}%)\", fontsize=14)\n        plt.tight_layout()\n        plt.show()\n        \n        # Confusion matrix\n        from sklearn.metrics import confusion_matrix\n        cm = confusion_matrix(test_labels[:100].numpy(), predictions.numpy())\n        \n        fig, ax = plt.subplots(figsize=(6, 5))\n        im = ax.imshow(cm, cmap='Blues')\n        ax.set_xlabel('Predicted')\n        ax.set_ylabel('True')\n        ax.set_title('Confusion Matrix')\n        ax.set_xticks([0, 1])\n        ax.set_yticks([0, 1])\n        \n        # Add text annotations\n        for i in range(2):\n            for j in range(2):\n                ax.text(j, i, str(cm[i, j]), ha='center', va='center', \n                       color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=14)\n        \n        plt.colorbar(im)\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(f\"Dataset not found at {data_path}\")\nelse:\n    print(\"No trained model found. Run training first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (soen-toolkit)",
   "language": "python",
   "name": "soen_toolkit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}