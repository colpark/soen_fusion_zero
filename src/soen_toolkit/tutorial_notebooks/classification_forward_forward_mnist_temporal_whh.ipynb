{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward-Forward MNIST Classification with Cross-Neuron Recurrence (W_hh)\n",
    "\n",
    "**UPGRADE**: Adds hidden-to-hidden connections (W_hh) to enable cross-neuron temporal communication.\n",
    "\n",
    "## Key Difference from Original Temporal Version\n",
    "\n",
    "| Version | Dynamics | Description |\n",
    "|---------|----------|-------------|\n",
    "| Original | `s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t])` | Self-recurrence only (decay) |\n",
    "| **This** | `s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t] + W_hh×s[t-1])` | Cross-neuron recurrence |\n",
    "\n",
    "## Why W_hh Matters for Sequence Classification\n",
    "\n",
    "**Without W_hh (original)**:\n",
    "- Each neuron only remembers its own past (leaky integration)\n",
    "- Neurons cannot share information through time\n",
    "- Pattern at t=0 cannot influence neuron j at t=27 unless j saw the pattern\n",
    "\n",
    "**With W_hh (this version)**:\n",
    "- Neurons can communicate temporal patterns to each other\n",
    "- Early patterns can be \"passed\" to specialized neurons for later processing\n",
    "- Enables true sequence memory, not just signal averaging\n",
    "\n",
    "## Hardware Compatibility (SOEN)\n",
    "\n",
    "From Shainline's 2021 paper \"Optoelectronic Intelligence\":\n",
    "- Cross-neuron connections ARE possible via optical waveguide routing\n",
    "- Photons from neuron i can be routed to synaptic input of neuron j\n",
    "- Constraint: Topology must be fixed at fabrication time\n",
    "- This notebook assumes W_hh topology is pre-determined (all-to-all or structured)\n",
    "\n",
    "## Expected Improvement\n",
    "\n",
    "Previous best (without W_hh): ~30.5% test accuracy  \n",
    "Target with W_hh: Significant improvement through temporal pattern sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FORWARD-FORWARD WITH CROSS-NEURON RECURRENCE (W_hh)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct MNIST download without torchvision\n",
    "def download_mnist(data_dir='./data/mnist'):\n",
    "    \"\"\"Download MNIST dataset without torchvision.\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    base_url = 'https://ossci-datasets.s3.amazonaws.com/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz',\n",
    "    }\n",
    "    \n",
    "    paths = {}\n",
    "    for key, filename in files.items():\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        paths[key] = filepath\n",
    "    \n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_mnist_images(filepath):\n",
    "    \"\"\"Load MNIST images from gzipped IDX file - keep as 28x28.\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        n_images = int.from_bytes(f.read(4), 'big')\n",
    "        n_rows = int.from_bytes(f.read(4), 'big')\n",
    "        n_cols = int.from_bytes(f.read(4), 'big')\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return data.reshape(n_images, n_rows, n_cols).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def load_mnist_labels(filepath):\n",
    "    \"\"\"Load MNIST labels from gzipped IDX file.\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        n_labels = int.from_bytes(f.read(4), 'big')\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "\n",
    "# Download and load MNIST\n",
    "paths = download_mnist()\n",
    "X_train_full = torch.from_numpy(load_mnist_images(paths['train_images']))\n",
    "y_train_full = torch.from_numpy(load_mnist_labels(paths['train_labels'])).long()\n",
    "X_test_full = torch.from_numpy(load_mnist_images(paths['test_images']))\n",
    "y_test_full = torch.from_numpy(load_mnist_labels(paths['test_labels'])).long()\n",
    "\n",
    "print(f\"Full dataset: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n",
    "\n",
    "# Use training data subset\n",
    "N_TRAIN = 20000\n",
    "N_TEST = 2000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_idx = torch.randperm(len(X_train_full))[:N_TRAIN]\n",
    "test_idx = torch.randperm(len(X_test_full))[:N_TEST]\n",
    "\n",
    "X_train = X_train_full[train_idx]\n",
    "y_train = y_train_full[train_idx]\n",
    "X_test = X_test_full[test_idx]\n",
    "y_test = y_test_full[test_idx]\n",
    "\n",
    "# Scale to SOEN operating range [0.025, 0.275]\n",
    "X_train = X_train * 0.25 + 0.025\n",
    "X_test = X_test * 0.25 + 0.025\n",
    "\n",
    "print(f\"\\nUsing subset:\")\n",
    "print(f\"  Training set: {X_train.shape} (N × rows × cols)\")\n",
    "print(f\"  Test set: {X_test.shape}\")\n",
    "print(f\"  X range: [{X_train.min():.3f}, {X_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants and Label Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "N_ROWS = 28      # Number of timesteps (rows in image)\n",
    "N_COLS = 28      # Pixels per row\n",
    "LABEL_SCALE = 0.25\n",
    "\n",
    "# Input dimension: 28 pixels + 10 label = 38 per timestep\n",
    "INPUT_DIM_PER_ROW = N_COLS + N_CLASSES\n",
    "\n",
    "\n",
    "def embed_label_temporal(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Embed one-hot label into each row of the temporal sequence.\n",
    "    \n",
    "    Args:\n",
    "        X: [N, 28, 28] images (N samples, 28 rows, 28 cols)\n",
    "        y: [N] class labels (0-9)\n",
    "    \n",
    "    Returns:\n",
    "        X_embedded: [N, 28, 38] - each row has 28 pixels + 10 label dims\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    device = X.device\n",
    "    \n",
    "    # Create one-hot labels [N, 10]\n",
    "    one_hot = torch.zeros(N, n_classes, device=device)\n",
    "    one_hot.scatter_(1, y.unsqueeze(1), label_scale)\n",
    "    \n",
    "    # Expand to [N, 28, 10] - same label at each timestep\n",
    "    one_hot_expanded = one_hot.unsqueeze(1).expand(-1, N_ROWS, -1)\n",
    "    \n",
    "    # Concatenate: [N, 28, 28] + [N, 28, 10] = [N, 28, 38]\n",
    "    return torch.cat([X, one_hot_expanded], dim=2)\n",
    "\n",
    "\n",
    "def create_positive_negative_pairs_temporal(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Create positive and negative temporal sequences for Forward-Forward.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    device = X.device\n",
    "    \n",
    "    # Positive: correct labels\n",
    "    X_pos = embed_label_temporal(X, y, n_classes, label_scale)\n",
    "    \n",
    "    # Negative: random wrong labels\n",
    "    y_wrong = (y + torch.randint(1, n_classes, (N,), device=device)) % n_classes\n",
    "    X_neg = embed_label_temporal(X, y_wrong, n_classes, label_scale)\n",
    "    \n",
    "    return X_pos, X_neg\n",
    "\n",
    "\n",
    "print(f\"Input dimension per timestep: {INPUT_DIM_PER_ROW} ({N_COLS} pixels + {N_CLASSES} label)\")\n",
    "print(f\"Number of timesteps: {N_ROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. SOEN Layer with Cross-Neuron Recurrence (W_hh) - CORRECTED DYNAMICS\n\n### Previous (WRONG) Implementation:\n```python\npre = W_ih @ x[t] + W_hh @ h[t-1]\nactivated = tanh(pre)                    # Wrong: not state-dependent\nh[t] = alpha * h[t-1] + (1-alpha) * activated  # Wrong: beta should be 1.0, not 0.05!\n```\n\n### Correct SOEN Dynamics:\n```python\nphi = W_ih @ x[t] + W_hh @ s[t-1]       # Flux from synapses (W_hh adds recurrence)\nsquid_current = bias - s[t-1]            # STATE-DEPENDENT!\ng_val = soen_g(phi, squid_current)       # Nonlinearity depends on BOTH phi AND state\ns[t] = alpha * s[t-1] + beta * g_val     # beta = dt * gamma_plus = 1.0 (not 0.05!)\n```\n\n### Key Corrections:\n1. **beta = 1.0**: Input has FULL weight, not 5%\n2. **State-dependent nonlinearity**: `squid_current = bias - state` creates implicit feedback\n3. **Quantized flux response**: `g()` includes `cos(π*phi)` term from flux quantization"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class SOENSourceFunction:\n    \"\"\"\n    SOEN source function g(phi, squid_current) based on actual hardware dynamics.\n    \n    From the SOEN toolkit (Heaviside_fit_state_dep):\n        bias_diff = clamp(squid_current - C, min=ε)\n        cos_term = |cos(π * phi)| + ε      # Quantized flux response!\n        disc = A * bias_diff^K - B * cos_term^M\n        activation = sigmoid(100 * disc)    # Very steep (Heaviside-like)\n        g = activation * clamp(disc)^(1/N)\n    \n    This creates a state-dependent, flux-quantized nonlinearity.\n    \"\"\"\n    def __init__(self, A=0.37, B=0.32, C=1.06, K=1.92, M=2.50, N=2.63, epsilon=1e-6):\n        self.A = A\n        self.B = B\n        self.C = C\n        self.K = K\n        self.M = M\n        self.N = N\n        self.epsilon = epsilon\n    \n    def __call__(self, phi, squid_current):\n        \"\"\"\n        Compute source function value.\n        \n        Args:\n            phi: Flux from synaptic input [batch, hidden_dim]\n            squid_current: bias_current - state [batch, hidden_dim]\n        \n        Returns:\n            g_val: Source function output [batch, hidden_dim]\n        \"\"\"\n        bias_diff = torch.clamp(squid_current - self.C, min=self.epsilon)\n        cos_term = torch.abs(torch.cos(torch.pi * phi)) + self.epsilon\n        disc = self.A * (bias_diff ** self.K) - self.B * (cos_term ** self.M)\n        activation = torch.sigmoid(100.0 * disc)  # Very steep threshold\n        return activation * (torch.clamp(disc, min=self.epsilon) ** (1.0 / self.N))\n\n\nclass SOENRecurrentLayerCorrected(nn.Module):\n    \"\"\"\n    SOEN recurrent layer with CORRECTED dynamics and W_hh.\n    \n    Correct Dynamics (from SOEN toolkit):\n        phi[t] = W_ih @ x[t] + W_hh @ s[t-1] + phi_offset   # Flux includes W_hh!\n        squid_current = bias_current - s[t-1]               # State-dependent!\n        g_val = soen_g(phi[t], squid_current)               # Nonlinear source function\n        s[t] = alpha * s[t-1] + beta * g_val                # beta = 1.0, NOT 0.05!\n    \n    Key differences from previous implementation:\n    1. beta = dt * gamma_plus = 1.0 (input has FULL weight)\n    2. squid_current = bias - state (state-dependent nonlinearity)\n    3. g() is the actual SOEN source function with cos(π*phi) term\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dim, \n                 gamma_plus=1.0, gamma_minus=0.05, dt=1.0,\n                 bias_current=1.98, phi_offset=0.02,\n                 w_hh_scale=0.1, sparse_hh=False, sparsity=0.5):\n        \"\"\"\n        Args:\n            input_dim: Input features per timestep\n            hidden_dim: Number of hidden neurons\n            gamma_plus: Source function gain (default 1.0)\n            gamma_minus: Decay rate (default 0.05)\n            dt: Time step (default 1.0)\n            bias_current: SOEN bias current (default 1.98)\n            phi_offset: Flux offset (default 0.02)\n            w_hh_scale: W_hh initialization scale\n            sparse_hh: Use sparse W_hh\n            sparsity: Fraction of W_hh connections\n        \"\"\"\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.dt = dt\n        self.gamma_plus = gamma_plus\n        self.gamma_minus = gamma_minus\n        \n        # Derived coefficients\n        self.alpha = 1.0 - dt * gamma_minus  # e.g., 0.95\n        self.beta = dt * gamma_plus          # e.g., 1.0 (NOT 0.05!)\n        \n        # SOEN parameters\n        self.bias_current = bias_current\n        self.phi_offset = phi_offset\n        \n        # Source function\n        self.source_func = SOENSourceFunction()\n        \n        # Input-to-hidden weights (produces phi/flux)\n        self.W_ih = nn.Linear(input_dim, hidden_dim, bias=True)\n        nn.init.xavier_uniform_(self.W_ih.weight)\n        nn.init.constant_(self.W_ih.bias, phi_offset)\n        \n        # Hidden-to-hidden weights (W_hh - THE KEY UPGRADE!)\n        self.W_hh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n        nn.init.xavier_uniform_(self.W_hh.weight)\n        self.W_hh.weight.data *= w_hh_scale\n        \n        # Optional sparse mask\n        if sparse_hh:\n            mask = (torch.rand(hidden_dim, hidden_dim) < sparsity).float()\n            for i in range(hidden_dim):\n                if mask[i].sum() == 0:\n                    mask[i, torch.randint(0, hidden_dim, (1,))] = 1.0\n            self.register_buffer('hh_mask', mask)\n        else:\n            self.register_buffer('hh_mask', torch.ones(hidden_dim, hidden_dim))\n    \n    def forward(self, x):\n        \"\"\"\n        Forward pass with CORRECT SOEN dynamics.\n        \n        Args:\n            x: [batch, seq_len, input_dim]\n        \n        Returns:\n            states: [batch, seq_len, hidden_dim]\n        \"\"\"\n        batch_size, seq_len, _ = x.shape\n        device = x.device\n        \n        # Initialize state\n        s = torch.zeros(batch_size, self.hidden_dim, device=device)\n        \n        states = []\n        W_hh_effective = self.W_hh.weight * self.hh_mask\n        \n        for t in range(seq_len):\n            # Compute phi (flux) from input AND recurrent connections\n            phi_input = self.W_ih(x[:, t, :])           # From input\n            phi_recurrent = F.linear(s, W_hh_effective)  # From W_hh (THE UPGRADE!)\n            phi = phi_input + phi_recurrent\n            \n            # State-dependent squid current (THIS WAS MISSING!)\n            squid_current = self.bias_current - s\n            \n            # SOEN source function (state-dependent nonlinearity)\n            g_val = self.source_func(phi, squid_current)\n            \n            # CORRECT discretization: beta = 1.0, NOT (1-alpha)!\n            s = self.alpha * s + self.beta * g_val\n            \n            states.append(s)\n        \n        return torch.stack(states, dim=1)\n\n\nclass TemporalFFNetworkCorrected(nn.Module):\n    \"\"\"\n    Forward-Forward network with CORRECTED SOEN dynamics.\n    \"\"\"\n    \n    def __init__(self, input_dim, hidden_dims, \n                 gamma_plus=1.0, gamma_minus=0.05, dt=1.0,\n                 bias_current=1.98, phi_offset=0.02,\n                 w_hh_scale=0.1, sparse_hh=False, sparsity=0.5):\n        super().__init__()\n        \n        self.layers = nn.ModuleList()\n        \n        prev_dim = input_dim\n        for hidden_dim in hidden_dims:\n            layer = SOENRecurrentLayerCorrected(\n                input_dim=prev_dim,\n                hidden_dim=hidden_dim,\n                gamma_plus=gamma_plus,\n                gamma_minus=gamma_minus,\n                dt=dt,\n                bias_current=bias_current,\n                phi_offset=phi_offset,\n                w_hh_scale=w_hh_scale,\n                sparse_hh=sparse_hh,\n                sparsity=sparsity,\n            )\n            self.layers.append(layer)\n            prev_dim = hidden_dim\n    \n    def forward(self, x):\n        all_states = []\n        h = x\n        for layer in self.layers:\n            h = layer(h)\n            all_states.append(h)\n        return all_states\n\n\n# Test the CORRECTED network\nprint(\"=\"*70)\nprint(\"TESTING CORRECTED SOEN DYNAMICS\")\nprint(\"=\"*70)\n\ntest_net = TemporalFFNetworkCorrected(\n    input_dim=INPUT_DIM_PER_ROW,\n    hidden_dims=[24],\n    gamma_plus=1.0,\n    gamma_minus=0.05,\n    w_hh_scale=0.1\n)\n\nn_params = sum(p.numel() for p in test_net.parameters() if p.requires_grad)\nlayer = test_net.layers[0]\n\nprint(f\"\\nArchitecture: {INPUT_DIM_PER_ROW} → [24] → goodness\")\nprint(f\"Parameters: {n_params}\")\nprint(f\"\\nSOEN Dynamics:\")\nprint(f\"  alpha = 1 - dt*gamma_minus = {layer.alpha:.2f}\")\nprint(f\"  beta = dt*gamma_plus = {layer.beta:.2f} ← FULL input weight!\")\nprint(f\"  bias_current = {layer.bias_current}\")\nprint(f\"  phi_offset = {layer.phi_offset}\")\nprint(f\"\\nKey corrections:\")\nprint(f\"  ✓ beta = 1.0 (was incorrectly 0.05)\")\nprint(f\"  ✓ squid_current = bias - state (state-dependent)\")\nprint(f\"  ✓ g() uses cos(π*phi) for flux quantization\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward-Forward Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_goodness(activations):\n",
    "    \"\"\"\n",
    "    Compute goodness as mean of squared activations.\n",
    "    Hardware-compatible: measures mean power in the layer.\n",
    "    \n",
    "    Args:\n",
    "        activations: [batch, hidden_dim]\n",
    "    \n",
    "    Returns:\n",
    "        goodness: [batch] - mean squared activation per sample\n",
    "    \"\"\"\n",
    "    return (activations ** 2).mean(dim=1)\n",
    "\n",
    "\n",
    "def forward_forward_loss(goodness_pos, goodness_neg, margin=0.01):\n",
    "    \"\"\"\n",
    "    Contrastive Forward-Forward loss.\n",
    "    \n",
    "    Push G_pos to be greater than G_neg by at least margin.\n",
    "    \"\"\"\n",
    "    return F.softplus(margin - (goodness_pos - goodness_neg)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with W_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ff_temporal(model, X, y, batch_size=100, goodness_mode='all'):\n",
    "    \"\"\"\n",
    "    Evaluate temporal Forward-Forward model.\n",
    "    \n",
    "    For each sample, test all 10 label hypotheses and pick highest goodness.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X.shape[0]\n",
    "    all_predictions = []\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(start + batch_size, N)\n",
    "            X_batch = X[start:end].to(device)  # [B, 28, 28]\n",
    "            B = X_batch.shape[0]\n",
    "            \n",
    "            # Repeat each sample N_CLASSES times\n",
    "            X_repeated = X_batch.unsqueeze(1).expand(-1, N_CLASSES, -1, -1)\n",
    "            X_repeated = X_repeated.reshape(B * N_CLASSES, N_ROWS, N_COLS)\n",
    "            \n",
    "            y_hypotheses = torch.arange(N_CLASSES, device=device)\n",
    "            y_hypotheses = y_hypotheses.unsqueeze(0).expand(B, -1).reshape(B * N_CLASSES)\n",
    "            \n",
    "            X_embedded = embed_label_temporal(X_repeated, y_hypotheses)\n",
    "            \n",
    "            # Forward pass\n",
    "            layer_states = model(X_embedded)\n",
    "            \n",
    "            # Compute total goodness\n",
    "            total_goodness = torch.zeros(B * N_CLASSES, device=device)\n",
    "            for states in layer_states:\n",
    "                if goodness_mode == 'final':\n",
    "                    act = states[:, -1, :]  # Final timestep only\n",
    "                    total_goodness += compute_goodness(act)\n",
    "                else:  # 'all'\n",
    "                    for t in range(states.shape[1]):\n",
    "                        act = states[:, t, :]\n",
    "                        total_goodness += compute_goodness(act)\n",
    "            \n",
    "            # Reshape and get predictions\n",
    "            goodness_matrix = total_goodness.reshape(B, N_CLASSES)\n",
    "            predictions = goodness_matrix.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu())\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    accuracy = (all_predictions == y).float().mean().item()\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_forward_forward_whh(model, X_train, y_train, X_test, y_test,\n",
    "                               n_epochs=100, lr=0.01, margin=0.01,\n",
    "                               batch_size=64, eval_subset=1000, verbose=True,\n",
    "                               weight_decay=1e-4, lr_decay=0.98,\n",
    "                               local_in_time=True, goodness_mode='all',\n",
    "                               gradient_compensation=True):\n",
    "    \"\"\"\n",
    "    Train temporal Forward-Forward with W_hh.\n",
    "    \n",
    "    Args:\n",
    "        model: TemporalFFNetwork with W_hh\n",
    "        local_in_time: Compute loss at each timestep (hardware-compatible)\n",
    "        goodness_mode: 'final' or 'all' timesteps\n",
    "        gradient_compensation: Weight early timesteps more heavily\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'goodness_pos': [],\n",
    "        'goodness_neg': [],\n",
    "        'lr': [],\n",
    "    }\n",
    "    \n",
    "    N = X_train.shape[0]\n",
    "    n_batches = (N + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Evaluation subset\n",
    "    eval_idx = torch.randperm(N)[:min(eval_subset, N)]\n",
    "    X_train_eval = X_train[eval_idx]\n",
    "    y_train_eval = y_train[eval_idx]\n",
    "    \n",
    "    best_test_acc = 0\n",
    "    \n",
    "    # Gradient compensation weights (for vanishing gradient mitigation)\n",
    "    alpha = model.layers[0].alpha\n",
    "    n_timesteps = N_ROWS\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_g_pos = []\n",
    "        epoch_g_neg = []\n",
    "        \n",
    "        perm = torch.randperm(N)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "            \n",
    "            X_batch = X_shuffled[start:end].to(device)\n",
    "            y_batch = y_shuffled[start:end].to(device)\n",
    "            \n",
    "            X_pos, X_neg = create_positive_negative_pairs_temporal(X_batch, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            states_pos = model(X_pos)  # List of [batch, seq_len, hidden_dim]\n",
    "            states_neg = model(X_neg)\n",
    "            \n",
    "            total_loss = 0\n",
    "            batch_g_pos_list = []\n",
    "            batch_g_neg_list = []\n",
    "            \n",
    "            for layer_states_pos, layer_states_neg in zip(states_pos, states_neg):\n",
    "                seq_len = layer_states_pos.shape[1]\n",
    "                \n",
    "                if local_in_time:\n",
    "                    # Loss at each timestep\n",
    "                    for t in range(seq_len):\n",
    "                        act_pos = layer_states_pos[:, t, :]\n",
    "                        act_neg = layer_states_neg[:, t, :]\n",
    "                        \n",
    "                        g_pos = compute_goodness(act_pos)\n",
    "                        g_neg = compute_goodness(act_neg)\n",
    "                        \n",
    "                        batch_g_pos_list.append(g_pos.mean().item())\n",
    "                        batch_g_neg_list.append(g_neg.mean().item())\n",
    "                        \n",
    "                        timestep_loss = forward_forward_loss(g_pos, g_neg, margin)\n",
    "                        \n",
    "                        # Gradient compensation for vanishing gradients\n",
    "                        if gradient_compensation:\n",
    "                            weight = (1.0 / alpha) ** (seq_len - 1 - t)\n",
    "                            # Normalize\n",
    "                            normalizer = sum((1.0/alpha)**(seq_len-1-i) for i in range(seq_len))\n",
    "                            weight = weight / normalizer * seq_len\n",
    "                        else:\n",
    "                            weight = 1.0\n",
    "                        \n",
    "                        total_loss = total_loss + weight * timestep_loss\n",
    "                \n",
    "                elif goodness_mode == 'final':\n",
    "                    # Final timestep only\n",
    "                    act_pos = layer_states_pos[:, -1, :]\n",
    "                    act_neg = layer_states_neg[:, -1, :]\n",
    "                    \n",
    "                    g_pos = compute_goodness(act_pos)\n",
    "                    g_neg = compute_goodness(act_neg)\n",
    "                    \n",
    "                    batch_g_pos_list.append(g_pos.mean().item())\n",
    "                    batch_g_neg_list.append(g_neg.mean().item())\n",
    "                    \n",
    "                    total_loss = total_loss + forward_forward_loss(g_pos, g_neg, margin)\n",
    "                \n",
    "                else:  # 'all' without local_in_time\n",
    "                    for t in range(seq_len):\n",
    "                        act_pos = layer_states_pos[:, t, :]\n",
    "                        act_neg = layer_states_neg[:, t, :]\n",
    "                        \n",
    "                        g_pos = compute_goodness(act_pos)\n",
    "                        g_neg = compute_goodness(act_neg)\n",
    "                        \n",
    "                        batch_g_pos_list.append(g_pos.mean().item())\n",
    "                        batch_g_neg_list.append(g_neg.mean().item())\n",
    "                        \n",
    "                        total_loss = total_loss + forward_forward_loss(g_pos, g_neg, margin)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_g_pos.append(np.mean(batch_g_pos_list) if batch_g_pos_list else 0)\n",
    "            epoch_g_neg.append(np.mean(batch_g_neg_list) if batch_g_neg_list else 0)\n",
    "            \n",
    "            if verbose and batch_idx % 50 == 0:\n",
    "                print(f\"\\rEpoch {epoch+1}/{n_epochs} | Batch {batch_idx+1}/{n_batches} | \"\n",
    "                      f\"Loss: {total_loss.item():.4f}\", end=\"\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_mode = 'all' if local_in_time else goodness_mode\n",
    "        train_acc = evaluate_ff_temporal(model, X_train_eval, y_train_eval, goodness_mode=eval_mode)\n",
    "        test_acc = evaluate_ff_temporal(model, X_test, y_test, goodness_mode=eval_mode)\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['goodness_pos'].append(np.mean(epoch_g_pos))\n",
    "        history['goodness_neg'].append(np.mean(epoch_g_neg))\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if verbose:\n",
    "            sep = np.mean(epoch_g_pos) - np.mean(epoch_g_neg)\n",
    "            print(f\"\\rEpoch {epoch+1}/{n_epochs} | Loss: {epoch_loss/n_batches:.4f} | \"\n",
    "                  f\"Train: {train_acc:.4f} | Test: {test_acc:.4f} | \"\n",
    "                  f\"Best: {best_test_acc:.4f} | Sep: {sep:.4f}    \")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model with W_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hyperparameters - CORRECTED SOEN DYNAMICS\nHIDDEN_DIMS = [24]\nGAMMA_PLUS = 1.0     # Source function gain\nGAMMA_MINUS = 0.05   # Decay rate\nDT = 1.0             # Time step\nBIAS_CURRENT = 1.98  # SOEN bias current\nPHI_OFFSET = 0.02    # Flux offset\nW_HH_SCALE = 0.1     # W_hh initialization\nMARGIN = 0.01\nN_EPOCHS = 100\nLR = 0.01\nBATCH_SIZE = 64\nWEIGHT_DECAY = 1e-4\nLR_DECAY = 0.98\nLOCAL_IN_TIME = True\nGRADIENT_COMPENSATION = True\n\nprint(\"=\"*80)\nprint(\"TRAINING WITH CORRECTED SOEN DYNAMICS + W_hh\")\nprint(\"=\"*80)\nprint(f\"\\nArchitecture: {INPUT_DIM_PER_ROW} → {HIDDEN_DIMS} → goodness\")\nprint(f\"\\nCORRECTED Dynamics:\")\nprint(f\"  phi[t] = W_ih @ x[t] + W_hh @ s[t-1]\")\nprint(f\"  squid_current = bias - s[t-1]  ← STATE-DEPENDENT!\")\nprint(f\"  g_val = soen_g(phi, squid_current)  ← cos(π*phi) nonlinearity\")\nprint(f\"  s[t] = {1-DT*GAMMA_MINUS:.2f} * s[t-1] + {DT*GAMMA_PLUS:.1f} * g_val  ← beta=1.0!\")\nprint(f\"\\nTraining: {N_TRAIN} samples, Testing: {N_TEST} samples\")\nprint(\"=\"*80)\n\n# Build CORRECTED model\ntorch.manual_seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"\\nUsing device: {device}\")\n\nmodel = TemporalFFNetworkCorrected(\n    input_dim=INPUT_DIM_PER_ROW,\n    hidden_dims=HIDDEN_DIMS,\n    gamma_plus=GAMMA_PLUS,\n    gamma_minus=GAMMA_MINUS,\n    dt=DT,\n    bias_current=BIAS_CURRENT,\n    phi_offset=PHI_OFFSET,\n    w_hh_scale=W_HH_SCALE,\n    sparse_hh=False,\n).to(device)\n\nn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Parameters: {n_params}\")\n\n# Train\nhistory = train_forward_forward_whh(\n    model, X_train, y_train, X_test, y_test,\n    n_epochs=N_EPOCHS, lr=LR, margin=MARGIN,\n    batch_size=BATCH_SIZE, verbose=True,\n    weight_decay=WEIGHT_DECAY, lr_decay=LR_DECAY,\n    local_in_time=LOCAL_IN_TIME,\n    gradient_compensation=GRADIENT_COMPENSATION,\n)\n\nprint(\"=\"*80)\nprint(f\"Final train accuracy: {history['train_acc'][-1]:.4f}\")\nprint(f\"Final test accuracy: {history['test_acc'][-1]:.4f}\")\nprint(f\"Best test accuracy: {max(history['test_acc']):.4f}\")\nprint(f\"\\nComparison:\")\nprint(f\"  Previous (wrong beta=0.05): ~30%\")\nprint(f\"  Corrected (beta=1.0): {max(history['test_acc'])*100:.1f}%\")\nprint(f\"  Random baseline: 10%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['loss'], color='steelblue', lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Contrastive Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Goodness\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['goodness_pos'], label='Positive (G+)', color='green', lw=2)\n",
    "ax2.plot(history['goodness_neg'], label='Negative (G-)', color='red', lw=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Goodness')\n",
    "ax2.set_title('Goodness Values')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax3 = axes[0, 2]\n",
    "ax3.plot(history['lr'], color='orange', lw=2)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Learning Rate')\n",
    "ax3.set_title('Learning Rate Decay')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax4 = axes[1, 0]\n",
    "ax4.plot(history['train_acc'], label='Train', color='coral', lw=2)\n",
    "ax4.plot(history['test_acc'], label='Test', color='steelblue', lw=2)\n",
    "ax4.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random (10%)')\n",
    "ax4.axhline(y=0.305, color='purple', linestyle=':', alpha=0.7, label='Previous best (30.5%)')\n",
    "best_epoch = np.argmax(history['test_acc'])\n",
    "ax4.axvline(x=best_epoch, color='green', linestyle=':', alpha=0.7)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title(f'Classification Accuracy (Best: {max(history[\"test_acc\"]):.2%})')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim(0, 1.0)\n",
    "\n",
    "# Separation\n",
    "ax5 = axes[1, 1]\n",
    "separation = [p - n for p, n in zip(history['goodness_pos'], history['goodness_neg'])]\n",
    "ax5.plot(separation, color='purple', lw=2)\n",
    "ax5.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('G+ - G-')\n",
    "ax5.set_title('Goodness Separation')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Train vs Test gap\n",
    "ax6 = axes[1, 2]\n",
    "gap = [t - v for t, v in zip(history['train_acc'], history['test_acc'])]\n",
    "ax6.plot(gap, color='brown', lw=2)\n",
    "ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Train - Test')\n",
    "ax6.set_title('Generalization Gap')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Forward-Forward with W_hh ({sum(HIDDEN_DIMS)} neurons)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize W_hh Learned Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned W_hh matrix\n",
    "W_hh = model.layers[0].W_hh.weight.data.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# W_hh heatmap\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(W_hh, cmap='RdBu_r', aspect='auto', vmin=-np.abs(W_hh).max(), vmax=np.abs(W_hh).max())\n",
    "ax1.set_xlabel('From neuron')\n",
    "ax1.set_ylabel('To neuron')\n",
    "ax1.set_title('W_hh (Hidden-to-Hidden Weights)')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# W_hh distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(W_hh.flatten(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--')\n",
    "ax2.set_xlabel('Weight value')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title(f'W_hh Distribution (mean={W_hh.mean():.4f}, std={W_hh.std():.4f})')\n",
    "\n",
    "# Top connections\n",
    "ax3 = axes[2]\n",
    "# Find strongest connections\n",
    "flat_idx = np.argsort(np.abs(W_hh).flatten())[-20:][::-1]\n",
    "top_connections = [(i // W_hh.shape[1], i % W_hh.shape[1], W_hh.flatten()[i]) for i in flat_idx]\n",
    "labels = [f'{i}→{j}' for i, j, _ in top_connections]\n",
    "values = [v for _, _, v in top_connections]\n",
    "colors = ['green' if v > 0 else 'red' for v in values]\n",
    "ax3.barh(range(len(values)), values, color=colors, edgecolor='black')\n",
    "ax3.set_yticks(range(len(labels)))\n",
    "ax3.set_yticklabels(labels)\n",
    "ax3.axvline(x=0, color='black', linestyle='-')\n",
    "ax3.set_xlabel('Weight')\n",
    "ax3.set_title('Strongest W_hh Connections')\n",
    "\n",
    "plt.suptitle('Cross-Neuron Recurrence Analysis', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nW_hh Statistics:\")\n",
    "print(f\"  Shape: {W_hh.shape}\")\n",
    "print(f\"  Mean: {W_hh.mean():.6f}\")\n",
    "print(f\"  Std: {W_hh.std():.6f}\")\n",
    "print(f\"  Max: {W_hh.max():.6f}\")\n",
    "print(f\"  Min: {W_hh.min():.6f}\")\n",
    "print(f\"  Sparsity (|w|<0.01): {(np.abs(W_hh) < 0.01).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare W_hh Scales and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different W_hh configurations\n",
    "configs = [\n",
    "    {'name': 'No W_hh (baseline)', 'w_hh_scale': 0.0},\n",
    "    {'name': 'W_hh scale=0.05', 'w_hh_scale': 0.05},\n",
    "    {'name': 'W_hh scale=0.1', 'w_hh_scale': 0.1},\n",
    "    {'name': 'W_hh scale=0.2', 'w_hh_scale': 0.2},\n",
    "    {'name': 'W_hh scale=0.3', 'w_hh_scale': 0.3},\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "print(\"Comparing W_hh configurations...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in configs:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = TemporalFFNetwork(\n",
    "        input_dim=INPUT_DIM_PER_ROW,\n",
    "        hidden_dims=[24],\n",
    "        alpha=0.95,\n",
    "        activation='tanh',\n",
    "        w_hh_scale=config['w_hh_scale'],\n",
    "    ).to(device)\n",
    "    \n",
    "    # Zero out W_hh for baseline\n",
    "    if config['w_hh_scale'] == 0.0:\n",
    "        with torch.no_grad():\n",
    "            model.layers[0].W_hh.weight.zero_()\n",
    "            model.layers[0].W_hh.weight.requires_grad = False\n",
    "    \n",
    "    history = train_forward_forward_whh(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, margin=0.01,\n",
    "        batch_size=64, verbose=False,\n",
    "        weight_decay=1e-4, lr_decay=0.98,\n",
    "        local_in_time=True,\n",
    "        gradient_compensation=True,\n",
    "    )\n",
    "    \n",
    "    best_test = max(history['test_acc'])\n",
    "    comparison_results.append({\n",
    "        'config': config['name'],\n",
    "        'w_hh_scale': config['w_hh_scale'],\n",
    "        'train_acc': history['train_acc'][-1],\n",
    "        'test_acc': history['test_acc'][-1],\n",
    "        'best_test': best_test,\n",
    "    })\n",
    "    \n",
    "    print(f\"{config['name']:25s} | Final: {history['test_acc'][-1]:.4f} | Best: {best_test:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configuration\n",
    "best_config = max(comparison_results, key=lambda x: x['best_test'])\n",
    "baseline = [r for r in comparison_results if r['w_hh_scale'] == 0.0][0]\n",
    "\n",
    "print(f\"\\nBest configuration: {best_config['config']} with {best_config['best_test']:.2%}\")\n",
    "print(f\"Improvement over baseline: {(best_config['best_test'] - baseline['best_test'])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Try Different Alpha Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different alpha (retention) values\n",
    "alpha_configs = [\n",
    "    {'alpha': 0.90, 'name': 'α=0.90 (fast decay)'},\n",
    "    {'alpha': 0.95, 'name': 'α=0.95 (moderate)'},\n",
    "    {'alpha': 0.97, 'name': 'α=0.97 (slow decay)'},\n",
    "    {'alpha': 0.99, 'name': 'α=0.99 (very slow)'},\n",
    "]\n",
    "\n",
    "alpha_results = []\n",
    "\n",
    "print(\"Comparing alpha (retention factor) values with W_hh...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in alpha_configs:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = TemporalFFNetwork(\n",
    "        input_dim=INPUT_DIM_PER_ROW,\n",
    "        hidden_dims=[24],\n",
    "        alpha=config['alpha'],\n",
    "        activation='tanh',\n",
    "        w_hh_scale=0.1,\n",
    "    ).to(device)\n",
    "    \n",
    "    history = train_forward_forward_whh(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, margin=0.01,\n",
    "        batch_size=64, verbose=False,\n",
    "        weight_decay=1e-4, lr_decay=0.98,\n",
    "        local_in_time=True,\n",
    "        gradient_compensation=True,\n",
    "    )\n",
    "    \n",
    "    best_test = max(history['test_acc'])\n",
    "    alpha_results.append({\n",
    "        'config': config['name'],\n",
    "        'alpha': config['alpha'],\n",
    "        'best_test': best_test,\n",
    "        'history': history,\n",
    "    })\n",
    "    \n",
    "    print(f\"{config['name']:25s} | Best: {best_test:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for result in alpha_results:\n",
    "    ax.plot(result['history']['test_acc'], label=result['config'], lw=2)\n",
    "ax.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Effect of Alpha (Retention Factor) with W_hh')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 11. DEEP ARCHITECTURES (Multiple Layers with W_hh)\n\nA single layer may lack hierarchical feature extraction capacity. With multiple layers:\n- **Layer 1**: Extract low-level temporal patterns from input\n- **Layer 2**: Combine patterns into higher-level features  \n- **Layer 3+**: Further abstraction\n\nEach layer has its own W_hh for within-layer temporal communication.\n\n**Configurations to try** (all ≤24 neurons total):\n- `[24]` - Single wide layer (current)\n- `[12, 12]` - Two layers\n- `[8, 8, 8]` - Three layers\n- `[6, 6, 6, 6]` - Four layers\n- `[16, 8]` - Funnel (wide→narrow)\n- `[8, 16]` - Inverted funnel (narrow→wide)"
  },
  {
   "cell_type": "code",
   "source": "# Extended training for best deep architecture\nprint(\"=\"*80)\nprint(\"EXTENDED TRAINING FOR BEST DEEP ARCHITECTURE\")\nprint(\"=\"*80)\n\n# Get best architecture from the comparison\nbest_dims = best_depth['hidden_dims']\nprint(f\"Training {best_dims} for 100 epochs...\")\n\ntorch.manual_seed(42)\nmodel_deep = TemporalFFNetwork(\n    input_dim=INPUT_DIM_PER_ROW,\n    hidden_dims=best_dims,\n    alpha=0.95,\n    activation='tanh',\n    w_hh_scale=0.1,\n).to(device)\n\nn_params_deep = sum(p.numel() for p in model_deep.parameters() if p.requires_grad)\nprint(f\"Architecture: {best_dims}\")\nprint(f\"Total neurons: {sum(best_dims)}, Parameters: {n_params_deep}\")\nprint()\n\nhistory_deep = train_forward_forward_whh(\n    model_deep, X_train, y_train, X_test, y_test,\n    n_epochs=100, lr=0.01, margin=0.01,\n    batch_size=64, verbose=True,\n    weight_decay=1e-4, lr_decay=0.98,\n    local_in_time=True,\n    gradient_compensation=True,\n)\n\nprint(\"=\"*80)\nprint(f\"Architecture: {best_dims}\")\nprint(f\"Best test accuracy: {max(history_deep['test_acc']):.4f}\")\nprint(f\"Final test accuracy: {history_deep['test_acc'][-1]:.4f}\")\nprint(f\"Improvement over single [24] layer: check depth_results for comparison\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try sparse W_hh (more hardware-realistic)\n",
    "sparsity_configs = [\n",
    "    {'sparse_hh': False, 'sparsity': 1.0, 'name': 'Dense W_hh (100%)'},\n",
    "    {'sparse_hh': True, 'sparsity': 0.5, 'name': 'Sparse W_hh (50%)'},\n",
    "    {'sparse_hh': True, 'sparsity': 0.3, 'name': 'Sparse W_hh (30%)'},\n",
    "    {'sparse_hh': True, 'sparsity': 0.1, 'name': 'Sparse W_hh (10%)'},\n",
    "]\n",
    "\n",
    "sparsity_results = []\n",
    "\n",
    "print(\"Comparing sparse vs dense W_hh (hardware constraints)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in sparsity_configs:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = TemporalFFNetwork(\n",
    "        input_dim=INPUT_DIM_PER_ROW,\n",
    "        hidden_dims=[24],\n",
    "        alpha=0.95,\n",
    "        activation='tanh',\n",
    "        w_hh_scale=0.1,\n",
    "        sparse_hh=config['sparse_hh'],\n",
    "        sparsity=config['sparsity'],\n",
    "    ).to(device)\n",
    "    \n",
    "    history = train_forward_forward_whh(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, margin=0.01,\n",
    "        batch_size=64, verbose=False,\n",
    "        weight_decay=1e-4, lr_decay=0.98,\n",
    "        local_in_time=True,\n",
    "        gradient_compensation=True,\n",
    "    )\n",
    "    \n",
    "    best_test = max(history['test_acc'])\n",
    "    sparsity_results.append({\n",
    "        'config': config['name'],\n",
    "        'sparsity': config['sparsity'],\n",
    "        'best_test': best_test,\n",
    "    })\n",
    "    \n",
    "    print(f\"{config['name']:25s} | Best: {best_test:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Sparse W_hh is more hardware-realistic because optical routing\")\n",
    "print(\"      is expensive. 10-30% connectivity may be practical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONCLUSIONS: FORWARD-FORWARD WITH CROSS-NEURON RECURRENCE (W_hh)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. KEY UPGRADE:\")\n",
    "print(f\"   Original: s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t])\")\n",
    "print(f\"   This:     s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t] + W_hh×s[t-1])\")\n",
    "print(f\"   → W_hh allows neurons to share temporal patterns!\")\n",
    "\n",
    "print(f\"\\n2. ARCHITECTURE:\")\n",
    "print(f\"   Input: {INPUT_DIM_PER_ROW} ({N_COLS} pixels + {N_CLASSES} label)\")\n",
    "print(f\"   Timesteps: {N_ROWS}\")\n",
    "print(f\"   Hidden: {sum(HIDDEN_DIMS)} neurons\")\n",
    "print(f\"   W_ih: {INPUT_DIM_PER_ROW} × {HIDDEN_DIMS[0]} = {INPUT_DIM_PER_ROW * HIDDEN_DIMS[0]} weights\")\n",
    "print(f\"   W_hh: {HIDDEN_DIMS[0]} × {HIDDEN_DIMS[0]} = {HIDDEN_DIMS[0]**2} weights (NEW!)\")\n",
    "\n",
    "print(f\"\\n3. PERFORMANCE:\")\n",
    "print(f\"   Previous best (without W_hh): ~30.5%\")\n",
    "print(f\"   This version (with W_hh):    {max(history['test_acc'])*100:.1f}%\")\n",
    "print(f\"   Random baseline: 10%\")\n",
    "\n",
    "print(f\"\\n4. HARDWARE COMPATIBILITY:\")\n",
    "print(f\"   ✓ W_hh implemented via optical waveguide routing\")\n",
    "print(f\"   ✓ Topology fixed at fabrication (Shainline 2021)\")\n",
    "print(f\"   ✓ Sparse W_hh reduces routing complexity\")\n",
    "print(f\"   ✓ Local-in-time learning (no BPTT hardware needed)\")\n",
    "\n",
    "print(f\"\\n5. WHY W_hh HELPS:\")\n",
    "print(f\"   Without W_hh: Each neuron can only remember its own past\")\n",
    "print(f\"   With W_hh:    Neurons can share temporal information\")\n",
    "print(f\"   Example: Neuron 1 detects top rows → tells Neuron 2 for bottom rows\")\n",
    "\n",
    "print(f\"\\n6. LIMITATIONS:\")\n",
    "print(f\"   - Still learning with BPTT (for gradient flow)\")\n",
    "print(f\"   - Hardware would need different learning rule\")\n",
    "print(f\"   - Fixed topology may limit flexibility\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}