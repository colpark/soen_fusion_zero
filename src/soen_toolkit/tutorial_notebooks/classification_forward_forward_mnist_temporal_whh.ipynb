{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward-Forward MNIST Classification with Cross-Neuron Recurrence (W_hh)\n",
    "\n",
    "**UPGRADE**: Adds hidden-to-hidden connections (W_hh) to enable cross-neuron temporal communication.\n",
    "\n",
    "## Key Difference from Original Temporal Version\n",
    "\n",
    "| Version | Dynamics | Description |\n",
    "|---------|----------|-------------|\n",
    "| Original | `s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t])` | Self-recurrence only (decay) |\n",
    "| **This** | `s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t] + W_hh×s[t-1])` | Cross-neuron recurrence |\n",
    "\n",
    "## Why W_hh Matters for Sequence Classification\n",
    "\n",
    "**Without W_hh (original)**:\n",
    "- Each neuron only remembers its own past (leaky integration)\n",
    "- Neurons cannot share information through time\n",
    "- Pattern at t=0 cannot influence neuron j at t=27 unless j saw the pattern\n",
    "\n",
    "**With W_hh (this version)**:\n",
    "- Neurons can communicate temporal patterns to each other\n",
    "- Early patterns can be \"passed\" to specialized neurons for later processing\n",
    "- Enables true sequence memory, not just signal averaging\n",
    "\n",
    "## Hardware Compatibility (SOEN)\n",
    "\n",
    "From Shainline's 2021 paper \"Optoelectronic Intelligence\":\n",
    "- Cross-neuron connections ARE possible via optical waveguide routing\n",
    "- Photons from neuron i can be routed to synaptic input of neuron j\n",
    "- Constraint: Topology must be fixed at fabrication time\n",
    "- This notebook assumes W_hh topology is pre-determined (all-to-all or structured)\n",
    "\n",
    "## Expected Improvement\n",
    "\n",
    "Previous best (without W_hh): ~30.5% test accuracy  \n",
    "Target with W_hh: Significant improvement through temporal pattern sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FORWARD-FORWARD WITH CROSS-NEURON RECURRENCE (W_hh)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct MNIST download without torchvision\n",
    "def download_mnist(data_dir='./data/mnist'):\n",
    "    \"\"\"Download MNIST dataset without torchvision.\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    base_url = 'https://ossci-datasets.s3.amazonaws.com/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz',\n",
    "    }\n",
    "    \n",
    "    paths = {}\n",
    "    for key, filename in files.items():\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        paths[key] = filepath\n",
    "    \n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_mnist_images(filepath):\n",
    "    \"\"\"Load MNIST images from gzipped IDX file - keep as 28x28.\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        n_images = int.from_bytes(f.read(4), 'big')\n",
    "        n_rows = int.from_bytes(f.read(4), 'big')\n",
    "        n_cols = int.from_bytes(f.read(4), 'big')\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return data.reshape(n_images, n_rows, n_cols).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def load_mnist_labels(filepath):\n",
    "    \"\"\"Load MNIST labels from gzipped IDX file.\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        n_labels = int.from_bytes(f.read(4), 'big')\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "\n",
    "# Download and load MNIST\n",
    "paths = download_mnist()\n",
    "X_train_full = torch.from_numpy(load_mnist_images(paths['train_images']))\n",
    "y_train_full = torch.from_numpy(load_mnist_labels(paths['train_labels'])).long()\n",
    "X_test_full = torch.from_numpy(load_mnist_images(paths['test_images']))\n",
    "y_test_full = torch.from_numpy(load_mnist_labels(paths['test_labels'])).long()\n",
    "\n",
    "print(f\"Full dataset: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n",
    "\n",
    "# Use training data subset\n",
    "N_TRAIN = 20000\n",
    "N_TEST = 2000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_idx = torch.randperm(len(X_train_full))[:N_TRAIN]\n",
    "test_idx = torch.randperm(len(X_test_full))[:N_TEST]\n",
    "\n",
    "X_train = X_train_full[train_idx]\n",
    "y_train = y_train_full[train_idx]\n",
    "X_test = X_test_full[test_idx]\n",
    "y_test = y_test_full[test_idx]\n",
    "\n",
    "# Scale to SOEN operating range [0.025, 0.275]\n",
    "X_train = X_train * 0.25 + 0.025\n",
    "X_test = X_test * 0.25 + 0.025\n",
    "\n",
    "print(f\"\\nUsing subset:\")\n",
    "print(f\"  Training set: {X_train.shape} (N × rows × cols)\")\n",
    "print(f\"  Test set: {X_test.shape}\")\n",
    "print(f\"  X range: [{X_train.min():.3f}, {X_train.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Constants and Label Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "N_ROWS = 28      # Number of timesteps (rows in image)\n",
    "N_COLS = 28      # Pixels per row\n",
    "LABEL_SCALE = 0.25\n",
    "\n",
    "# Input dimension: 28 pixels + 10 label = 38 per timestep\n",
    "INPUT_DIM_PER_ROW = N_COLS + N_CLASSES\n",
    "\n",
    "\n",
    "def embed_label_temporal(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Embed one-hot label into each row of the temporal sequence.\n",
    "    \n",
    "    Args:\n",
    "        X: [N, 28, 28] images (N samples, 28 rows, 28 cols)\n",
    "        y: [N] class labels (0-9)\n",
    "    \n",
    "    Returns:\n",
    "        X_embedded: [N, 28, 38] - each row has 28 pixels + 10 label dims\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    device = X.device\n",
    "    \n",
    "    # Create one-hot labels [N, 10]\n",
    "    one_hot = torch.zeros(N, n_classes, device=device)\n",
    "    one_hot.scatter_(1, y.unsqueeze(1), label_scale)\n",
    "    \n",
    "    # Expand to [N, 28, 10] - same label at each timestep\n",
    "    one_hot_expanded = one_hot.unsqueeze(1).expand(-1, N_ROWS, -1)\n",
    "    \n",
    "    # Concatenate: [N, 28, 28] + [N, 28, 10] = [N, 28, 38]\n",
    "    return torch.cat([X, one_hot_expanded], dim=2)\n",
    "\n",
    "\n",
    "def create_positive_negative_pairs_temporal(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Create positive and negative temporal sequences for Forward-Forward.\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    device = X.device\n",
    "    \n",
    "    # Positive: correct labels\n",
    "    X_pos = embed_label_temporal(X, y, n_classes, label_scale)\n",
    "    \n",
    "    # Negative: random wrong labels\n",
    "    y_wrong = (y + torch.randint(1, n_classes, (N,), device=device)) % n_classes\n",
    "    X_neg = embed_label_temporal(X, y_wrong, n_classes, label_scale)\n",
    "    \n",
    "    return X_pos, X_neg\n",
    "\n",
    "\n",
    "print(f\"Input dimension per timestep: {INPUT_DIM_PER_ROW} ({N_COLS} pixels + {N_CLASSES} label)\")\n",
    "print(f\"Number of timesteps: {N_ROWS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SOEN Layer with Cross-Neuron Recurrence (W_hh)\n",
    "\n",
    "This is the key upgrade: adding W_hh connections to enable neurons to influence each other through time.\n",
    "\n",
    "### Dynamics\n",
    "\n",
    "**Original (self-recurrence only)**:\n",
    "```\n",
    "pre_activation[t] = W_ih @ x[t]\n",
    "s[t] = α × s[t-1] + (1-α) × g(pre_activation[t])\n",
    "```\n",
    "\n",
    "**Upgraded (cross-neuron recurrence)**:\n",
    "```\n",
    "pre_activation[t] = W_ih @ x[t] + W_hh @ s[t-1]  # <-- W_hh added!\n",
    "s[t] = α × s[t-1] + (1-α) × g(pre_activation[t])\n",
    "```\n",
    "\n",
    "The W_hh term allows neuron i's current state to depend on neuron j's previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOENRecurrentLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    SOEN-inspired recurrent layer with cross-neuron connections (W_hh).\n",
    "    \n",
    "    Dynamics:\n",
    "        pre[t] = W_ih @ x[t] + W_hh @ s[t-1] + bias\n",
    "        s[t] = alpha * s[t-1] + (1-alpha) * activation(pre[t])\n",
    "    \n",
    "    Where:\n",
    "        - W_ih: input-to-hidden weights [hidden_dim, input_dim]\n",
    "        - W_hh: hidden-to-hidden weights [hidden_dim, hidden_dim] (NEW!)\n",
    "        - alpha: retention factor (from gamma_minus)\n",
    "        - activation: SOEN-like squashing function\n",
    "    \n",
    "    Hardware notes:\n",
    "        - W_hh requires optical routing from neuron outputs back to synapses\n",
    "        - This is achievable in SOEN via waveguide design (Shainline 2021)\n",
    "        - Topology fixed at fabrication; weights are synaptic efficacies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, alpha=0.95, activation='tanh',\n",
    "                 w_hh_scale=0.1, sparse_hh=False, sparsity=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input features per timestep (38 for MNIST row + label)\n",
    "            hidden_dim: Number of hidden neurons (<=24 for constraint)\n",
    "            alpha: Retention factor (1 - dt*gamma_minus), default 0.95\n",
    "            activation: 'tanh' or 'relu' or 'soen' (Heaviside approximation)\n",
    "            w_hh_scale: Initialization scale for W_hh (smaller = more stable)\n",
    "            sparse_hh: If True, use sparse W_hh (more hardware-realistic)\n",
    "            sparsity: Fraction of W_hh connections to keep (if sparse_hh=True)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.alpha = alpha\n",
    "        self.activation_type = activation\n",
    "        self.sparse_hh = sparse_hh\n",
    "        \n",
    "        # Input-to-hidden weights (standard)\n",
    "        self.W_ih = nn.Linear(input_dim, hidden_dim, bias=True)\n",
    "        nn.init.xavier_uniform_(self.W_ih.weight)\n",
    "        nn.init.zeros_(self.W_ih.bias)\n",
    "        \n",
    "        # Hidden-to-hidden weights (THE KEY UPGRADE!)\n",
    "        self.W_hh = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        # Initialize smaller to prevent exploding activations\n",
    "        nn.init.xavier_uniform_(self.W_hh.weight)\n",
    "        self.W_hh.weight.data *= w_hh_scale\n",
    "        \n",
    "        # Optional: sparse mask for W_hh (hardware constraint)\n",
    "        if sparse_hh:\n",
    "            mask = (torch.rand(hidden_dim, hidden_dim) < sparsity).float()\n",
    "            # Ensure some connectivity (at least 1 connection per neuron)\n",
    "            for i in range(hidden_dim):\n",
    "                if mask[i].sum() == 0:\n",
    "                    mask[i, torch.randint(0, hidden_dim, (1,))] = 1.0\n",
    "            self.register_buffer('hh_mask', mask)\n",
    "        else:\n",
    "            self.register_buffer('hh_mask', torch.ones(hidden_dim, hidden_dim))\n",
    "    \n",
    "    def soen_activation(self, x):\n",
    "        \"\"\"SOEN-like activation: approximates Heaviside with smooth transition.\"\"\"\n",
    "        # Soft Heaviside: sigmoid stretched to match SOEN operating range\n",
    "        return torch.sigmoid(10.0 * (x - 0.1))  # Threshold around 0.1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through temporal sequence.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, input_dim] - temporal input sequence\n",
    "        \n",
    "        Returns:\n",
    "            states: [batch, seq_len, hidden_dim] - hidden states at all timesteps\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        device = x.device\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros(batch_size, self.hidden_dim, device=device)\n",
    "        \n",
    "        # Collect states for all timesteps\n",
    "        states = []\n",
    "        \n",
    "        # Apply sparse mask to W_hh\n",
    "        W_hh_effective = self.W_hh.weight * self.hh_mask\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Input contribution\n",
    "            input_term = self.W_ih(x[:, t, :])  # [batch, hidden_dim]\n",
    "            \n",
    "            # Recurrent contribution (THE KEY UPGRADE!)\n",
    "            recurrent_term = F.linear(h, W_hh_effective)  # [batch, hidden_dim]\n",
    "            \n",
    "            # Combined pre-activation\n",
    "            pre_activation = input_term + recurrent_term\n",
    "            \n",
    "            # Apply activation\n",
    "            if self.activation_type == 'tanh':\n",
    "                activated = torch.tanh(pre_activation)\n",
    "            elif self.activation_type == 'relu':\n",
    "                activated = F.relu(pre_activation)\n",
    "            elif self.activation_type == 'soen':\n",
    "                activated = self.soen_activation(pre_activation)\n",
    "            else:\n",
    "                activated = torch.tanh(pre_activation)\n",
    "            \n",
    "            # Leaky integration (SOEN temporal dynamics)\n",
    "            h = self.alpha * h + (1 - self.alpha) * activated\n",
    "            \n",
    "            states.append(h)\n",
    "        \n",
    "        # Stack: [batch, seq_len, hidden_dim]\n",
    "        return torch.stack(states, dim=1)\n",
    "\n",
    "\n",
    "class TemporalFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward-Forward network for temporal sequence classification.\n",
    "    \n",
    "    Uses SOEN recurrent layers with W_hh for cross-neuron communication.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dims, alpha=0.95, activation='tanh',\n",
    "                 w_hh_scale=0.1, sparse_hh=False, sparsity=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: Input features per timestep\n",
    "            hidden_dims: List of hidden dimensions for each layer\n",
    "            alpha: Retention factor for temporal dynamics\n",
    "            activation: Activation function ('tanh', 'relu', 'soen')\n",
    "            w_hh_scale: Scale for W_hh initialization\n",
    "            sparse_hh: Use sparse W_hh connections\n",
    "            sparsity: Fraction of W_hh connections to keep\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layer = SOENRecurrentLayer(\n",
    "                input_dim=prev_dim,\n",
    "                hidden_dim=hidden_dim,\n",
    "                alpha=alpha,\n",
    "                activation=activation,\n",
    "                w_hh_scale=w_hh_scale,\n",
    "                sparse_hh=sparse_hh,\n",
    "                sparsity=sparsity,\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "            prev_dim = hidden_dim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass returning states from all layers.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, seq_len, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "            all_states: List of [batch, seq_len, hidden_dim] for each layer\n",
    "        \"\"\"\n",
    "        all_states = []\n",
    "        h = x\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            h = layer(h)  # [batch, seq_len, hidden_dim]\n",
    "            all_states.append(h)\n",
    "        \n",
    "        return all_states\n",
    "\n",
    "\n",
    "# Test the network\n",
    "print(\"Testing TemporalFFNetwork with W_hh...\")\n",
    "test_net = TemporalFFNetwork(\n",
    "    input_dim=INPUT_DIM_PER_ROW,\n",
    "    hidden_dims=[24],\n",
    "    alpha=0.95,\n",
    "    w_hh_scale=0.1\n",
    ")\n",
    "\n",
    "n_params = sum(p.numel() for p in test_net.parameters() if p.requires_grad)\n",
    "n_params_wih = 38 * 24 + 24  # W_ih + bias\n",
    "n_params_whh = 24 * 24       # W_hh (no bias)\n",
    "\n",
    "print(f\"\\nArchitecture: {INPUT_DIM_PER_ROW} → [24] → goodness\")\n",
    "print(f\"Total parameters: {n_params}\")\n",
    "print(f\"  W_ih parameters: {n_params_wih} ({INPUT_DIM_PER_ROW} × 24 + 24 bias)\")\n",
    "print(f\"  W_hh parameters: {n_params_whh} (24 × 24) ← NEW!\")\n",
    "print(f\"\\nW_hh enables cross-neuron temporal communication!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward-Forward Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_goodness(activations):\n",
    "    \"\"\"\n",
    "    Compute goodness as mean of squared activations.\n",
    "    Hardware-compatible: measures mean power in the layer.\n",
    "    \n",
    "    Args:\n",
    "        activations: [batch, hidden_dim]\n",
    "    \n",
    "    Returns:\n",
    "        goodness: [batch] - mean squared activation per sample\n",
    "    \"\"\"\n",
    "    return (activations ** 2).mean(dim=1)\n",
    "\n",
    "\n",
    "def forward_forward_loss(goodness_pos, goodness_neg, margin=0.01):\n",
    "    \"\"\"\n",
    "    Contrastive Forward-Forward loss.\n",
    "    \n",
    "    Push G_pos to be greater than G_neg by at least margin.\n",
    "    \"\"\"\n",
    "    return F.softplus(margin - (goodness_pos - goodness_neg)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with W_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ff_temporal(model, X, y, batch_size=100, goodness_mode='all'):\n",
    "    \"\"\"\n",
    "    Evaluate temporal Forward-Forward model.\n",
    "    \n",
    "    For each sample, test all 10 label hypotheses and pick highest goodness.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X.shape[0]\n",
    "    all_predictions = []\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(start + batch_size, N)\n",
    "            X_batch = X[start:end].to(device)  # [B, 28, 28]\n",
    "            B = X_batch.shape[0]\n",
    "            \n",
    "            # Repeat each sample N_CLASSES times\n",
    "            X_repeated = X_batch.unsqueeze(1).expand(-1, N_CLASSES, -1, -1)\n",
    "            X_repeated = X_repeated.reshape(B * N_CLASSES, N_ROWS, N_COLS)\n",
    "            \n",
    "            y_hypotheses = torch.arange(N_CLASSES, device=device)\n",
    "            y_hypotheses = y_hypotheses.unsqueeze(0).expand(B, -1).reshape(B * N_CLASSES)\n",
    "            \n",
    "            X_embedded = embed_label_temporal(X_repeated, y_hypotheses)\n",
    "            \n",
    "            # Forward pass\n",
    "            layer_states = model(X_embedded)\n",
    "            \n",
    "            # Compute total goodness\n",
    "            total_goodness = torch.zeros(B * N_CLASSES, device=device)\n",
    "            for states in layer_states:\n",
    "                if goodness_mode == 'final':\n",
    "                    act = states[:, -1, :]  # Final timestep only\n",
    "                    total_goodness += compute_goodness(act)\n",
    "                else:  # 'all'\n",
    "                    for t in range(states.shape[1]):\n",
    "                        act = states[:, t, :]\n",
    "                        total_goodness += compute_goodness(act)\n",
    "            \n",
    "            # Reshape and get predictions\n",
    "            goodness_matrix = total_goodness.reshape(B, N_CLASSES)\n",
    "            predictions = goodness_matrix.argmax(dim=1)\n",
    "            all_predictions.append(predictions.cpu())\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    accuracy = (all_predictions == y).float().mean().item()\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def train_forward_forward_whh(model, X_train, y_train, X_test, y_test,\n",
    "                               n_epochs=100, lr=0.01, margin=0.01,\n",
    "                               batch_size=64, eval_subset=1000, verbose=True,\n",
    "                               weight_decay=1e-4, lr_decay=0.98,\n",
    "                               local_in_time=True, goodness_mode='all',\n",
    "                               gradient_compensation=True):\n",
    "    \"\"\"\n",
    "    Train temporal Forward-Forward with W_hh.\n",
    "    \n",
    "    Args:\n",
    "        model: TemporalFFNetwork with W_hh\n",
    "        local_in_time: Compute loss at each timestep (hardware-compatible)\n",
    "        goodness_mode: 'final' or 'all' timesteps\n",
    "        gradient_compensation: Weight early timesteps more heavily\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'goodness_pos': [],\n",
    "        'goodness_neg': [],\n",
    "        'lr': [],\n",
    "    }\n",
    "    \n",
    "    N = X_train.shape[0]\n",
    "    n_batches = (N + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Evaluation subset\n",
    "    eval_idx = torch.randperm(N)[:min(eval_subset, N)]\n",
    "    X_train_eval = X_train[eval_idx]\n",
    "    y_train_eval = y_train[eval_idx]\n",
    "    \n",
    "    best_test_acc = 0\n",
    "    \n",
    "    # Gradient compensation weights (for vanishing gradient mitigation)\n",
    "    alpha = model.layers[0].alpha\n",
    "    n_timesteps = N_ROWS\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_g_pos = []\n",
    "        epoch_g_neg = []\n",
    "        \n",
    "        perm = torch.randperm(N)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "            \n",
    "            X_batch = X_shuffled[start:end].to(device)\n",
    "            y_batch = y_shuffled[start:end].to(device)\n",
    "            \n",
    "            X_pos, X_neg = create_positive_negative_pairs_temporal(X_batch, y_batch)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            states_pos = model(X_pos)  # List of [batch, seq_len, hidden_dim]\n",
    "            states_neg = model(X_neg)\n",
    "            \n",
    "            total_loss = 0\n",
    "            batch_g_pos_list = []\n",
    "            batch_g_neg_list = []\n",
    "            \n",
    "            for layer_states_pos, layer_states_neg in zip(states_pos, states_neg):\n",
    "                seq_len = layer_states_pos.shape[1]\n",
    "                \n",
    "                if local_in_time:\n",
    "                    # Loss at each timestep\n",
    "                    for t in range(seq_len):\n",
    "                        act_pos = layer_states_pos[:, t, :]\n",
    "                        act_neg = layer_states_neg[:, t, :]\n",
    "                        \n",
    "                        g_pos = compute_goodness(act_pos)\n",
    "                        g_neg = compute_goodness(act_neg)\n",
    "                        \n",
    "                        batch_g_pos_list.append(g_pos.mean().item())\n",
    "                        batch_g_neg_list.append(g_neg.mean().item())\n",
    "                        \n",
    "                        timestep_loss = forward_forward_loss(g_pos, g_neg, margin)\n",
    "                        \n",
    "                        # Gradient compensation for vanishing gradients\n",
    "                        if gradient_compensation:\n",
    "                            weight = (1.0 / alpha) ** (seq_len - 1 - t)\n",
    "                            # Normalize\n",
    "                            normalizer = sum((1.0/alpha)**(seq_len-1-i) for i in range(seq_len))\n",
    "                            weight = weight / normalizer * seq_len\n",
    "                        else:\n",
    "                            weight = 1.0\n",
    "                        \n",
    "                        total_loss = total_loss + weight * timestep_loss\n",
    "                \n",
    "                elif goodness_mode == 'final':\n",
    "                    # Final timestep only\n",
    "                    act_pos = layer_states_pos[:, -1, :]\n",
    "                    act_neg = layer_states_neg[:, -1, :]\n",
    "                    \n",
    "                    g_pos = compute_goodness(act_pos)\n",
    "                    g_neg = compute_goodness(act_neg)\n",
    "                    \n",
    "                    batch_g_pos_list.append(g_pos.mean().item())\n",
    "                    batch_g_neg_list.append(g_neg.mean().item())\n",
    "                    \n",
    "                    total_loss = total_loss + forward_forward_loss(g_pos, g_neg, margin)\n",
    "                \n",
    "                else:  # 'all' without local_in_time\n",
    "                    for t in range(seq_len):\n",
    "                        act_pos = layer_states_pos[:, t, :]\n",
    "                        act_neg = layer_states_neg[:, t, :]\n",
    "                        \n",
    "                        g_pos = compute_goodness(act_pos)\n",
    "                        g_neg = compute_goodness(act_neg)\n",
    "                        \n",
    "                        batch_g_pos_list.append(g_pos.mean().item())\n",
    "                        batch_g_neg_list.append(g_neg.mean().item())\n",
    "                        \n",
    "                        total_loss = total_loss + forward_forward_loss(g_pos, g_neg, margin)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += total_loss.item()\n",
    "            epoch_g_pos.append(np.mean(batch_g_pos_list) if batch_g_pos_list else 0)\n",
    "            epoch_g_neg.append(np.mean(batch_g_neg_list) if batch_g_neg_list else 0)\n",
    "            \n",
    "            if verbose and batch_idx % 50 == 0:\n",
    "                print(f\"\\rEpoch {epoch+1}/{n_epochs} | Batch {batch_idx+1}/{n_batches} | \"\n",
    "                      f\"Loss: {total_loss.item():.4f}\", end=\"\")\n",
    "        \n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_mode = 'all' if local_in_time else goodness_mode\n",
    "        train_acc = evaluate_ff_temporal(model, X_train_eval, y_train_eval, goodness_mode=eval_mode)\n",
    "        test_acc = evaluate_ff_temporal(model, X_test, y_test, goodness_mode=eval_mode)\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['goodness_pos'].append(np.mean(epoch_g_pos))\n",
    "        history['goodness_neg'].append(np.mean(epoch_g_neg))\n",
    "        history['lr'].append(current_lr)\n",
    "        \n",
    "        if verbose:\n",
    "            sep = np.mean(epoch_g_pos) - np.mean(epoch_g_neg)\n",
    "            print(f\"\\rEpoch {epoch+1}/{n_epochs} | Loss: {epoch_loss/n_batches:.4f} | \"\n",
    "                  f\"Train: {train_acc:.4f} | Test: {test_acc:.4f} | \"\n",
    "                  f\"Best: {best_test_acc:.4f} | Sep: {sep:.4f}    \")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Model with W_hh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "HIDDEN_DIMS = [24]  # Best under 26 constraint\n",
    "ALPHA = 0.95        # Retention factor (1 - dt*gamma_minus)\n",
    "W_HH_SCALE = 0.1    # W_hh initialization scale\n",
    "ACTIVATION = 'tanh' # Activation function\n",
    "MARGIN = 0.01\n",
    "N_EPOCHS = 100\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 1e-4\n",
    "LR_DECAY = 0.98\n",
    "\n",
    "# Training mode\n",
    "LOCAL_IN_TIME = True\n",
    "GRADIENT_COMPENSATION = True\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FORWARD-FORWARD WITH CROSS-NEURON RECURRENCE (W_hh)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nArchitecture: {INPUT_DIM_PER_ROW} → {HIDDEN_DIMS} → goodness\")\n",
    "print(f\"Timesteps: {N_ROWS} (one per row)\")\n",
    "print(f\"Alpha (retention): {ALPHA}\")\n",
    "print(f\"W_hh scale: {W_HH_SCALE}\")\n",
    "print(f\"\\nKEY UPGRADE:\")\n",
    "print(f\"  Original: s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t])\")\n",
    "print(f\"  This:     s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t] + W_hh×s[t-1])\")\n",
    "print(f\"  → W_hh enables cross-neuron temporal communication!\")\n",
    "print(f\"\\nTraining: {N_TRAIN} samples, Testing: {N_TEST} samples\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build model\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "model = TemporalFFNetwork(\n",
    "    input_dim=INPUT_DIM_PER_ROW,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    alpha=ALPHA,\n",
    "    activation=ACTIVATION,\n",
    "    w_hh_scale=W_HH_SCALE,\n",
    "    sparse_hh=False,\n",
    ").to(device)\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Parameters: {n_params}\")\n",
    "print(f\"  W_ih: {INPUT_DIM_PER_ROW * HIDDEN_DIMS[0] + HIDDEN_DIMS[0]}\")\n",
    "print(f\"  W_hh: {HIDDEN_DIMS[0] * HIDDEN_DIMS[0]} ← NEW!\")\n",
    "\n",
    "# Train\n",
    "history = train_forward_forward_whh(\n",
    "    model, X_train, y_train, X_test, y_test,\n",
    "    n_epochs=N_EPOCHS, lr=LR, margin=MARGIN,\n",
    "    batch_size=BATCH_SIZE, verbose=True,\n",
    "    weight_decay=WEIGHT_DECAY, lr_decay=LR_DECAY,\n",
    "    local_in_time=LOCAL_IN_TIME,\n",
    "    gradient_compensation=GRADIENT_COMPENSATION,\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final test accuracy: {history['test_acc'][-1]:.4f}\")\n",
    "print(f\"Best test accuracy: {max(history['test_acc']):.4f}\")\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Previous best (without W_hh): ~30.5%\")\n",
    "print(f\"  This version (with W_hh): {max(history['test_acc'])*100:.1f}%\")\n",
    "print(f\"  Random baseline: 10%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['loss'], color='steelblue', lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Contrastive Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Goodness\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['goodness_pos'], label='Positive (G+)', color='green', lw=2)\n",
    "ax2.plot(history['goodness_neg'], label='Negative (G-)', color='red', lw=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Goodness')\n",
    "ax2.set_title('Goodness Values')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate\n",
    "ax3 = axes[0, 2]\n",
    "ax3.plot(history['lr'], color='orange', lw=2)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Learning Rate')\n",
    "ax3.set_title('Learning Rate Decay')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax4 = axes[1, 0]\n",
    "ax4.plot(history['train_acc'], label='Train', color='coral', lw=2)\n",
    "ax4.plot(history['test_acc'], label='Test', color='steelblue', lw=2)\n",
    "ax4.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random (10%)')\n",
    "ax4.axhline(y=0.305, color='purple', linestyle=':', alpha=0.7, label='Previous best (30.5%)')\n",
    "best_epoch = np.argmax(history['test_acc'])\n",
    "ax4.axvline(x=best_epoch, color='green', linestyle=':', alpha=0.7)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.set_title(f'Classification Accuracy (Best: {max(history[\"test_acc\"]):.2%})')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim(0, 1.0)\n",
    "\n",
    "# Separation\n",
    "ax5 = axes[1, 1]\n",
    "separation = [p - n for p, n in zip(history['goodness_pos'], history['goodness_neg'])]\n",
    "ax5.plot(separation, color='purple', lw=2)\n",
    "ax5.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('G+ - G-')\n",
    "ax5.set_title('Goodness Separation')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Train vs Test gap\n",
    "ax6 = axes[1, 2]\n",
    "gap = [t - v for t, v in zip(history['train_acc'], history['test_acc'])]\n",
    "ax6.plot(gap, color='brown', lw=2)\n",
    "ax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Train - Test')\n",
    "ax6.set_title('Generalization Gap')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Forward-Forward with W_hh ({sum(HIDDEN_DIMS)} neurons)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize W_hh Learned Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the learned W_hh matrix\n",
    "W_hh = model.layers[0].W_hh.weight.data.cpu().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# W_hh heatmap\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(W_hh, cmap='RdBu_r', aspect='auto', vmin=-np.abs(W_hh).max(), vmax=np.abs(W_hh).max())\n",
    "ax1.set_xlabel('From neuron')\n",
    "ax1.set_ylabel('To neuron')\n",
    "ax1.set_title('W_hh (Hidden-to-Hidden Weights)')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# W_hh distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(W_hh.flatten(), bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(x=0, color='red', linestyle='--')\n",
    "ax2.set_xlabel('Weight value')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title(f'W_hh Distribution (mean={W_hh.mean():.4f}, std={W_hh.std():.4f})')\n",
    "\n",
    "# Top connections\n",
    "ax3 = axes[2]\n",
    "# Find strongest connections\n",
    "flat_idx = np.argsort(np.abs(W_hh).flatten())[-20:][::-1]\n",
    "top_connections = [(i // W_hh.shape[1], i % W_hh.shape[1], W_hh.flatten()[i]) for i in flat_idx]\n",
    "labels = [f'{i}→{j}' for i, j, _ in top_connections]\n",
    "values = [v for _, _, v in top_connections]\n",
    "colors = ['green' if v > 0 else 'red' for v in values]\n",
    "ax3.barh(range(len(values)), values, color=colors, edgecolor='black')\n",
    "ax3.set_yticks(range(len(labels)))\n",
    "ax3.set_yticklabels(labels)\n",
    "ax3.axvline(x=0, color='black', linestyle='-')\n",
    "ax3.set_xlabel('Weight')\n",
    "ax3.set_title('Strongest W_hh Connections')\n",
    "\n",
    "plt.suptitle('Cross-Neuron Recurrence Analysis', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nW_hh Statistics:\")\n",
    "print(f\"  Shape: {W_hh.shape}\")\n",
    "print(f\"  Mean: {W_hh.mean():.6f}\")\n",
    "print(f\"  Std: {W_hh.std():.6f}\")\n",
    "print(f\"  Max: {W_hh.max():.6f}\")\n",
    "print(f\"  Min: {W_hh.min():.6f}\")\n",
    "print(f\"  Sparsity (|w|<0.01): {(np.abs(W_hh) < 0.01).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compare W_hh Scales and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different W_hh configurations\n",
    "configs = [\n",
    "    {'name': 'No W_hh (baseline)', 'w_hh_scale': 0.0},\n",
    "    {'name': 'W_hh scale=0.05', 'w_hh_scale': 0.05},\n",
    "    {'name': 'W_hh scale=0.1', 'w_hh_scale': 0.1},\n",
    "    {'name': 'W_hh scale=0.2', 'w_hh_scale': 0.2},\n",
    "    {'name': 'W_hh scale=0.3', 'w_hh_scale': 0.3},\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "print(\"Comparing W_hh configurations...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in configs:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = TemporalFFNetwork(\n",
    "        input_dim=INPUT_DIM_PER_ROW,\n",
    "        hidden_dims=[24],\n",
    "        alpha=0.95,\n",
    "        activation='tanh',\n",
    "        w_hh_scale=config['w_hh_scale'],\n",
    "    ).to(device)\n",
    "    \n",
    "    # Zero out W_hh for baseline\n",
    "    if config['w_hh_scale'] == 0.0:\n",
    "        with torch.no_grad():\n",
    "            model.layers[0].W_hh.weight.zero_()\n",
    "            model.layers[0].W_hh.weight.requires_grad = False\n",
    "    \n",
    "    history = train_forward_forward_whh(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, margin=0.01,\n",
    "        batch_size=64, verbose=False,\n",
    "        weight_decay=1e-4, lr_decay=0.98,\n",
    "        local_in_time=True,\n",
    "        gradient_compensation=True,\n",
    "    )\n",
    "    \n",
    "    best_test = max(history['test_acc'])\n",
    "    comparison_results.append({\n",
    "        'config': config['name'],\n",
    "        'w_hh_scale': config['w_hh_scale'],\n",
    "        'train_acc': history['train_acc'][-1],\n",
    "        'test_acc': history['test_acc'][-1],\n",
    "        'best_test': best_test,\n",
    "    })\n",
    "    \n",
    "    print(f\"{config['name']:25s} | Final: {history['test_acc'][-1]:.4f} | Best: {best_test:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best configuration\n",
    "best_config = max(comparison_results, key=lambda x: x['best_test'])\n",
    "baseline = [r for r in comparison_results if r['w_hh_scale'] == 0.0][0]\n",
    "\n",
    "print(f\"\\nBest configuration: {best_config['config']} with {best_config['best_test']:.2%}\")\n",
    "print(f\"Improvement over baseline: {(best_config['best_test'] - baseline['best_test'])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Try Different Alpha Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different alpha (retention) values\n",
    "alpha_configs = [\n",
    "    {'alpha': 0.90, 'name': 'α=0.90 (fast decay)'},\n",
    "    {'alpha': 0.95, 'name': 'α=0.95 (moderate)'},\n",
    "    {'alpha': 0.97, 'name': 'α=0.97 (slow decay)'},\n",
    "    {'alpha': 0.99, 'name': 'α=0.99 (very slow)'},\n",
    "]\n",
    "\n",
    "alpha_results = []\n",
    "\n",
    "print(\"Comparing alpha (retention factor) values with W_hh...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in alpha_configs:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = TemporalFFNetwork(\n",
    "        input_dim=INPUT_DIM_PER_ROW,\n",
    "        hidden_dims=[24],\n",
    "        alpha=config['alpha'],\n",
    "        activation='tanh',\n",
    "        w_hh_scale=0.1,\n",
    "    ).to(device)\n",
    "    \n",
    "    history = train_forward_forward_whh(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, margin=0.01,\n",
    "        batch_size=64, verbose=False,\n",
    "        weight_decay=1e-4, lr_decay=0.98,\n",
    "        local_in_time=True,\n",
    "        gradient_compensation=True,\n",
    "    )\n",
    "    \n",
    "    best_test = max(history['test_acc'])\n",
    "    alpha_results.append({\n",
    "        'config': config['name'],\n",
    "        'alpha': config['alpha'],\n",
    "        'best_test': best_test,\n",
    "        'history': history,\n",
    "    })\n",
    "    \n",
    "    print(f\"{config['name']:25s} | Best: {best_test:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for result in alpha_results:\n",
    "    ax.plot(result['history']['test_acc'], label=result['config'], lw=2)\n",
    "ax.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Effect of Alpha (Retention Factor) with W_hh')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sparse W_hh (Hardware-Realistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try sparse W_hh (more hardware-realistic)\n",
    "sparsity_configs = [\n",
    "    {'sparse_hh': False, 'sparsity': 1.0, 'name': 'Dense W_hh (100%)'},\n",
    "    {'sparse_hh': True, 'sparsity': 0.5, 'name': 'Sparse W_hh (50%)'},\n",
    "    {'sparse_hh': True, 'sparsity': 0.3, 'name': 'Sparse W_hh (30%)'},\n",
    "    {'sparse_hh': True, 'sparsity': 0.1, 'name': 'Sparse W_hh (10%)'},\n",
    "]\n",
    "\n",
    "sparsity_results = []\n",
    "\n",
    "print(\"Comparing sparse vs dense W_hh (hardware constraints)...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for config in sparsity_configs:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = TemporalFFNetwork(\n",
    "        input_dim=INPUT_DIM_PER_ROW,\n",
    "        hidden_dims=[24],\n",
    "        alpha=0.95,\n",
    "        activation='tanh',\n",
    "        w_hh_scale=0.1,\n",
    "        sparse_hh=config['sparse_hh'],\n",
    "        sparsity=config['sparsity'],\n",
    "    ).to(device)\n",
    "    \n",
    "    history = train_forward_forward_whh(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, margin=0.01,\n",
    "        batch_size=64, verbose=False,\n",
    "        weight_decay=1e-4, lr_decay=0.98,\n",
    "        local_in_time=True,\n",
    "        gradient_compensation=True,\n",
    "    )\n",
    "    \n",
    "    best_test = max(history['test_acc'])\n",
    "    sparsity_results.append({\n",
    "        'config': config['name'],\n",
    "        'sparsity': config['sparsity'],\n",
    "        'best_test': best_test,\n",
    "    })\n",
    "    \n",
    "    print(f\"{config['name']:25s} | Best: {best_test:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNote: Sparse W_hh is more hardware-realistic because optical routing\")\n",
    "print(\"      is expensive. 10-30% connectivity may be practical.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONCLUSIONS: FORWARD-FORWARD WITH CROSS-NEURON RECURRENCE (W_hh)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. KEY UPGRADE:\")\n",
    "print(f\"   Original: s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t])\")\n",
    "print(f\"   This:     s[t] = α×s[t-1] + (1-α)×g(W_ih×x[t] + W_hh×s[t-1])\")\n",
    "print(f\"   → W_hh allows neurons to share temporal patterns!\")\n",
    "\n",
    "print(f\"\\n2. ARCHITECTURE:\")\n",
    "print(f\"   Input: {INPUT_DIM_PER_ROW} ({N_COLS} pixels + {N_CLASSES} label)\")\n",
    "print(f\"   Timesteps: {N_ROWS}\")\n",
    "print(f\"   Hidden: {sum(HIDDEN_DIMS)} neurons\")\n",
    "print(f\"   W_ih: {INPUT_DIM_PER_ROW} × {HIDDEN_DIMS[0]} = {INPUT_DIM_PER_ROW * HIDDEN_DIMS[0]} weights\")\n",
    "print(f\"   W_hh: {HIDDEN_DIMS[0]} × {HIDDEN_DIMS[0]} = {HIDDEN_DIMS[0]**2} weights (NEW!)\")\n",
    "\n",
    "print(f\"\\n3. PERFORMANCE:\")\n",
    "print(f\"   Previous best (without W_hh): ~30.5%\")\n",
    "print(f\"   This version (with W_hh):    {max(history['test_acc'])*100:.1f}%\")\n",
    "print(f\"   Random baseline: 10%\")\n",
    "\n",
    "print(f\"\\n4. HARDWARE COMPATIBILITY:\")\n",
    "print(f\"   ✓ W_hh implemented via optical waveguide routing\")\n",
    "print(f\"   ✓ Topology fixed at fabrication (Shainline 2021)\")\n",
    "print(f\"   ✓ Sparse W_hh reduces routing complexity\")\n",
    "print(f\"   ✓ Local-in-time learning (no BPTT hardware needed)\")\n",
    "\n",
    "print(f\"\\n5. WHY W_hh HELPS:\")\n",
    "print(f\"   Without W_hh: Each neuron can only remember its own past\")\n",
    "print(f\"   With W_hh:    Neurons can share temporal information\")\n",
    "print(f\"   Example: Neuron 1 detects top rows → tells Neuron 2 for bottom rows\")\n",
    "\n",
    "print(f\"\\n6. LIMITATIONS:\")\n",
    "print(f\"   - Still learning with BPTT (for gradient flow)\")\n",
    "print(f\"   - Hardware would need different learning rule\")\n",
    "print(f\"   - Fixed topology may limit flexibility\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
