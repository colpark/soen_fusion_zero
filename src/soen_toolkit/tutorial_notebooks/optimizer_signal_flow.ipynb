{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Signal Flow & Training Dynamics in SOEN Model\n",
    "\n",
    "This notebook visualizes how signals propagate through the minimal SOEN model and how gradients flow during training.\n",
    "\n",
    "## Model Architecture\n",
    "```\n",
    "Input x → [J_in] → φ → SingleDendrite → s → [J_out] → Output y\n",
    "```\n",
    "\n",
    "## Key Equations\n",
    "\n",
    "### 1. Input Flux Computation\n",
    "$$\\phi(t) = J_{in} \\cdot x(t) + \\phi_{offset}$$\n",
    "\n",
    "### 2. SingleDendrite Dynamics (ODE)\n",
    "$$\\frac{ds}{dt} = \\gamma^+ \\cdot g(\\phi) - \\gamma^- \\cdot s$$\n",
    "\n",
    "where $g(\\phi)$ is the source function (e.g., Heaviside fit)\n",
    "\n",
    "### 3. Output Computation\n",
    "$$y = J_{out} \\cdot s_{final}$$\n",
    "\n",
    "### 4. Loss Function\n",
    "$$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - y_{target,i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Build Model with Tracking Enabled\n",
    "\n",
    "We enable `track_phi=True` to record the input flux at each timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_with_tracking(j_in=0.15, j_out=1.5):\n",
    "    \"\"\"Build SOEN model with phi tracking enabled.\"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=50.0,\n",
    "        input_type=\"state\",\n",
    "        track_phi=True,  # Track input flux!\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layer0 = LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    )\n",
    "    \n",
    "    layer1 = LayerConfig(\n",
    "        layer_id=1,\n",
    "        layer_type=\"SingleDendrite\",\n",
    "        params={\n",
    "            \"dim\": 1,\n",
    "            \"solver\": \"FE\",\n",
    "            \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "            \"phi_offset\": 0.02,\n",
    "            \"bias_current\": 1.98,\n",
    "            \"gamma_plus\": 0.0005,\n",
    "            \"gamma_minus\": 1e-6,\n",
    "            \"learnable_params\": {\n",
    "                \"phi_offset\": False,\n",
    "                \"bias_current\": False,\n",
    "                \"gamma_plus\": False,\n",
    "                \"gamma_minus\": False,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    layer2 = LayerConfig(\n",
    "        layer_id=2,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    )\n",
    "    \n",
    "    conn01 = ConnectionConfig(\n",
    "        from_layer=0, to_layer=1,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"constant\", \"value\": j_in},\n",
    "    )\n",
    "    \n",
    "    conn12 = ConnectionConfig(\n",
    "        from_layer=1, to_layer=2,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"constant\", \"value\": j_out},\n",
    "    )\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=[layer0, layer1, layer2],\n",
    "        connections_config=[conn01, conn12],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "J_IN = 0.15\n",
    "J_OUT = 1.5\n",
    "model = build_model_with_tracking(J_IN, J_OUT)\n",
    "\n",
    "print(\"Trainable parameters:\")\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"  {name}: {p.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Create Test Input Signal\n",
    "\n",
    "We'll use a simple constant input to clearly see the dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input: constant value over time\n",
    "SEQ_LEN = 50\n",
    "INPUT_VALUE = 0.15  # Constant input flux\n",
    "\n",
    "# Single sample, constant input\n",
    "x_input = torch.full((1, SEQ_LEN, 1), INPUT_VALUE)\n",
    "\n",
    "print(f\"Input shape: {x_input.shape}\")\n",
    "print(f\"Input value: {INPUT_VALUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Visualize Complete Signal Flow\n",
    "\n",
    "Let's trace the signal through every stage of the model:\n",
    "\n",
    "```\n",
    "x(t) → φ(t) = J_in·x(t) + φ_offset → s(t) via ODE → y = J_out·s_final\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    final_hist, all_histories = model(x_input)\n",
    "\n",
    "# Extract signals\n",
    "x_signal = x_input[0, :, 0].numpy()  # Input signal\n",
    "s_history = all_histories[1][0, :, 0].numpy()  # SingleDendrite state over time\n",
    "output_history = all_histories[2][0, :, 0].numpy()  # Output layer state\n",
    "\n",
    "# Get phi (input flux to SingleDendrite) if tracked\n",
    "phi_history = None\n",
    "if hasattr(model.layers[1], 'phi_history') and model.layers[1].phi_history is not None:\n",
    "    phi_history = model.layers[1].phi_history[0, :, 0].numpy()\n",
    "\n",
    "# Compute phi manually for visualization\n",
    "phi_offset = 0.02\n",
    "phi_computed = J_IN * x_signal + phi_offset\n",
    "\n",
    "# Time axis\n",
    "t = np.arange(SEQ_LEN)\n",
    "t_state = np.arange(len(s_history))  # State has +1 for initial condition\n",
    "\n",
    "print(f\"Input x: {INPUT_VALUE}\")\n",
    "print(f\"Computed φ = J_in·x + φ_offset = {J_IN}·{INPUT_VALUE} + {phi_offset} = {J_IN * INPUT_VALUE + phi_offset:.4f}\")\n",
    "print(f\"Final state s: {s_history[-1]:.4f}\")\n",
    "print(f\"Output y = J_out·s = {J_OUT}·{s_history[-1]:.4f} = {J_OUT * s_history[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = GridSpec(3, 3, figure=fig, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# === Row 1: Signal Flow ===\n",
    "\n",
    "# 1a. Input Signal x(t)\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(t, x_signal, 'b-', linewidth=2, label='x(t)')\n",
    "ax1.fill_between(t, 0, x_signal, alpha=0.3)\n",
    "ax1.set_xlabel('Time step')\n",
    "ax1.set_ylabel('x(t)')\n",
    "ax1.set_title('① Input Signal x(t)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylim(0, 0.25)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# 1b. Input Flux φ(t) = J_in·x(t) + φ_offset\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(t, phi_computed, 'g-', linewidth=2, label=f'φ = {J_IN}·x + {phi_offset}')\n",
    "ax2.axhline(y=phi_offset, color='gray', linestyle='--', label=f'φ_offset = {phi_offset}')\n",
    "ax2.fill_between(t, phi_offset, phi_computed, alpha=0.3, color='green', label=f'J_in·x contribution')\n",
    "ax2.set_xlabel('Time step')\n",
    "ax2.set_ylabel('φ(t)')\n",
    "ax2.set_title('② Input Flux φ(t) = J_in·x + φ_offset', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=8)\n",
    "\n",
    "# 1c. SingleDendrite State s(t)\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.plot(t_state, s_history, 'r-', linewidth=2, label='s(t)')\n",
    "ax3.axhline(y=s_history[-1], color='red', linestyle='--', alpha=0.5, label=f's_final = {s_history[-1]:.3f}')\n",
    "ax3.fill_between(t_state, 0, s_history, alpha=0.3, color='red')\n",
    "ax3.set_xlabel('Time step')\n",
    "ax3.set_ylabel('s(t)')\n",
    "ax3.set_title('③ SingleDendrite State s(t)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.legend()\n",
    "\n",
    "# === Row 2: Parameter Effects ===\n",
    "\n",
    "# 2a. Effect of J_in on φ\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "j_in_values = [0.05, 0.10, 0.15, 0.20, 0.25]\n",
    "colors = plt.cm.Blues(np.linspace(0.3, 1, len(j_in_values)))\n",
    "for j_in, color in zip(j_in_values, colors):\n",
    "    phi_val = j_in * INPUT_VALUE + phi_offset\n",
    "    ax4.axhline(y=phi_val, color=color, linewidth=2, label=f'J_in={j_in:.2f} → φ={phi_val:.3f}')\n",
    "ax4.set_ylabel('φ (input flux)')\n",
    "ax4.set_title('Effect of J_in on Input Flux φ', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlim(0, 1)\n",
    "ax4.set_xticks([])\n",
    "ax4.legend(loc='right', fontsize=8)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2b. Effect of J_in on s(t) dynamics\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "for j_in, color in zip(j_in_values, colors):\n",
    "    test_model = build_model_with_tracking(j_in=j_in, j_out=J_OUT)\n",
    "    test_model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, test_histories = test_model(x_input)\n",
    "    s_test = test_histories[1][0, :, 0].numpy()\n",
    "    ax5.plot(s_test, color=color, linewidth=2, label=f'J_in={j_in:.2f}')\n",
    "ax5.set_xlabel('Time step')\n",
    "ax5.set_ylabel('s(t)')\n",
    "ax5.set_title('Effect of J_in on State Evolution', fontsize=12, fontweight='bold')\n",
    "ax5.legend(fontsize=8)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 2c. Effect of J_out on output\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "j_out_values = [0.5, 1.0, 1.5, 2.0, 2.5]\n",
    "s_final = s_history[-1]\n",
    "outputs = [j_out * s_final for j_out in j_out_values]\n",
    "colors_out = plt.cm.Oranges(np.linspace(0.3, 1, len(j_out_values)))\n",
    "bars = ax6.bar(range(len(j_out_values)), outputs, color=colors_out)\n",
    "ax6.set_xticks(range(len(j_out_values)))\n",
    "ax6.set_xticklabels([f'J_out={j:.1f}' for j in j_out_values], rotation=45)\n",
    "ax6.set_ylabel('Output y = J_out · s_final')\n",
    "ax6.set_title(f'Effect of J_out on Output (s_final={s_final:.3f})', fontsize=12, fontweight='bold')\n",
    "ax6.grid(True, alpha=0.3, axis='y')\n",
    "for bar, out in zip(bars, outputs):\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{out:.2f}', ha='center', fontsize=9)\n",
    "\n",
    "# === Row 3: Complete Pipeline Diagram ===\n",
    "ax7 = fig.add_subplot(gs[2, :])\n",
    "ax7.set_xlim(0, 10)\n",
    "ax7.set_ylim(0, 3)\n",
    "ax7.axis('off')\n",
    "ax7.set_title('Complete Signal Flow Pipeline', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Draw boxes and arrows\n",
    "box_style = dict(boxstyle='round,pad=0.3', facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "arrow_style = dict(arrowstyle='->', color='black', linewidth=2)\n",
    "\n",
    "# Input\n",
    "ax7.text(0.5, 2, f'Input\\nx = {INPUT_VALUE}', ha='center', va='center', fontsize=11, bbox=box_style)\n",
    "\n",
    "# Arrow with J_in\n",
    "ax7.annotate('', xy=(1.8, 2), xytext=(1.1, 2), arrowprops=arrow_style)\n",
    "ax7.text(1.45, 2.4, f'× J_in\\n({J_IN})', ha='center', va='center', fontsize=10, color='blue', fontweight='bold')\n",
    "\n",
    "# Phi computation\n",
    "ax7.text(2.5, 2, f'+ φ_offset\\n({phi_offset})', ha='center', va='center', fontsize=10, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightgreen', edgecolor='black'))\n",
    "\n",
    "# Arrow to phi\n",
    "ax7.annotate('', xy=(3.8, 2), xytext=(3.2, 2), arrowprops=arrow_style)\n",
    "\n",
    "# Phi result\n",
    "phi_val = J_IN * INPUT_VALUE + phi_offset\n",
    "ax7.text(4.5, 2, f'φ = {phi_val:.4f}', ha='center', va='center', fontsize=11, \n",
    "         bbox=dict(boxstyle='round', facecolor='lightyellow', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Arrow to ODE\n",
    "ax7.annotate('', xy=(5.8, 2), xytext=(5.2, 2), arrowprops=arrow_style)\n",
    "\n",
    "# SingleDendrite ODE\n",
    "ax7.text(6.8, 2, 'SingleDendrite\\nODE Solver', ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightsalmon', edgecolor='black', linewidth=2))\n",
    "ax7.text(6.8, 1.2, r'$\\frac{ds}{dt} = \\gamma^+ g(\\phi) - \\gamma^- s$', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Arrow to s_final\n",
    "ax7.annotate('', xy=(8.3, 2), xytext=(7.7, 2), arrowprops=arrow_style)\n",
    "\n",
    "# s_final\n",
    "ax7.text(8.8, 2, f's_final\\n= {s_history[-1]:.4f}', ha='center', va='center', fontsize=11,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Arrow with J_out\n",
    "ax7.annotate('', xy=(9.8, 2), xytext=(9.4, 2), arrowprops=arrow_style)\n",
    "ax7.text(9.6, 2.4, f'× J_out\\n({J_OUT})', ha='center', va='center', fontsize=10, color='orange', fontweight='bold')\n",
    "\n",
    "# Output\n",
    "output_val = J_OUT * s_history[-1]\n",
    "ax7.text(10.3, 2, f'Output\\ny = {output_val:.4f}', ha='center', va='center', fontsize=11, \n",
    "         bbox=dict(boxstyle='round', facecolor='plum', edgecolor='black', linewidth=2))\n",
    "\n",
    "# Formula summary\n",
    "ax7.text(5.5, 0.5, \n",
    "         f'Summary: y = J_out × s_final = {J_OUT} × {s_history[-1]:.4f} = {output_val:.4f}\\n'\n",
    "         f'where s_final comes from ODE driven by φ = J_in × x + φ_offset = {J_IN} × {INPUT_VALUE} + {phi_offset} = {phi_val:.4f}',\n",
    "         ha='center', va='center', fontsize=11, \n",
    "         bbox=dict(boxstyle='round', facecolor='white', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Gradient Flow During Training\n",
    "\n",
    "During backpropagation, gradients flow backward through the computation graph:\n",
    "\n",
    "```\n",
    "∂L/∂J_out ← ∂L/∂y × ∂y/∂J_out\n",
    "∂L/∂J_in  ← ∂L/∂y × ∂y/∂s × ∂s/∂φ × ∂φ/∂J_in\n",
    "```\n",
    "\n",
    "### Gradient Formulas:\n",
    "\n",
    "**For J_out (simple):**\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial J_{out}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot s_{final}$$\n",
    "\n",
    "**For J_in (through ODE):**\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial J_{in}} = \\frac{\\partial \\mathcal{L}}{\\partial y} \\cdot J_{out} \\cdot \\frac{\\partial s_{final}}{\\partial \\phi} \\cdot x$$\n",
    "\n",
    "The term $\\frac{\\partial s_{final}}{\\partial \\phi}$ requires backpropagation through the ODE solver (adjoint method or direct differentiation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate gradient computation\n",
    "model = build_model_with_tracking(J_IN, J_OUT)\n",
    "model.train()\n",
    "\n",
    "# Target value\n",
    "y_target = torch.tensor([[1.0]])  # We want output to be 1.0\n",
    "\n",
    "# Forward pass\n",
    "final_hist, all_histories = model(x_input)\n",
    "y_pred = final_hist[:, -1, :]  # Take final timestep\n",
    "\n",
    "print(f\"Input x: {INPUT_VALUE}\")\n",
    "print(f\"Predicted y: {y_pred.item():.4f}\")\n",
    "print(f\"Target y: {y_target.item():.4f}\")\n",
    "\n",
    "# Compute loss\n",
    "loss = nn.MSELoss()(y_pred, y_target)\n",
    "print(f\"\\nLoss: {loss.item():.6f}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Show gradients\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GRADIENTS:\")\n",
    "print(\"=\"*50)\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Value:    {p.item():.6f}\")\n",
    "        print(f\"  Gradient: {p.grad.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient flow\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get parameter info\n",
    "params = {name: (p.item(), p.grad.item()) for name, p in model.named_parameters() if p.requires_grad}\n",
    "param_names = list(params.keys())\n",
    "values = [params[n][0] for n in param_names]\n",
    "grads = [params[n][1] for n in param_names]\n",
    "\n",
    "# Plot 1: Parameter values\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(['J_in', 'J_out'], values, color=['blue', 'orange'])\n",
    "ax1.set_ylabel('Parameter Value')\n",
    "ax1.set_title('Current Parameter Values', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "for bar, val in zip(bars1, values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "             f'{val:.4f}', ha='center', fontsize=11)\n",
    "\n",
    "# Plot 2: Gradients\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if g < 0 else 'red' for g in grads]\n",
    "bars2 = ax2.bar(['∂L/∂J_in', '∂L/∂J_out'], grads, color=colors)\n",
    "ax2.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax2.set_ylabel('Gradient Value')\n",
    "ax2.set_title('Gradients (direction to INCREASE loss)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "for bar, grad in zip(bars2, grads):\n",
    "    offset = 0.01 if grad >= 0 else -0.01\n",
    "    va = 'bottom' if grad >= 0 else 'top'\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + offset, \n",
    "             f'{grad:.4f}', ha='center', va=va, fontsize=11)\n",
    "\n",
    "plt.suptitle(f'Gradient Analysis (y_pred={y_pred.item():.3f}, y_target={y_target.item():.3f}, loss={loss.item():.4f})', \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "for name, (val, grad) in zip(['J_in', 'J_out'], [(values[0], grads[0]), (values[1], grads[1])]):\n",
    "    direction = \"DECREASE\" if grad > 0 else \"INCREASE\"\n",
    "    print(f\"  {name}: gradient = {grad:.4f} → optimizer will {direction} this parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Training Dynamics Visualization\n",
    "\n",
    "Let's watch how J_in and J_out evolve during training, and see how the signal flow changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with detailed tracking\n",
    "def train_with_tracking(target_y, n_epochs=50, lr=0.1):\n",
    "    \"\"\"Train model and track all intermediate values.\"\"\"\n",
    "    model = build_model_with_tracking(j_in=0.1, j_out=0.5)  # Start far from optimal\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {\n",
    "        'j_in': [], 'j_out': [],\n",
    "        'phi': [], 's_final': [], 'y_pred': [],\n",
    "        'loss': [],\n",
    "        'grad_j_in': [], 'grad_j_out': []\n",
    "    }\n",
    "    \n",
    "    y_target = torch.tensor([[target_y]])\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get current parameters\n",
    "        params = list(model.parameters())\n",
    "        j_in = params[0].item()\n",
    "        j_out = params[1].item()\n",
    "        \n",
    "        # Forward\n",
    "        final_hist, all_hist = model(x_input)\n",
    "        y_pred = final_hist[:, -1, :]\n",
    "        s_final = all_hist[1][0, -1, 0].item()\n",
    "        phi = j_in * INPUT_VALUE + 0.02\n",
    "        \n",
    "        # Loss & backward\n",
    "        loss = criterion(y_pred, y_target)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Record\n",
    "        history['j_in'].append(j_in)\n",
    "        history['j_out'].append(j_out)\n",
    "        history['phi'].append(phi)\n",
    "        history['s_final'].append(s_final)\n",
    "        history['y_pred'].append(y_pred.item())\n",
    "        history['loss'].append(loss.item())\n",
    "        history['grad_j_in'].append(params[0].grad.item())\n",
    "        history['grad_j_out'].append(params[1].grad.item())\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Train to reach target output of 1.0\n",
    "TARGET_Y = 1.0\n",
    "history = train_with_tracking(TARGET_Y, n_epochs=100, lr=0.05)\n",
    "\n",
    "print(f\"Training complete!\")\n",
    "print(f\"  Initial: J_in={history['j_in'][0]:.4f}, J_out={history['j_out'][0]:.4f}, y={history['y_pred'][0]:.4f}\")\n",
    "print(f\"  Final:   J_in={history['j_in'][-1]:.4f}, J_out={history['j_out'][-1]:.4f}, y={history['y_pred'][-1]:.4f}\")\n",
    "print(f\"  Target:  y={TARGET_Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive training visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "epochs = range(len(history['loss']))\n",
    "\n",
    "# 1. Loss curve\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(epochs, history['loss'], 'k-', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Training Loss', fontsize=12, fontweight='bold')\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. J_in evolution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(epochs, history['j_in'], 'b-', linewidth=2, label='J_in')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('J_in')\n",
    "ax2.set_title('J_in Evolution (Input Weight)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. J_out evolution\n",
    "ax3 = axes[0, 2]\n",
    "ax3.plot(epochs, history['j_out'], 'orange', linewidth=2, label='J_out')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('J_out')\n",
    "ax3.set_title('J_out Evolution (Output Weight)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Signal flow: phi\n",
    "ax4 = axes[1, 0]\n",
    "ax4.plot(epochs, history['phi'], 'g-', linewidth=2)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('φ = J_in·x + φ_offset')\n",
    "ax4.set_title('Input Flux φ Evolution', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Signal flow: s_final\n",
    "ax5 = axes[1, 1]\n",
    "ax5.plot(epochs, history['s_final'], 'r-', linewidth=2)\n",
    "ax5.set_xlabel('Epoch')\n",
    "ax5.set_ylabel('s_final')\n",
    "ax5.set_title('SingleDendrite Final State Evolution', fontsize=12, fontweight='bold')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Output y vs target\n",
    "ax6 = axes[1, 2]\n",
    "ax6.plot(epochs, history['y_pred'], 'purple', linewidth=2, label='y_pred')\n",
    "ax6.axhline(y=TARGET_Y, color='red', linestyle='--', linewidth=2, label=f'Target = {TARGET_Y}')\n",
    "ax6.set_xlabel('Epoch')\n",
    "ax6.set_ylabel('Output y')\n",
    "ax6.set_title('Output y = J_out · s_final', fontsize=12, fontweight='bold')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training Dynamics: How J_in and J_out are Learned', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient evolution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.plot(epochs, history['grad_j_in'], 'b-', linewidth=2)\n",
    "ax1.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('∂L/∂J_in')\n",
    "ax1.set_title('Gradient of Loss w.r.t. J_in', fontsize=12, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs, history['grad_j_out'], 'orange', linewidth=2)\n",
    "ax2.axhline(y=0, color='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('∂L/∂J_out')\n",
    "ax2.set_title('Gradient of Loss w.r.t. J_out', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Gradient Evolution During Training', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGradient Interpretation:\")\n",
    "print(\"  - When gradient is positive: optimizer decreases the parameter\")\n",
    "print(\"  - When gradient is negative: optimizer increases the parameter\")\n",
    "print(\"  - As training converges, gradients approach zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 6. Summary: What J_in and J_out Control\n",
    "\n",
    "### J_in (Input Weight)\n",
    "- **Location**: Connection from Input layer to SingleDendrite\n",
    "- **Formula**: φ = **J_in** · x + φ_offset\n",
    "- **Effect**: Scales the input signal before it enters the neuron\n",
    "- **Physical meaning**: Controls how strongly the input flux drives the neuron\n",
    "\n",
    "### J_out (Output Weight)  \n",
    "- **Location**: Connection from SingleDendrite to Output layer\n",
    "- **Formula**: y = **J_out** · s_final\n",
    "- **Effect**: Scales the neuron's final state to produce the output\n",
    "- **Physical meaning**: Controls the readout gain from the neuron\n",
    "\n",
    "### Training Process\n",
    "1. **Forward pass**: x → φ → s(t) → y\n",
    "2. **Loss computation**: L = (y - y_target)²\n",
    "3. **Backward pass**: Compute ∂L/∂J_in and ∂L/∂J_out\n",
    "4. **Update**: J_in -= lr · ∂L/∂J_in, J_out -= lr · ∂L/∂J_out\n",
    "\n",
    "### Key Insight\n",
    "With only J_in and J_out, the model can learn **scaling** relationships but NOT **affine** transformations (y = αx + β) because there's no learnable bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Notebook complete!\")\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"  1. J_in controls input scaling: φ = J_in·x + φ_offset\")\n",
    "print(\"  2. J_out controls output scaling: y = J_out·s_final\")\n",
    "print(\"  3. Gradients flow backward through the ODE solver\")\n",
    "print(\"  4. Both parameters adjust to minimize the loss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
