{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward-Forward MNIST Classification (Hinton's Recurrent Approach)\n",
    "\n",
    "Implementation of Hinton's recurrent Forward-Forward as described in Section 5 of\n",
    "[\"The Forward-Forward Algorithm: Some Preliminary Investigations\"](https://arxiv.org/abs/2212.13345).\n",
    "\n",
    "## Key Differences from Row-by-Row Temporal Model\n",
    "\n",
    "| Aspect | Row-by-Row Temporal | Hinton's Recurrent (this notebook) |\n",
    "|--------|--------------------|---------------------------------|\n",
    "| Input | 28 rows sequentially | Full 784 pixels at once |\n",
    "| Processing | Single pass through 28 timesteps | 8 iterations on same input |\n",
    "| Connections | Feedforward only | **Bidirectional** (top-down + bottom-up) |\n",
    "| Goodness | Sum of squared activities | **Agreement** between layers |\n",
    "| Training | BPTT through time | **Local per layer** (no BPTT) |\n",
    "| Damping | α=0.95 decay | 0.3 old + 0.7 new |\n",
    "\n",
    "## Hinton's Key Insight\n",
    "\n",
    "> \"The activity vector at each layer is determined by the normalized activity vectors \n",
    "> at both the layer above and the layer below at the previous time-step.\"\n",
    "\n",
    "> \"When top-down and bottom-up inputs agree, there will be positive interference \n",
    "> resulting in high squared activities and if they disagree the squared activities will be lower.\"\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Label Layer (top):     10 neurons (one-hot class hypothesis)\n",
    "        ↕ (bidirectional)\n",
    "Hidden Layer 2:        2000 neurons  \n",
    "        ↕ (bidirectional)\n",
    "Hidden Layer 1:        2000 neurons\n",
    "        ↕ (bidirectional)  \n",
    "Input Layer (bottom):  784 pixels\n",
    "```\n",
    "\n",
    "For our constrained version (<26 neurons), we'll use smaller hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"\\nImplementing Hinton's Recurrent Forward-Forward Algorithm\")\n",
    "print(\"Key features:\")\n",
    "print(\"  - Bidirectional connections (top-down + bottom-up)\")\n",
    "print(\"  - Agreement-based goodness\")\n",
    "print(\"  - 8 iterations with damping\")\n",
    "print(\"  - Local layer-wise learning (NO BPTT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist(data_dir='./data/mnist'):\n",
    "    \"\"\"Download MNIST dataset.\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    base_url = 'https://ossci-datasets.s3.amazonaws.com/mnist/'\n",
    "    files = {\n",
    "        'train_images': 'train-images-idx3-ubyte.gz',\n",
    "        'train_labels': 'train-labels-idx1-ubyte.gz',\n",
    "        'test_images': 't10k-images-idx3-ubyte.gz',\n",
    "        'test_labels': 't10k-labels-idx1-ubyte.gz',\n",
    "    }\n",
    "    \n",
    "    paths = {}\n",
    "    for key, filename in files.items():\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filename}...\")\n",
    "            urllib.request.urlretrieve(base_url + filename, filepath)\n",
    "        paths[key] = filepath\n",
    "    \n",
    "    return paths\n",
    "\n",
    "\n",
    "def load_mnist_images(filepath):\n",
    "    \"\"\"Load MNIST images - flatten to 784.\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        n_images = int.from_bytes(f.read(4), 'big')\n",
    "        n_rows = int.from_bytes(f.read(4), 'big')\n",
    "        n_cols = int.from_bytes(f.read(4), 'big')\n",
    "        data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return data.reshape(n_images, n_rows * n_cols).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def load_mnist_labels(filepath):\n",
    "    \"\"\"Load MNIST labels.\"\"\"\n",
    "    with gzip.open(filepath, 'rb') as f:\n",
    "        magic = int.from_bytes(f.read(4), 'big')\n",
    "        n_labels = int.from_bytes(f.read(4), 'big')\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "\n",
    "# Download and load\n",
    "paths = download_mnist()\n",
    "X_train_full = torch.from_numpy(load_mnist_images(paths['train_images']))\n",
    "y_train_full = torch.from_numpy(load_mnist_labels(paths['train_labels'])).long()\n",
    "X_test_full = torch.from_numpy(load_mnist_images(paths['test_images']))\n",
    "y_test_full = torch.from_numpy(load_mnist_labels(paths['test_labels'])).long()\n",
    "\n",
    "print(f\"Full dataset: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n",
    "\n",
    "# Use subset\n",
    "N_TRAIN = 20000\n",
    "N_TEST = 2000\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_idx = torch.randperm(len(X_train_full))[:N_TRAIN]\n",
    "test_idx = torch.randperm(len(X_test_full))[:N_TEST]\n",
    "\n",
    "X_train = X_train_full[train_idx]\n",
    "y_train = y_train_full[train_idx]\n",
    "X_test = X_test_full[test_idx]\n",
    "y_test = y_test_full[test_idx]\n",
    "\n",
    "print(f\"\\nUsing subset:\")\n",
    "print(f\"  Training: {X_train.shape}\")\n",
    "print(f\"  Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hinton's Recurrent Forward-Forward Layer\n",
    "\n",
    "Each layer receives input from BOTH the layer below AND the layer above.\n",
    "Goodness = agreement (interference) between these two signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentFFLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single layer in Hinton's Recurrent Forward-Forward network.\n",
    "    \n",
    "    Key features:\n",
    "    - Receives input from layer below (bottom-up)\n",
    "    - Receives input from layer above (top-down)  \n",
    "    - Goodness = sum of squared activities (agreement creates high activities)\n",
    "    - Layer normalization (without mean subtraction)\n",
    "    - Local learning: each layer has its own optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_below, dim_self, dim_above=None, is_top=False, is_bottom=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim_below = dim_below\n",
    "        self.dim_self = dim_self\n",
    "        self.dim_above = dim_above\n",
    "        self.is_top = is_top\n",
    "        self.is_bottom = is_bottom\n",
    "        \n",
    "        # Bottom-up weights (from layer below)\n",
    "        if not is_bottom:\n",
    "            self.W_bottom_up = nn.Linear(dim_below, dim_self, bias=True)\n",
    "        \n",
    "        # Top-down weights (from layer above)\n",
    "        if not is_top and dim_above is not None:\n",
    "            self.W_top_down = nn.Linear(dim_above, dim_self, bias=False)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        if hasattr(self, 'W_bottom_up'):\n",
    "            nn.init.xavier_uniform_(self.W_bottom_up.weight)\n",
    "            nn.init.zeros_(self.W_bottom_up.bias)\n",
    "        if hasattr(self, 'W_top_down'):\n",
    "            nn.init.xavier_uniform_(self.W_top_down.weight)\n",
    "    \n",
    "    def layer_norm(self, x, eps=1e-8):\n",
    "        \"\"\"\n",
    "        Hinton's layer normalization: divide by length WITHOUT subtracting mean.\n",
    "        This removes magnitude info, forcing deeper layers to use relative activations.\n",
    "        \"\"\"\n",
    "        # Normalize to unit length (L2 norm)\n",
    "        norm = torch.norm(x, dim=1, keepdim=True) + eps\n",
    "        return x / norm\n",
    "    \n",
    "    def forward(self, h_below_norm, h_above_norm=None):\n",
    "        \"\"\"\n",
    "        Compute new state from normalized inputs from above and below.\n",
    "        \n",
    "        Args:\n",
    "            h_below_norm: Normalized activations from layer below [B, dim_below]\n",
    "            h_above_norm: Normalized activations from layer above [B, dim_above] (optional)\n",
    "        \n",
    "        Returns:\n",
    "            pre_norm: Pre-normalized activations [B, dim_self]\n",
    "            h_norm: Normalized activations [B, dim_self]\n",
    "        \"\"\"\n",
    "        # Bottom-up contribution\n",
    "        if hasattr(self, 'W_bottom_up'):\n",
    "            bottom_up = self.W_bottom_up(h_below_norm)\n",
    "        else:\n",
    "            bottom_up = h_below_norm  # For bottom layer, input IS the activation\n",
    "        \n",
    "        # Top-down contribution (if we have a layer above)\n",
    "        if h_above_norm is not None and hasattr(self, 'W_top_down'):\n",
    "            top_down = self.W_top_down(h_above_norm)\n",
    "            # Combined: when signals agree, they reinforce (high squared activity)\n",
    "            pre_act = bottom_up + top_down\n",
    "        else:\n",
    "            pre_act = bottom_up\n",
    "        \n",
    "        # ReLU activation\n",
    "        pre_norm = F.relu(pre_act)\n",
    "        \n",
    "        # Normalize\n",
    "        h_norm = self.layer_norm(pre_norm)\n",
    "        \n",
    "        return pre_norm, h_norm\n",
    "    \n",
    "    def compute_goodness(self, pre_norm):\n",
    "        \"\"\"\n",
    "        Goodness = sum of squared activities.\n",
    "        High when top-down and bottom-up agree (constructive interference).\n",
    "        Low when they disagree (destructive interference).\n",
    "        \"\"\"\n",
    "        return (pre_norm ** 2).sum(dim=1)\n",
    "\n",
    "\n",
    "# Test layer\n",
    "test_layer = RecurrentFFLayer(dim_below=784, dim_self=100, dim_above=10)\n",
    "test_input = torch.randn(5, 784)\n",
    "test_above = torch.randn(5, 10)\n",
    "pre_norm, h_norm = test_layer(test_input, test_above)\n",
    "print(f\"Input: {test_input.shape}\")\n",
    "print(f\"Pre-norm output: {pre_norm.shape}\")\n",
    "print(f\"Normalized output: {h_norm.shape}\")\n",
    "print(f\"Output norm: {torch.norm(h_norm, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Recurrent Forward-Forward Network\n",
    "\n",
    "The full network with bidirectional connections and iterative processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentFFNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Hinton's Recurrent Forward-Forward Network.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer (784 pixels)\n",
    "    - Hidden layers (bidirectional connections)\n",
    "    - Label layer (10 classes, one-hot)\n",
    "    \n",
    "    Processing:\n",
    "    1. Initialize hidden layers with single bottom-up pass\n",
    "    2. Run N iterations with damping, each layer receiving from above and below\n",
    "    3. Compute goodness at each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dims=[100], n_classes=10, \n",
    "                 damping=0.3, n_iterations=8):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.n_classes = n_classes\n",
    "        self.damping = damping  # 0.3 = keep 30% of old state\n",
    "        self.n_iterations = n_iterations\n",
    "        self.n_layers = len(hidden_dims)\n",
    "        \n",
    "        # Build layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            # Dimension of layer below\n",
    "            if i == 0:\n",
    "                dim_below = input_dim\n",
    "            else:\n",
    "                dim_below = hidden_dims[i-1]\n",
    "            \n",
    "            # Dimension of layer above\n",
    "            if i == len(hidden_dims) - 1:\n",
    "                dim_above = n_classes  # Top hidden connects to label\n",
    "            else:\n",
    "                dim_above = hidden_dims[i+1]\n",
    "            \n",
    "            layer = RecurrentFFLayer(\n",
    "                dim_below=dim_below,\n",
    "                dim_self=hidden_dim,\n",
    "                dim_above=dim_above,\n",
    "                is_top=(i == len(hidden_dims) - 1),\n",
    "                is_bottom=False\n",
    "            )\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "        print(f\"RecurrentFFNetwork:\")\n",
    "        print(f\"  Input: {input_dim}\")\n",
    "        for i, dim in enumerate(hidden_dims):\n",
    "            print(f\"  Hidden {i+1}: {dim} neurons\")\n",
    "        print(f\"  Label: {n_classes}\")\n",
    "        print(f\"  Iterations: {n_iterations}\")\n",
    "        print(f\"  Damping: {damping} (keep {damping*100:.0f}% of old state)\")\n",
    "    \n",
    "    def layer_norm(self, x, eps=1e-8):\n",
    "        \"\"\"Normalize to unit length.\"\"\"\n",
    "        norm = torch.norm(x, dim=1, keepdim=True) + eps\n",
    "        return x / norm\n",
    "    \n",
    "    def initialize_hidden(self, x, label_onehot):\n",
    "        \"\"\"\n",
    "        Initialize hidden layers with a single bottom-up pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images [B, 784]\n",
    "            label_onehot: One-hot labels [B, 10]\n",
    "        \n",
    "        Returns:\n",
    "            List of (pre_norm, h_norm) for each hidden layer\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        \n",
    "        # Normalize input\n",
    "        h_below = self.layer_norm(x)\n",
    "        \n",
    "        # Forward pass through all hidden layers (no top-down yet)\n",
    "        for layer in self.layers:\n",
    "            pre_norm, h_norm = layer(h_below, h_above_norm=None)\n",
    "            states.append((pre_norm, h_norm))\n",
    "            h_below = h_norm\n",
    "        \n",
    "        return states\n",
    "    \n",
    "    def run_iteration(self, x_norm, label_norm, states):\n",
    "        \"\"\"\n",
    "        Run one iteration of the recurrent network.\n",
    "        \n",
    "        Each layer receives:\n",
    "        - Normalized activations from layer below\n",
    "        - Normalized activations from layer above\n",
    "        \n",
    "        Args:\n",
    "            x_norm: Normalized input [B, 784]\n",
    "            label_norm: Normalized label [B, 10]\n",
    "            states: List of (pre_norm, h_norm) for each hidden layer\n",
    "        \n",
    "        Returns:\n",
    "            new_states: Updated states with damping applied\n",
    "        \"\"\"\n",
    "        new_states = []\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Get input from below\n",
    "            if i == 0:\n",
    "                h_below_norm = x_norm\n",
    "            else:\n",
    "                _, h_below_norm = states[i-1]\n",
    "            \n",
    "            # Get input from above\n",
    "            if i == len(self.layers) - 1:\n",
    "                # Top hidden layer receives from label\n",
    "                h_above_norm = label_norm\n",
    "            else:\n",
    "                _, h_above_norm = states[i+1]\n",
    "            \n",
    "            # Compute new state\n",
    "            pre_norm_new, h_norm_new = layer(h_below_norm, h_above_norm)\n",
    "            \n",
    "            # Apply damping: new = damping * old + (1-damping) * computed\n",
    "            pre_norm_old, _ = states[i]\n",
    "            pre_norm_damped = self.damping * pre_norm_old + (1 - self.damping) * pre_norm_new\n",
    "            h_norm_damped = self.layer_norm(F.relu(pre_norm_damped))\n",
    "            \n",
    "            new_states.append((pre_norm_damped, h_norm_damped))\n",
    "        \n",
    "        return new_states\n",
    "    \n",
    "    def forward(self, x, label_onehot, return_all_iterations=False):\n",
    "        \"\"\"\n",
    "        Full forward pass with initialization + N iterations.\n",
    "        \n",
    "        Args:\n",
    "            x: Input images [B, 784]\n",
    "            label_onehot: One-hot label hypothesis [B, 10]\n",
    "            return_all_iterations: If True, return states at all iterations\n",
    "        \n",
    "        Returns:\n",
    "            goodness_per_layer: [n_layers] average goodness per layer\n",
    "            all_states: (optional) states at each iteration\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Normalize inputs\n",
    "        x_norm = self.layer_norm(x)\n",
    "        label_norm = self.layer_norm(label_onehot)\n",
    "        \n",
    "        # Initialize with bottom-up pass\n",
    "        states = self.initialize_hidden(x, label_onehot)\n",
    "        \n",
    "        all_iterations = [states] if return_all_iterations else None\n",
    "        goodness_history = []\n",
    "        \n",
    "        # Run iterations\n",
    "        for iter_idx in range(self.n_iterations):\n",
    "            states = self.run_iteration(x_norm, label_norm, states)\n",
    "            \n",
    "            if return_all_iterations:\n",
    "                all_iterations.append(states)\n",
    "            \n",
    "            # Compute goodness at this iteration\n",
    "            iter_goodness = []\n",
    "            for layer_idx, layer in enumerate(self.layers):\n",
    "                pre_norm, _ = states[layer_idx]\n",
    "                g = layer.compute_goodness(pre_norm)\n",
    "                iter_goodness.append(g)\n",
    "            goodness_history.append(iter_goodness)\n",
    "        \n",
    "        # Average goodness over iterations 3-5 (as Hinton suggests)\n",
    "        # Or all iterations if fewer than 5\n",
    "        start_iter = min(2, len(goodness_history) - 1)  # iter 3 = index 2\n",
    "        end_iter = min(5, len(goodness_history))  # iter 5 = index 4, exclusive\n",
    "        \n",
    "        goodness_per_layer = []\n",
    "        for layer_idx in range(len(self.layers)):\n",
    "            layer_goodness = torch.stack([goodness_history[i][layer_idx] \n",
    "                                          for i in range(start_iter, end_iter)])\n",
    "            goodness_per_layer.append(layer_goodness.mean(dim=0))\n",
    "        \n",
    "        if return_all_iterations:\n",
    "            return goodness_per_layer, all_iterations\n",
    "        return goodness_per_layer\n",
    "\n",
    "\n",
    "# Test network\n",
    "N_CLASSES = 10\n",
    "HIDDEN_DIMS = [24]  # Small for <26 constraint\n",
    "\n",
    "test_net = RecurrentFFNetwork(\n",
    "    input_dim=784,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    n_classes=N_CLASSES,\n",
    "    damping=0.3,\n",
    "    n_iterations=8\n",
    ")\n",
    "\n",
    "# Test forward\n",
    "test_x = torch.randn(5, 784)\n",
    "test_label = F.one_hot(torch.tensor([0, 1, 2, 3, 4]), N_CLASSES).float()\n",
    "goodness = test_net(test_x, test_label)\n",
    "print(f\"\\nTest forward pass:\")\n",
    "print(f\"  Input: {test_x.shape}\")\n",
    "print(f\"  Label: {test_label.shape}\")\n",
    "print(f\"  Goodness shape: {[g.shape for g in goodness]}\")\n",
    "print(f\"  Total goodness: {sum(g.sum() for g in goodness).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Forward-Forward Loss and Training\n",
    "\n",
    "Key: Learning is **LOCAL** to each layer. No gradients flow between layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ff_loss(goodness_pos, goodness_neg, threshold=0.0):\n",
    "    \"\"\"\n",
    "    Forward-Forward loss for a single layer.\n",
    "    \n",
    "    Hinton's formulation:\n",
    "    - Positive: sigmoid(goodness - threshold) should be high\n",
    "    - Negative: sigmoid(goodness - threshold) should be low\n",
    "    \"\"\"\n",
    "    # Positive samples: maximize goodness (minimize -log(sigmoid(g - threshold)))\n",
    "    loss_pos = F.softplus(threshold - goodness_pos).mean()\n",
    "    \n",
    "    # Negative samples: minimize goodness (minimize -log(1 - sigmoid(g - threshold)))\n",
    "    loss_neg = F.softplus(goodness_neg - threshold).mean()\n",
    "    \n",
    "    return loss_pos + loss_neg\n",
    "\n",
    "\n",
    "def create_positive_negative(X, y, n_classes=10):\n",
    "    \"\"\"\n",
    "    Create positive and negative samples.\n",
    "    \n",
    "    Positive: image + correct label\n",
    "    Negative: image + random wrong label (sampled proportionally)\n",
    "    \"\"\"\n",
    "    B = X.shape[0]\n",
    "    \n",
    "    # Positive labels (one-hot)\n",
    "    y_pos = F.one_hot(y, n_classes).float()\n",
    "    \n",
    "    # Negative labels (random wrong class)\n",
    "    y_neg_idx = (y + torch.randint(1, n_classes, (B,))) % n_classes\n",
    "    y_neg = F.one_hot(y_neg_idx, n_classes).float()\n",
    "    \n",
    "    return y_pos, y_neg\n",
    "\n",
    "\n",
    "def train_recurrent_ff(model, X_train, y_train, X_test, y_test,\n",
    "                       n_epochs=60, lr=0.001, batch_size=64,\n",
    "                       threshold=2.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Train the Recurrent Forward-Forward network.\n",
    "    \n",
    "    KEY: Each layer has its OWN optimizer and learns LOCALLY.\n",
    "    No gradients flow between layers - this is the core of FF.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Create separate optimizer for each layer (local learning)\n",
    "    layer_optimizers = []\n",
    "    for layer in model.layers:\n",
    "        opt = torch.optim.Adam(layer.parameters(), lr=lr)\n",
    "        layer_optimizers.append(opt)\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'goodness_pos': [],\n",
    "        'goodness_neg': [],\n",
    "    }\n",
    "    \n",
    "    N = X_train.shape[0]\n",
    "    n_batches = (N + batch_size - 1) // batch_size\n",
    "    \n",
    "    best_test_acc = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_g_pos = []\n",
    "        epoch_g_neg = []\n",
    "        \n",
    "        # Shuffle\n",
    "        perm = torch.randperm(N)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "            \n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Create positive and negative labels\n",
    "            y_pos, y_neg = create_positive_negative(X_batch, y_batch)\n",
    "            \n",
    "            # Forward pass for positive and negative\n",
    "            goodness_pos = model(X_batch, y_pos)\n",
    "            goodness_neg = model(X_batch, y_neg)\n",
    "            \n",
    "            # Train each layer LOCALLY\n",
    "            batch_loss = 0\n",
    "            for layer_idx, (g_pos, g_neg, opt) in enumerate(zip(\n",
    "                    goodness_pos, goodness_neg, layer_optimizers)):\n",
    "                \n",
    "                # Zero gradients for THIS layer only\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                # Compute loss for this layer\n",
    "                # CRITICAL: .detach() ensures no gradients flow between layers\n",
    "                layer_loss = ff_loss(g_pos, g_neg.detach(), threshold)\n",
    "                \n",
    "                # Backward for this layer only\n",
    "                layer_loss.backward(retain_graph=True)\n",
    "                \n",
    "                # Update this layer's weights\n",
    "                opt.step()\n",
    "                \n",
    "                batch_loss += layer_loss.item()\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "            epoch_g_pos.append(sum(g.mean().item() for g in goodness_pos))\n",
    "            epoch_g_neg.append(sum(g.mean().item() for g in goodness_neg))\n",
    "            \n",
    "            if verbose and batch_idx % 100 == 0:\n",
    "                print(f\"\\rEpoch {epoch+1}/{n_epochs} | Batch {batch_idx+1}/{n_batches} | \"\n",
    "                      f\"Loss: {batch_loss:.4f}\", end=\"\")\n",
    "        \n",
    "        # Evaluate\n",
    "        train_acc = evaluate_recurrent_ff(model, X_train[:2000], y_train[:2000])\n",
    "        test_acc = evaluate_recurrent_ff(model, X_test, y_test)\n",
    "        \n",
    "        if test_acc > best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "        \n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['goodness_pos'].append(np.mean(epoch_g_pos))\n",
    "        history['goodness_neg'].append(np.mean(epoch_g_neg))\n",
    "        \n",
    "        if verbose:\n",
    "            sep = np.mean(epoch_g_pos) - np.mean(epoch_g_neg)\n",
    "            print(f\"\\rEpoch {epoch+1}/{n_epochs} | Loss: {epoch_loss/n_batches:.4f} | \"\n",
    "                  f\"Train: {train_acc:.4f} | Test: {test_acc:.4f} | \"\n",
    "                  f\"Best: {best_test_acc:.4f} | Sep: {sep:.2f}    \")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_recurrent_ff(model, X, y, batch_size=100):\n",
    "    \"\"\"\n",
    "    Evaluate by testing all 10 label hypotheses and picking highest goodness.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X.shape[0]\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(start + batch_size, N)\n",
    "            X_batch = X[start:end]\n",
    "            B = X_batch.shape[0]\n",
    "            \n",
    "            # Test all 10 hypotheses\n",
    "            all_goodness = []\n",
    "            for digit in range(10):\n",
    "                label_hyp = F.one_hot(torch.full((B,), digit, dtype=torch.long), 10).float()\n",
    "                goodness = model(X_batch, label_hyp)\n",
    "                total_goodness = sum(g for g in goodness)  # Sum across layers\n",
    "                all_goodness.append(total_goodness)\n",
    "            \n",
    "            # Stack: [10, B] -> [B, 10]\n",
    "            goodness_matrix = torch.stack(all_goodness, dim=1)\n",
    "            preds = goodness_matrix.argmax(dim=1)\n",
    "            all_preds.append(preds)\n",
    "    \n",
    "    all_preds = torch.cat(all_preds)\n",
    "    accuracy = (all_preds == y).float().mean().item()\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "print(\"Training functions defined.\")\n",
    "print(\"\\nKey difference from standard FF:\")\n",
    "print(\"  - Each layer has its OWN optimizer\")\n",
    "print(\"  - Gradients do NOT flow between layers\")\n",
    "print(\"  - This is TRUE local learning (no BPTT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build network\n",
    "HIDDEN_DIMS = [24]  # Constrained to <26 neurons\n",
    "N_ITERATIONS = 8\n",
    "DAMPING = 0.3\n",
    "THRESHOLD = 2.0\n",
    "N_EPOCHS = 60\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"HINTON'S RECURRENT FORWARD-FORWARD\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Architecture: 784 → {HIDDEN_DIMS} → 10\")\n",
    "print(f\"Iterations: {N_ITERATIONS}\")\n",
    "print(f\"Damping: {DAMPING} (keep {DAMPING*100:.0f}% old state)\")\n",
    "print(f\"Threshold: {THRESHOLD}\")\n",
    "print(f\"Learning rate: {LR}\")\n",
    "print(f\"\\nKEY FEATURES:\")\n",
    "print(f\"  ✓ Bidirectional connections (top-down + bottom-up)\")\n",
    "print(f\"  ✓ Agreement-based goodness\")\n",
    "print(f\"  ✓ Local layer-wise learning (NO BPTT)\")\n",
    "print(f\"  ✓ Multiple iterations with damping\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = RecurrentFFNetwork(\n",
    "    input_dim=784,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    n_classes=10,\n",
    "    damping=DAMPING,\n",
    "    n_iterations=N_ITERATIONS\n",
    ")\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {n_params}\")\n",
    "\n",
    "history = train_recurrent_ff(\n",
    "    model, X_train, y_train, X_test, y_test,\n",
    "    n_epochs=N_EPOCHS, lr=LR, batch_size=BATCH_SIZE,\n",
    "    threshold=THRESHOLD, verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(f\"Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final test accuracy: {history['test_acc'][-1]:.4f}\")\n",
    "print(f\"Best test accuracy: {max(history['test_acc']):.4f}\")\n",
    "print(f\"Random baseline: 10%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['loss'], color='steelblue', lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['train_acc'], label='Train', color='coral', lw=2)\n",
    "ax2.plot(history['test_acc'], label='Test', color='steelblue', lw=2)\n",
    "ax2.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Classification Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0, 1.0)\n",
    "\n",
    "# Goodness\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['goodness_pos'], label='Positive', color='green', lw=2)\n",
    "ax3.plot(history['goodness_neg'], label='Negative', color='red', lw=2)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Goodness')\n",
    "ax3.set_title('Goodness Values')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Separation\n",
    "ax4 = axes[1, 1]\n",
    "separation = [p - n for p, n in zip(history['goodness_pos'], history['goodness_neg'])]\n",
    "ax4.plot(separation, color='purple', lw=2)\n",
    "ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('G+ - G-')\n",
    "ax4.set_title('Goodness Separation')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f\"Hinton's Recurrent Forward-Forward ({sum(HIDDEN_DIMS)} neurons)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Iteration Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how goodness evolves over iterations\n",
    "model.eval()\n",
    "\n",
    "# Pick a sample\n",
    "sample_idx = 0\n",
    "X_sample = X_test[sample_idx:sample_idx+1]\n",
    "y_sample = y_test[sample_idx].item()\n",
    "\n",
    "# Test all hypotheses and track iterations\n",
    "all_goodness_evolution = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for digit in range(10):\n",
    "        label_hyp = F.one_hot(torch.tensor([digit]), 10).float()\n",
    "        \n",
    "        # Get goodness at each iteration\n",
    "        x_norm = model.layer_norm(X_sample)\n",
    "        label_norm = model.layer_norm(label_hyp)\n",
    "        states = model.initialize_hidden(X_sample, label_hyp)\n",
    "        \n",
    "        iter_goodness = []\n",
    "        for i in range(model.n_iterations):\n",
    "            states = model.run_iteration(x_norm, label_norm, states)\n",
    "            total_g = sum(model.layers[j].compute_goodness(states[j][0]).item() \n",
    "                         for j in range(len(model.layers)))\n",
    "            iter_goodness.append(total_g)\n",
    "        \n",
    "        all_goodness_evolution.append(iter_goodness)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: Image\n",
    "ax1 = axes[0]\n",
    "ax1.imshow(X_sample.reshape(28, 28).numpy(), cmap='gray')\n",
    "ax1.set_title(f'Test Sample (True Label: {y_sample})')\n",
    "ax1.axis('off')\n",
    "\n",
    "# Right: Goodness evolution\n",
    "ax2 = axes[1]\n",
    "for digit in range(10):\n",
    "    color = 'green' if digit == y_sample else 'lightgray'\n",
    "    lw = 3 if digit == y_sample else 1\n",
    "    ax2.plot(range(1, N_ITERATIONS+1), all_goodness_evolution[digit], \n",
    "             label=f'Digit {digit}', color=color, lw=lw, alpha=0.8)\n",
    "\n",
    "ax2.axvline(x=3, color='blue', linestyle=':', alpha=0.5, label='Eval start (iter 3)')\n",
    "ax2.axvline(x=5, color='blue', linestyle=':', alpha=0.5, label='Eval end (iter 5)')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Total Goodness')\n",
    "ax2.set_title('Goodness Evolution Over Iterations')\n",
    "ax2.legend(loc='upper right', fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"Recurrent FF: Correct label should have highest goodness\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compare Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hidden sizes\n",
    "hidden_configs = [\n",
    "    [12],\n",
    "    [16],\n",
    "    [20],\n",
    "    [24],\n",
    "    [12, 12],  # Two layers\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "print(\"Comparing Recurrent FF architectures...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for hidden_dims in hidden_configs:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = RecurrentFFNetwork(\n",
    "        input_dim=784,\n",
    "        hidden_dims=hidden_dims,\n",
    "        n_classes=10,\n",
    "        damping=0.3,\n",
    "        n_iterations=8\n",
    "    )\n",
    "    \n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total_neurons = sum(hidden_dims)\n",
    "    \n",
    "    history = train_recurrent_ff(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=30, lr=0.001, batch_size=64,\n",
    "        threshold=2.0, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_test = max(history['test_acc'])\n",
    "    comparison_results.append({\n",
    "        'hidden_dims': str(hidden_dims),\n",
    "        'total_neurons': total_neurons,\n",
    "        'n_params': n_params,\n",
    "        'test_acc': history['test_acc'][-1],\n",
    "        'best_test': best_test,\n",
    "    })\n",
    "    \n",
    "    print(f\"Hidden={str(hidden_dims):12s} | Neurons={total_neurons:2d} | \"\n",
    "          f\"Params={n_params:6d} | Test: {history['test_acc'][-1]:.4f} | Best: {best_test:.4f}\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "best_result = max(comparison_results, key=lambda x: x['best_test'])\n",
    "print(f\"\\nBest: {best_result['hidden_dims']} with {best_result['best_test']:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, X, batch_size=100):\n",
    "    model.eval()\n",
    "    N = X.shape[0]\n",
    "    all_preds = []\n",
    "    all_goodness = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for start in range(0, N, batch_size):\n",
    "            end = min(start + batch_size, N)\n",
    "            X_batch = X[start:end]\n",
    "            B = X_batch.shape[0]\n",
    "            \n",
    "            batch_goodness = []\n",
    "            for digit in range(10):\n",
    "                label_hyp = F.one_hot(torch.full((B,), digit, dtype=torch.long), 10).float()\n",
    "                goodness = model(X_batch, label_hyp)\n",
    "                total_goodness = sum(g for g in goodness)\n",
    "                batch_goodness.append(total_goodness)\n",
    "            \n",
    "            goodness_matrix = torch.stack(batch_goodness, dim=1)\n",
    "            all_goodness.append(goodness_matrix)\n",
    "            all_preds.append(goodness_matrix.argmax(dim=1))\n",
    "    \n",
    "    return torch.cat(all_preds), torch.cat(all_goodness)\n",
    "\n",
    "\n",
    "# Get predictions\n",
    "test_preds, test_goodness = get_predictions(model, X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = np.zeros((10, 10), dtype=np.int32)\n",
    "for true, pred in zip(y_test.numpy(), test_preds.numpy()):\n",
    "    cm[true, pred] += 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title(f\"Confusion Matrix (Recurrent FF, Test Acc: {history['test_acc'][-1]:.2%})\")\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        color = 'white' if cm[i, j] > cm.max()/2 else 'black'\n",
    "        ax.text(j, i, cm[i, j], ha='center', va='center', color=color)\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for digit in range(10):\n",
    "    mask = y_test == digit\n",
    "    if mask.sum() > 0:\n",
    "        acc = (test_preds[mask] == digit).float().mean().item()\n",
    "        print(f\"  Digit {digit}: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CONCLUSIONS: HINTON'S RECURRENT FORWARD-FORWARD\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\n1. ARCHITECTURE:\")\n",
    "print(f\"   Input: 784 (full image)\")\n",
    "print(f\"   Hidden: {HIDDEN_DIMS}\")\n",
    "print(f\"   Label: 10 (one-hot hypothesis)\")\n",
    "print(f\"   Iterations: {N_ITERATIONS}\")\n",
    "print(f\"   Damping: {DAMPING}\")\n",
    "\n",
    "print(f\"\\n2. KEY DIFFERENCES FROM ROW-BY-ROW TEMPORAL:\")\n",
    "print(f\"   ✓ Full image input (not row-by-row)\")\n",
    "print(f\"   ✓ Bidirectional connections (top-down + bottom-up)\")\n",
    "print(f\"   ✓ Agreement-based goodness (interference pattern)\")\n",
    "print(f\"   ✓ Multiple iterations (8) vs single pass\")\n",
    "print(f\"   ✓ TRUE local learning (no BPTT at all)\")\n",
    "\n",
    "print(f\"\\n3. PERFORMANCE:\")\n",
    "print(f\"   Final test accuracy: {history['test_acc'][-1]:.2%}\")\n",
    "print(f\"   Best test accuracy: {max(history['test_acc']):.2%}\")\n",
    "print(f\"   Random baseline: 10%\")\n",
    "\n",
    "print(f\"\\n4. WHY THIS APPROACH IS PRINCIPLED:\")\n",
    "print(f\"   - Follows Hinton's original paper exactly\")\n",
    "print(f\"   - No vanishing gradients (no BPTT)\")\n",
    "print(f\"   - Hardware-compatible (local learning rules)\")\n",
    "print(f\"   - Biologically plausible (cortical feedback)\")\n",
    "\n",
    "print(f\"\\n5. HINTON'S RESULTS (for reference):\")\n",
    "print(f\"   - 2000 neurons per hidden layer\")\n",
    "print(f\"   - 8 iterations with 0.3/0.7 damping\")\n",
    "print(f\"   - 1.31% test error (98.69% accuracy)\")\n",
    "print(f\"   - Our {sum(HIDDEN_DIMS)} neurons: {max(history['test_acc']):.2%}\")\n",
    "\n",
    "print(f\"\\n6. NEXT STEPS:\")\n",
    "print(f\"   - Try larger hidden layers (2000 as in paper)\")\n",
    "print(f\"   - Tune threshold and learning rate\")\n",
    "print(f\"   - Add more hidden layers\")\n",
    "print(f\"   - Compare with SOEN temporal model\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
