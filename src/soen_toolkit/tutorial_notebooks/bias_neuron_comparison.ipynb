{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bias Neuron Comparison: Can SOEN Learn Linear Regression?\n",
    "\n",
    "This notebook compares two SOEN model architectures on a linear regression task:\n",
    "\n",
    "## Model A: No Bias Input (Original)\n",
    "```\n",
    "x ──[J_in]──→ SingleDendrite ──[J_out]──→ y\n",
    "```\n",
    "- Trainable: J_in, J_out (2 params)\n",
    "- φ = J_in · x + φ_offset (fixed)\n",
    "\n",
    "## Model B: With Bias Input (New)\n",
    "```\n",
    "x ──[J_α]──┐\n",
    "            ├──→ SingleDendrite ──→ y  (J_out fixed to 1)\n",
    "1 ──[J_β]──┘\n",
    "```\n",
    "- Trainable: J_α, J_β (2 params)\n",
    "- φ = J_α · x + J_β · 1 + φ_offset\n",
    "- J_β acts as learnable bias!\n",
    "\n",
    "## Task\n",
    "Learn y = 2.0·x + 0.5 (linear regression with slope and intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Generate Linear Regression Data\n",
    "\n",
    "Target: y = α·x + β where α=2.0, β=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth parameters\n",
    "TRUE_ALPHA = 2.0  # slope\n",
    "TRUE_BETA = 0.5   # intercept\n",
    "\n",
    "# Data generation\n",
    "N_SAMPLES = 100\n",
    "SEQ_LEN = 50\n",
    "\n",
    "# Input values (within SOEN operating range)\n",
    "x_values = torch.linspace(0.05, 0.20, N_SAMPLES)\n",
    "\n",
    "# Create input sequences (constant over time)\n",
    "X_data = x_values.unsqueeze(1).unsqueeze(2).expand(-1, SEQ_LEN, 1).clone()\n",
    "\n",
    "# Target outputs\n",
    "y_data = TRUE_ALPHA * x_values + TRUE_BETA\n",
    "y_data = y_data.unsqueeze(1)  # [N, 1]\n",
    "\n",
    "print(f\"Task: y = {TRUE_ALPHA}·x + {TRUE_BETA}\")\n",
    "print(f\"Input shape: {X_data.shape}\")\n",
    "print(f\"Target shape: {y_data.shape}\")\n",
    "print(f\"Input range: [{x_values.min():.3f}, {x_values.max():.3f}]\")\n",
    "print(f\"Target range: [{y_data.min():.3f}, {y_data.max():.3f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_values.numpy(), y_data.squeeze().numpy(), alpha=0.6, label='Data')\n",
    "plt.plot(x_values.numpy(), TRUE_ALPHA * x_values.numpy() + TRUE_BETA, 'r-', \n",
    "         linewidth=2, label=f'y = {TRUE_ALPHA}x + {TRUE_BETA}')\n",
    "plt.xlabel('Input x')\n",
    "plt.ylabel('Target y')\n",
    "plt.title('Linear Regression Task')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Define Both Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_A(j_in=0.15, j_out=1.0):\n",
    "    \"\"\"\n",
    "    Model A: Original architecture (no bias input)\n",
    "    \n",
    "    x ──[J_in]──→ SingleDendrite ──[J_out]──→ y\n",
    "    \n",
    "    Trainable: J_in, J_out (2 params)\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=50.0,\n",
    "        input_type=\"state\",\n",
    "        track_phi=True,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    # Input: 1 channel\n",
    "    layer0 = LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    )\n",
    "    \n",
    "    # SingleDendrite: 1 neuron\n",
    "    layer1 = LayerConfig(\n",
    "        layer_id=1,\n",
    "        layer_type=\"SingleDendrite\",\n",
    "        params={\n",
    "            \"dim\": 1,\n",
    "            \"solver\": \"FE\",\n",
    "            \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "            \"phi_offset\": 0.02,\n",
    "            \"bias_current\": 1.98,\n",
    "            \"gamma_plus\": 0.0005,\n",
    "            \"gamma_minus\": 1e-6,\n",
    "            \"learnable_params\": {\n",
    "                \"phi_offset\": False,\n",
    "                \"bias_current\": False,\n",
    "                \"gamma_plus\": False,\n",
    "                \"gamma_minus\": False,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Output: 1 channel\n",
    "    layer2 = LayerConfig(\n",
    "        layer_id=2,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    )\n",
    "    \n",
    "    # Connection 0→1: J_in (learnable)\n",
    "    conn01 = ConnectionConfig(\n",
    "        from_layer=0, to_layer=1,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"constant\", \"value\": j_in},\n",
    "    )\n",
    "    \n",
    "    # Connection 1→2: J_out (learnable)\n",
    "    conn12 = ConnectionConfig(\n",
    "        from_layer=1, to_layer=2,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"constant\", \"value\": j_out},\n",
    "    )\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=[layer0, layer1, layer2],\n",
    "        connections_config=[conn01, conn12],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_B(j_alpha=0.15, j_beta=0.1):\n",
    "    \"\"\"\n",
    "    Model B: With bias input\n",
    "    \n",
    "    x ──[J_α]──┐\n",
    "               ├──→ SingleDendrite ──[1.0]──→ y\n",
    "    1 ──[J_β]──┘\n",
    "    \n",
    "    Trainable: J_α, J_β (2 params)\n",
    "    J_out is fixed to 1.0\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=50.0,\n",
    "        input_type=\"state\",\n",
    "        track_phi=True,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    # Input: 2 channels (x and bias=1)\n",
    "    layer0 = LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 2},  # Channel 0: x, Channel 1: bias (always 1)\n",
    "    )\n",
    "    \n",
    "    # SingleDendrite: 1 neuron\n",
    "    layer1 = LayerConfig(\n",
    "        layer_id=1,\n",
    "        layer_type=\"SingleDendrite\",\n",
    "        params={\n",
    "            \"dim\": 1,\n",
    "            \"solver\": \"FE\",\n",
    "            \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "            \"phi_offset\": 0.02,\n",
    "            \"bias_current\": 1.98,\n",
    "            \"gamma_plus\": 0.0005,\n",
    "            \"gamma_minus\": 1e-6,\n",
    "            \"learnable_params\": {\n",
    "                \"phi_offset\": False,\n",
    "                \"bias_current\": False,\n",
    "                \"gamma_plus\": False,\n",
    "                \"gamma_minus\": False,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Output: 1 channel\n",
    "    layer2 = LayerConfig(\n",
    "        layer_id=2,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    )\n",
    "    \n",
    "    # Connection 0→1: [J_α, J_β] (2x1 matrix, both learnable)\n",
    "    # This creates a 2x1 weight matrix where:\n",
    "    #   - Weight[0] = J_α (multiplies x)\n",
    "    #   - Weight[1] = J_β (multiplies bias=1)\n",
    "    conn01 = ConnectionConfig(\n",
    "        from_layer=0, to_layer=1,\n",
    "        connection_type=\"all_to_all\",  # 2 inputs → 1 neuron = 2 weights\n",
    "        learnable=True,\n",
    "        params={\n",
    "            \"init\": \"constant\", \n",
    "            \"value\": j_alpha,  # Initial value for all weights\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # Connection 1→2: Fixed to 1.0 (not learnable)\n",
    "    conn12 = ConnectionConfig(\n",
    "        from_layer=1, to_layer=2,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=False,  # FIXED!\n",
    "        params={\"init\": \"constant\", \"value\": 1.0},\n",
    "    )\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=[layer0, layer1, layer2],\n",
    "        connections_config=[conn01, conn12],\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Build and inspect both models\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL A: No Bias Input\")\n",
    "print(\"=\" * 60)\n",
    "model_A = build_model_A()\n",
    "print(f\"Layers: {[l.dim for l in model_A.layers]}\")\n",
    "print(\"Trainable parameters:\")\n",
    "for name, p in model_A.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"  {name}: shape={list(p.shape)}, value={p.data.numpy().flatten()}\")\n",
    "print(f\"Total trainable: {sum(p.numel() for p in model_A.parameters() if p.requires_grad)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL B: With Bias Input\")\n",
    "print(\"=\" * 60)\n",
    "model_B = build_model_B()\n",
    "print(f\"Layers: {[l.dim for l in model_B.layers]}\")\n",
    "print(\"Trainable parameters:\")\n",
    "for name, p in model_B.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"  {name}: shape={list(p.shape)}, value={p.data.numpy().flatten()}\")\n",
    "print(f\"Total trainable: {sum(p.numel() for p in model_B.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Prepare Input Data for Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model A input: just x\n",
    "X_model_A = X_data.clone()  # [N, T, 1]\n",
    "\n",
    "# Model B input: [x, 1] (concatenate bias channel)\n",
    "bias_channel = torch.ones(N_SAMPLES, SEQ_LEN, 1)  # Constant 1\n",
    "X_model_B = torch.cat([X_data, bias_channel], dim=2)  # [N, T, 2]\n",
    "\n",
    "print(f\"Model A input shape: {X_model_A.shape}\")\n",
    "print(f\"Model B input shape: {X_model_B.shape}\")\n",
    "print(f\"\\nModel B input sample [0, 0, :]: {X_model_B[0, 0, :].numpy()}\")\n",
    "print(f\"  Channel 0 (x): {X_model_B[0, 0, 0].item():.4f}\")\n",
    "print(f\"  Channel 1 (bias): {X_model_B[0, 0, 1].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, n_epochs=200, lr=0.05, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a SOEN model on regression task.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'loss': [], 'params': []}\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        final_hist, _ = model(X_train)\n",
    "        y_pred = final_hist[:, -1, :]  # Take final timestep\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record\n",
    "        history['loss'].append(loss.item())\n",
    "        params = [p.clone().detach().numpy().flatten() for p in model.parameters() if p.requires_grad]\n",
    "        history['params'].append(params)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"  Epoch {epoch+1}: Loss = {loss.item():.6f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Train Both Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 300\n",
    "LR = 0.05\n",
    "\n",
    "# Train Model A\n",
    "print(\"Training Model A (no bias input)...\")\n",
    "model_A = build_model_A(j_in=0.1, j_out=0.5)\n",
    "history_A = train_model(model_A, X_model_A, y_data, n_epochs=N_EPOCHS, lr=LR)\n",
    "\n",
    "print(\"\\nTraining Model B (with bias input)...\")\n",
    "model_B = build_model_B(j_alpha=0.1, j_beta=0.1)\n",
    "history_B = train_model(model_B, X_model_B, y_data, n_epochs=N_EPOCHS, lr=LR)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model A final loss: {history_A['loss'][-1]:.6f}\")\n",
    "print(f\"Model B final loss: {history_B['loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Compare Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(N_EPOCHS)\n",
    "\n",
    "# Linear scale\n",
    "ax1 = axes[0]\n",
    "ax1.plot(epochs, history_A['loss'], 'b-', linewidth=2, label='Model A (no bias)')\n",
    "ax1.plot(epochs, history_B['loss'], 'r-', linewidth=2, label='Model B (with bias)')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Training Loss Comparison (Linear Scale)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs, history_A['loss'], 'b-', linewidth=2, label='Model A (no bias)')\n",
    "ax2.plot(epochs, history_B['loss'], 'r-', linewidth=2, label='Model B (with bias)')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MSE Loss (log scale)')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title('Training Loss Comparison (Log Scale)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Report improvement\n",
    "improvement = (history_A['loss'][-1] - history_B['loss'][-1]) / history_A['loss'][-1] * 100\n",
    "print(f\"\\nModel B achieves {improvement:.1f}% lower loss than Model A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Compare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from both models\n",
    "model_A.eval()\n",
    "model_B.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_A = model_A(X_model_A)[0][:, -1, :].squeeze().numpy()\n",
    "    pred_B = model_B(X_model_B)[0][:, -1, :].squeeze().numpy()\n",
    "\n",
    "x_plot = x_values.numpy()\n",
    "y_true = y_data.squeeze().numpy()\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Model A predictions\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(x_plot, y_true, alpha=0.5, label='True', color='gray')\n",
    "ax1.scatter(x_plot, pred_A, alpha=0.7, label='Model A prediction', color='blue')\n",
    "ax1.plot(x_plot, TRUE_ALPHA * x_plot + TRUE_BETA, 'k--', linewidth=2, label=f'y = {TRUE_ALPHA}x + {TRUE_BETA}')\n",
    "ax1.set_xlabel('Input x')\n",
    "ax1.set_ylabel('Output y')\n",
    "ax1.set_title(f'Model A: No Bias\\nFinal Loss = {history_A[\"loss\"][-1]:.6f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Model B predictions\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(x_plot, y_true, alpha=0.5, label='True', color='gray')\n",
    "ax2.scatter(x_plot, pred_B, alpha=0.7, label='Model B prediction', color='red')\n",
    "ax2.plot(x_plot, TRUE_ALPHA * x_plot + TRUE_BETA, 'k--', linewidth=2, label=f'y = {TRUE_ALPHA}x + {TRUE_BETA}')\n",
    "ax2.set_xlabel('Input x')\n",
    "ax2.set_ylabel('Output y')\n",
    "ax2.set_title(f'Model B: With Bias\\nFinal Loss = {history_B[\"loss\"][-1]:.6f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Side-by-side comparison\n",
    "ax3 = axes[2]\n",
    "ax3.plot(x_plot, y_true, 'k-', linewidth=3, label='True target', alpha=0.8)\n",
    "ax3.plot(x_plot, pred_A, 'b--', linewidth=2, label='Model A (no bias)')\n",
    "ax3.plot(x_plot, pred_B, 'r--', linewidth=2, label='Model B (with bias)')\n",
    "ax3.set_xlabel('Input x')\n",
    "ax3.set_ylabel('Output y')\n",
    "ax3.set_title('Direct Comparison')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Analyze the Learned Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"LEARNED PARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nModel A (no bias):\")\n",
    "params_A = list(model_A.parameters())\n",
    "j_in_A = params_A[0].item()\n",
    "j_out_A = params_A[1].item()\n",
    "print(f\"  J_in = {j_in_A:.4f}\")\n",
    "print(f\"  J_out = {j_out_A:.4f}\")\n",
    "print(f\"  Signal flow: φ = {j_in_A:.4f}·x + 0.02 → s → y = {j_out_A:.4f}·s\")\n",
    "\n",
    "print(\"\\nModel B (with bias):\")\n",
    "params_B = list(model_B.parameters())\n",
    "j_weights = params_B[0].data.numpy().flatten()\n",
    "j_alpha = j_weights[0]\n",
    "j_beta = j_weights[1] if len(j_weights) > 1 else 0\n",
    "print(f\"  J_α (x weight) = {j_alpha:.4f}\")\n",
    "print(f\"  J_β (bias weight) = {j_beta:.4f}\")\n",
    "print(f\"  Signal flow: φ = {j_alpha:.4f}·x + {j_beta:.4f}·1 + 0.02 → s → y = s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TARGET RELATIONSHIP\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  y = {TRUE_ALPHA}·x + {TRUE_BETA}\")\n",
    "print(f\"\\nNote: The SOEN model has nonlinear dynamics, so it cannot\")\n",
    "print(f\"perfectly represent a linear function. The bias input helps\")\n",
    "print(f\"by providing an independent offset term.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Visualize the SingleDendrite Transfer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the nonlinear transfer function of SingleDendrite\n",
    "# by varying input and measuring steady-state output\n",
    "\n",
    "def measure_transfer_function(model, x_range, seq_len=100, is_model_B=False):\n",
    "    \"\"\"Measure the input-output relationship of a trained model.\"\"\"\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    \n",
    "    for x_val in x_range:\n",
    "        if is_model_B:\n",
    "            # Model B: [x, 1]\n",
    "            x_input = torch.tensor([[[x_val, 1.0]]]).expand(-1, seq_len, -1)\n",
    "        else:\n",
    "            # Model A: [x]\n",
    "            x_input = torch.tensor([[[x_val]]]).expand(-1, seq_len, -1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            final_hist, _ = model(x_input)\n",
    "            y_out = final_hist[0, -1, 0].item()\n",
    "        outputs.append(y_out)\n",
    "    \n",
    "    return np.array(outputs)\n",
    "\n",
    "# Extended input range to see full transfer function\n",
    "x_extended = np.linspace(0.0, 0.3, 100)\n",
    "\n",
    "y_transfer_A = measure_transfer_function(model_A, x_extended, is_model_B=False)\n",
    "y_transfer_B = measure_transfer_function(model_B, x_extended, is_model_B=True)\n",
    "y_linear = TRUE_ALPHA * x_extended + TRUE_BETA\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_extended, y_linear, 'k-', linewidth=3, label=f'Target: y = {TRUE_ALPHA}x + {TRUE_BETA}', alpha=0.7)\n",
    "plt.plot(x_extended, y_transfer_A, 'b-', linewidth=2, label='Model A (no bias)')\n",
    "plt.plot(x_extended, y_transfer_B, 'r-', linewidth=2, label='Model B (with bias)')\n",
    "\n",
    "# Mark training region\n",
    "plt.axvspan(0.05, 0.20, alpha=0.2, color='green', label='Training region')\n",
    "\n",
    "plt.xlabel('Input x', fontsize=12)\n",
    "plt.ylabel('Output y', fontsize=12)\n",
    "plt.title('Transfer Functions: SOEN Models vs Linear Target', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation:\")\n",
    "print(\"  The SingleDendrite introduces nonlinearity, so neither model can\")\n",
    "print(\"  perfectly match the linear target. However, Model B (with bias)\")\n",
    "print(\"  has more flexibility to approximate the offset term.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute residuals\n",
    "residuals_A = y_true - pred_A\n",
    "residuals_B = y_true - pred_B\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Residuals vs x\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(x_plot, residuals_A, alpha=0.7, label=f'Model A (std={residuals_A.std():.4f})', color='blue')\n",
    "ax1.scatter(x_plot, residuals_B, alpha=0.7, label=f'Model B (std={residuals_B.std():.4f})', color='red')\n",
    "ax1.axhline(y=0, color='black', linestyle='--', linewidth=1)\n",
    "ax1.set_xlabel('Input x')\n",
    "ax1.set_ylabel('Residual (True - Predicted)')\n",
    "ax1.set_title('Residuals vs Input')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residual histogram\n",
    "ax2 = axes[1]\n",
    "ax2.hist(residuals_A, bins=20, alpha=0.6, label=f'Model A (mean={residuals_A.mean():.4f})', color='blue')\n",
    "ax2.hist(residuals_B, bins=20, alpha=0.6, label=f'Model B (mean={residuals_B.mean():.4f})', color='red')\n",
    "ax2.axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "ax2.set_xlabel('Residual')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.set_title('Residual Distribution')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResidual Statistics:\")\n",
    "print(f\"  Model A: mean={residuals_A.mean():.4f}, std={residuals_A.std():.4f}, MSE={np.mean(residuals_A**2):.6f}\")\n",
    "print(f\"  Model B: mean={residuals_B.mean():.4f}, std={residuals_B.std():.4f}, MSE={np.mean(residuals_B**2):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: BIAS NEURON COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nTask: Learn y = {TRUE_ALPHA}·x + {TRUE_BETA}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"MODEL A: No Bias Input\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Architecture: x ──[J_in]──→ SingleDendrite ──[J_out]──→ y\")\n",
    "print(f\"  Trainable params: J_in, J_out (2 params)\")\n",
    "print(f\"  Final loss: {history_A['loss'][-1]:.6f}\")\n",
    "print(f\"  Limitation: Cannot independently control slope and offset\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"MODEL B: With Bias Input\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  Architecture: [x, 1] ──[J_α, J_β]──→ SingleDendrite ──→ y\")\n",
    "print(f\"  Trainable params: J_α, J_β (2 params)\")\n",
    "print(f\"  Final loss: {history_B['loss'][-1]:.6f}\")\n",
    "print(f\"  Advantage: J_β provides independent bias control\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "if history_B['loss'][-1] < history_A['loss'][-1]:\n",
    "    improvement = (history_A['loss'][-1] - history_B['loss'][-1]) / history_A['loss'][-1] * 100\n",
    "    print(f\"  ✓ Model B achieves {improvement:.1f}% lower loss\")\n",
    "    print(f\"  ✓ Bias input improves regression capability\")\n",
    "else:\n",
    "    print(f\"  • Model A performs comparably or better\")\n",
    "    print(f\"  • Bias may not be necessary for this task\")\n",
    "\n",
    "print(f\"\\n  ⚠ Neither model achieves perfect linear regression because\")\n",
    "print(f\"    the SingleDendrite introduces nonlinear dynamics.\")\n",
    "print(f\"    The models learn the best nonlinear approximation.\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"HARDWARE COMPATIBILITY\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  ✓ Both models use only connection weights (J values)\")\n",
    "print(f\"  ✓ No internal neuron parameters are trained\")\n",
    "print(f\"  ✓ Model B's bias input is just an extra input channel\")\n",
    "print(f\"    (constant current source in hardware)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
