{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0",
   "source": [
    "# Classification with Forward-Forward Algorithm\n",
    "\n",
    "## The Problem with Backpropagation\n",
    "\n",
    "Standard backpropagation requires:\n",
    "- Global error signal propagated backwards through all layers\n",
    "- Surrogate gradients for non-differentiable SOEN dynamics\n",
    "- Weight transport (knowing downstream weights for gradient computation)\n",
    "\n",
    "**None of these are hardware-compatible.**\n",
    "\n",
    "## The Forward-Forward Algorithm (Hinton, 2022)\n",
    "\n",
    "Replace backprop with **two forward passes**:\n",
    "\n",
    "```\n",
    "POSITIVE PASS: Real data + correct label → maximize \"goodness\"\n",
    "NEGATIVE PASS: Real data + wrong label  → minimize \"goodness\"\n",
    "```\n",
    "\n",
    "**Goodness** = sum of squared activations (or any local measure)\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────────────────┐\n",
    "                    │     FORWARD-FORWARD LEARNING        │\n",
    "                    ├─────────────────────────────────────┤\n",
    "                    │                                     │\n",
    "                    │  Positive: x ⊕ y_correct            │\n",
    "                    │     → Layer 1 → goodness₁ ↑         │\n",
    "                    │     → Layer 2 → goodness₂ ↑         │\n",
    "                    │                                     │\n",
    "                    │  Negative: x ⊕ y_wrong              │\n",
    "                    │     → Layer 1 → goodness₁ ↓         │\n",
    "                    │     → Layer 2 → goodness₂ ↓         │\n",
    "                    │                                     │\n",
    "                    │  Each layer learns LOCALLY!         │\n",
    "                    └─────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Why This is Hardware-Compatible\n",
    "\n",
    "| Property | Backprop | Forward-Forward |\n",
    "|----------|----------|------------------|\n",
    "| Error propagation | Global, backwards | None |\n",
    "| Weight updates | Requires downstream info | Local only |\n",
    "| Gradient computation | Through all layers | Per-layer |\n",
    "| Hardware feasibility | Difficult | Possible |\n",
    "\n",
    "## Our Implementation\n",
    "\n",
    "1. **Label embedding**: Concatenate one-hot label to input\n",
    "2. **Goodness metric**: Sum of squared SOEN neuron outputs\n",
    "3. **Per-layer loss**: Maximize goodness for positive, minimize for negative\n",
    "4. **Threshold**: Goodness > θ means \"positive\" prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-1",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-2",
   "source": [
    "## 1. Generate Circle-in-Ring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-3",
   "outputs": [],
   "source": [
    "def generate_circle_ring_data(n_samples=500, inner_radius=0.3, outer_radius_min=0.5, \n",
    "                               outer_radius_max=0.8, noise=0.05):\n",
    "    \"\"\"\n",
    "    Generate 2D classification data: circle inside a ring.\n",
    "    \"\"\"\n",
    "    n_each = n_samples // 2\n",
    "    \n",
    "    # Class 0: Inner circle\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_inner = np.random.uniform(0, inner_radius, n_each)\n",
    "    x_inner = r_inner * np.cos(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    y_inner = r_inner * np.sin(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    # Class 1: Outer ring\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_outer = np.random.uniform(outer_radius_min, outer_radius_max, n_each)\n",
    "    x_outer = r_outer * np.cos(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    y_outer = r_outer * np.sin(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    X = np.vstack([\n",
    "        np.column_stack([x_inner, y_inner]),\n",
    "        np.column_stack([x_outer, y_outer])\n",
    "    ])\n",
    "    y = np.array([0] * n_each + [1] * n_each)\n",
    "    \n",
    "    idx = np.random.permutation(len(y))\n",
    "    X, y = X[idx], y[idx]\n",
    "    \n",
    "    # Scale to SOEN operating range\n",
    "    X = (X + 1) / 2 * 0.25 + 0.025\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.LongTensor(y)\n",
    "\n",
    "\n",
    "N_SAMPLES = 500\n",
    "X_data, y_data = generate_circle_ring_data(N_SAMPLES)\n",
    "\n",
    "print(f\"Dataset shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "print(f\"Class distribution: {(y_data == 0).sum().item()} inner, {(y_data == 1).sum().item()} outer\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "for c, color in enumerate(['blue', 'red']):\n",
    "    mask = y_data == c\n",
    "    plt.scatter(X_data[mask, 0], X_data[mask, 1], c=color, alpha=0.6, s=20)\n",
    "plt.title('Circle vs Ring Dataset')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-4",
   "source": [
    "## 2. Forward-Forward Data Preparation\n",
    "\n",
    "Key idea: Embed the label into the input!\n",
    "\n",
    "```\n",
    "Original input: [x₁, x₂]           (2D)\n",
    "With label:     [x₁, x₂, l₀, l₁]   (4D, one-hot label appended)\n",
    "\n",
    "Positive sample: x with CORRECT label\n",
    "Negative sample: x with WRONG label\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-5",
   "outputs": [],
   "source": "N_CLASSES = 2\nSEQ_LEN = 50\nLABEL_SCALE = 0.25  # Stronger label signal (was 0.15)\n\ndef embed_label(X, y, n_classes=2, label_scale=LABEL_SCALE):\n    \"\"\"\n    Embed one-hot label into input.\n    \n    Args:\n        X: [N, input_dim] input features\n        y: [N] class labels (integers)\n        n_classes: number of classes\n        label_scale: scale for label embedding (stronger = easier to learn)\n    \n    Returns:\n        X_embedded: [N, input_dim + n_classes]\n    \"\"\"\n    N = X.shape[0]\n    one_hot = torch.zeros(N, n_classes)\n    one_hot.scatter_(1, y.unsqueeze(1), label_scale)\n    return torch.cat([X, one_hot], dim=1)\n\n\ndef create_positive_negative_pairs(X, y, n_classes=2, label_scale=LABEL_SCALE):\n    \"\"\"\n    Create positive and negative samples for Forward-Forward.\n    \n    Positive: x with correct label\n    Negative: x with random wrong label\n    \"\"\"\n    N = X.shape[0]\n    \n    # Positive: correct labels\n    X_pos = embed_label(X, y, n_classes, label_scale)\n    \n    # Negative: random wrong labels\n    y_wrong = (y + torch.randint(1, n_classes, (N,))) % n_classes\n    X_neg = embed_label(X, y_wrong, n_classes, label_scale)\n    \n    return X_pos, X_neg\n\n\n# Create positive and negative samples\nX_pos, X_neg = create_positive_negative_pairs(X_data, y_data, N_CLASSES)\n\nprint(f\"Positive samples shape: {X_pos.shape}\")\nprint(f\"Negative samples shape: {X_neg.shape}\")\nprint(f\"Label scale: {LABEL_SCALE} (stronger signal for better separation)\")\nprint(f\"\\nExample positive sample (class 0): {X_pos[y_data == 0][0]}\")\nprint(f\"Example negative sample (class 0): {X_neg[y_data == 0][0]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-6",
   "outputs": [],
   "source": [
    "# Expand to sequence for SOEN\n",
    "X_pos_seq = X_pos.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "X_neg_seq = X_neg.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "\n",
    "print(f\"SOEN input shapes:\")\n",
    "print(f\"  Positive: {X_pos_seq.shape}\")\n",
    "print(f\"  Negative: {X_neg_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-7",
   "source": "## 3. Forward-Forward SOEN Model\n\nArchitecture:\n- Input: 4D (2D features + 2D one-hot label)\n- Hidden layers: SingleDendrite neurons\n- Goodness: **Mean** of squared outputs per layer (normalized!)\n\n```\nNormalized Goodness: G = (1/d) * Σ h_j²\n\nBenefits:\n- Threshold ~0.5 works for ANY architecture\n- [4] neurons, [16] neurons, [32,32] neurons → same threshold!\n- No hyperparameter tuning per layer size\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-8",
   "outputs": [],
   "source": [
    "def build_ff_soen_model(hidden_dims, input_dim=4, dt=50.0):\n",
    "    \"\"\"\n",
    "    Build a SOEN model for Forward-Forward training.\n",
    "    \n",
    "    Args:\n",
    "        hidden_dims: List of hidden layer dimensions\n",
    "        input_dim: Input dimension (features + label embedding)\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=dt,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = []\n",
    "    connections = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": input_dim},\n",
    "    ))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        \n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "                \"learnable_params\": {\n",
    "                    \"phi_offset\": False,\n",
    "                    \"bias_current\": False,\n",
    "                    \"gamma_plus\": False,\n",
    "                    \"gamma_minus\": False,\n",
    "                },\n",
    "            },\n",
    "        ))\n",
    "        \n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Test model\n",
    "HIDDEN_DIMS = [16, 16]\n",
    "test_model = build_ff_soen_model(HIDDEN_DIMS, input_dim=4)\n",
    "print(f\"Model layers: {[l.dim for l in test_model.layers]}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in test_model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-9",
   "source": "## 4. Goodness Function and Forward-Forward Loss\n\n### Hardware-Compatible Goodness\n\nGoodness = mean of squared activations (no normalization needed):\n$$G = \\frac{1}{d} \\sum_j h_j^2$$\n\n**Hardware mapping**: This is simply the mean power in the layer!\n- Each neuron's current squared ∝ power dissipation\n- Sum across neurons = total layer power\n- Divide by neuron count = mean power (or just use sum)\n\n### Loss Function (Simulation Only)\n\n$$\\mathcal{L} = \\underbrace{\\text{softplus}(\\theta - G_{pos})}_{\\text{push pos above } \\theta} + \\underbrace{\\text{softplus}(G_{neg} - \\theta)}_{\\text{push neg below } \\theta} + \\underbrace{\\text{softplus}(m - (G_{pos} - G_{neg}))}_{\\text{contrastive margin}}$$\n\n**Note**: The loss function only exists during simulation training. \nThe trained weights transfer to hardware; the loss does not."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-10",
   "outputs": [],
   "source": "def compute_goodness(activations):\n    \"\"\"\n    Compute goodness as mean of squared activations.\n    \n    G = (1/d) * Σ h_j²\n    \n    Hardware-compatible: just measures mean power in the layer.\n    No normalization needed - threshold is calibrated for SOEN dynamics.\n    \n    Args:\n        activations: [N, dim] layer activations\n    \n    Returns:\n        goodness: [N] goodness score per sample\n    \"\"\"\n    return (activations ** 2).mean(dim=1)\n\n\ndef forward_forward_loss(goodness_pos, goodness_neg, threshold=0.1, margin=0.05):\n    \"\"\"\n    Forward-Forward loss with contrastive term.\n    \n    Hardware note: This loss is only used during simulation training.\n    The trained weights transfer to hardware; the loss function does not.\n    \n    Args:\n        threshold: calibrated for SOEN activation magnitudes (~0.05-0.15)\n        margin: minimum desired gap between pos and neg goodness\n    \"\"\"\n    # Want goodness_pos > threshold\n    loss_pos = F.softplus(threshold - goodness_pos).mean()\n    \n    # Want goodness_neg < threshold\n    loss_neg = F.softplus(goodness_neg - threshold).mean()\n    \n    # Contrastive: directly maximize the gap (training stability)\n    contrastive = F.softplus(margin - (goodness_pos - goodness_neg)).mean()\n    \n    return loss_pos + loss_neg + contrastive\n\n\n# Test goodness computation\ntest_activations = torch.randn(5, 16) * 0.3  # SOEN-scale activations\nprint(f\"Test activations shape: {test_activations.shape}\")\nprint(f\"Goodness (raw, hardware-compatible): {compute_goodness(test_activations)}\")\nprint(f\"Typical SOEN goodness range: 0.01 - 0.2\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-11",
   "source": "## 5. Layer-wise Forward-Forward Training\n\n### True Local Learning (Key for Stability!)\n\nEach layer is trained **completely independently** with its own optimizer:\n\n```\nLayer 1:\n    optimizer_1 = Adam(layer_1_weights)\n    g_pos_1 = goodness(normalize(activations_1_pos))\n    g_neg_1 = goodness(normalize(activations_1_neg))\n    loss_1 = FF_loss(g_pos_1, g_neg_1) + contrastive(g_pos_1, g_neg_1)\n    loss_1.backward()\n    optimizer_1.step()\n\nLayer 2:\n    optimizer_2 = Adam(layer_2_weights)\n    g_pos_2 = goodness(normalize(activations_2_pos))\n    g_neg_2 = goodness(normalize(activations_2_neg))\n    loss_2 = FF_loss(g_pos_2, g_neg_2) + contrastive(g_pos_2, g_neg_2)\n    loss_2.backward()\n    optimizer_2.step()\n```\n\n### Why This Helps:\n1. **No gradient interference** between layers\n2. **Each layer learns its own optimal representation**\n3. **More stable** than joint optimization\n4. **True local learning** - each layer only knows its own goodness"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-12",
   "outputs": [],
   "source": "def get_layer_activations(model, X_seq, layer_idx):\n    \"\"\"Get activations from a specific layer.\"\"\"\n    _, layer_states = model(X_seq)\n    return layer_states[layer_idx][:, -1, :]\n\n\ndef train_forward_forward_layerwise(model, X_pos_seq, X_neg_seq, n_epochs=200, lr=0.01, \n                                     threshold=0.1, margin=0.05, verbose=True):\n    \"\"\"\n    Train SOEN model with Forward-Forward algorithm.\n    \n    Hardware-compatible design:\n    - Goodness = mean squared activations (measurable as power)\n    - Layer-wise training (local learning)\n    - Contrastive loss for stable training (simulation only)\n    \n    The trained weights transfer to hardware.\n    \"\"\"\n    model.train()\n    \n    hidden_layer_indices = [i for i, l in enumerate(model.layers) if l.layer_type != 'Input']\n    \n    # Layer-wise optimizers (simulation construct for finding good weights)\n    layer_optimizers = []\n    for conn_key in model.connections.keys():\n        conn_params = [model.connections[conn_key]]\n        layer_optimizers.append(torch.optim.Adam(conn_params, lr=lr))\n    \n    history = {\n        'loss': [],\n        'goodness_pos': [],\n        'goodness_neg': [],\n        'accuracy': [],\n        'separation': [],\n    }\n    \n    for epoch in range(n_epochs):\n        total_loss = 0\n        all_goodness_pos = []\n        all_goodness_neg = []\n        \n        _, layer_states_pos = model(X_pos_seq)\n        _, layer_states_neg = model(X_neg_seq)\n        \n        for layer_idx, opt in zip(hidden_layer_indices, layer_optimizers):\n            opt.zero_grad()\n            \n            act_pos = layer_states_pos[layer_idx][:, -1, :]\n            act_neg = layer_states_neg[layer_idx][:, -1, :]\n            \n            # Hardware-compatible goodness (no normalization)\n            g_pos = compute_goodness(act_pos)\n            g_neg = compute_goodness(act_neg)\n            \n            all_goodness_pos.append(g_pos.mean().item())\n            all_goodness_neg.append(g_neg.mean().item())\n            \n            layer_loss = forward_forward_loss(g_pos, g_neg, threshold, margin)\n            total_loss = total_loss + layer_loss.item()\n            \n            layer_loss.backward(retain_graph=True)\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            opt.step()\n        \n        with torch.no_grad():\n            acc = evaluate_ff_accuracy(model, X_data, y_data, n_classes=N_CLASSES)\n        \n        mean_g_pos = np.mean(all_goodness_pos)\n        mean_g_neg = np.mean(all_goodness_neg)\n        separation = mean_g_pos - mean_g_neg\n        \n        history['loss'].append(total_loss)\n        history['goodness_pos'].append(mean_g_pos)\n        history['goodness_neg'].append(mean_g_neg)\n        history['accuracy'].append(acc)\n        history['separation'].append(separation)\n        \n        if verbose and (epoch + 1) % 20 == 0:\n            print(f\"Epoch {epoch+1}: Loss={total_loss:.4f}, \"\n                  f\"G_pos={mean_g_pos:.4f}, G_neg={mean_g_neg:.4f}, \"\n                  f\"Sep={separation:.4f}, Acc={acc:.4f}\")\n    \n    return history\n\n\ndef evaluate_ff_accuracy(model, X, y, n_classes=2, label_scale=LABEL_SCALE):\n    \"\"\"\n    Evaluate Forward-Forward model accuracy.\n    \n    Hardware-compatible inference:\n    - Run forward pass for each class hypothesis\n    - Sum goodness (power) across layers\n    - Predict class with highest total goodness\n    \"\"\"\n    model.eval()\n    N = X.shape[0]\n    \n    all_goodness = []\n    \n    for c in range(n_classes):\n        y_test = torch.full((N,), c, dtype=torch.long)\n        X_test = embed_label(X, y_test, n_classes, label_scale)\n        X_test_seq = X_test.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n        \n        with torch.no_grad():\n            _, layer_states = model(X_test_seq)\n            total_goodness = torch.zeros(N)\n            for layer_idx in range(1, len(model.layers)):\n                act = layer_states[layer_idx][:, -1, :]\n                total_goodness += compute_goodness(act)\n        \n        all_goodness.append(total_goodness)\n    \n    goodness_matrix = torch.stack(all_goodness, dim=1)\n    predictions = goodness_matrix.argmax(dim=1)\n    \n    accuracy = (predictions == y).float().mean().item()\n    model.train()\n    return accuracy\n\n\ndef train_forward_forward(model, X_pos_seq, X_neg_seq, n_epochs=200, lr=0.01, \n                          threshold=0.1, verbose=True):\n    \"\"\"Wrapper for layer-wise training.\"\"\"\n    return train_forward_forward_layerwise(\n        model, X_pos_seq, X_neg_seq, \n        n_epochs=n_epochs, lr=lr, threshold=threshold, \n        margin=0.05, verbose=verbose\n    )"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-13",
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-14",
   "outputs": [],
   "source": "# Build model - recreate data with LABEL_SCALE\nX_pos, X_neg = create_positive_negative_pairs(X_data, y_data, N_CLASSES, LABEL_SCALE)\nX_pos_seq = X_pos.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\nX_neg_seq = X_neg.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n\nHIDDEN_DIMS = [16, 16]\nTHRESHOLD = 0.1  # Calibrated for SOEN activation magnitudes\nN_EPOCHS = 300\nLR = 0.01\n\nprint(f\"Training Forward-Forward SOEN classifier (HARDWARE-COMPATIBLE)...\")\nprint(f\"Hidden dimensions: {HIDDEN_DIMS}\")\nprint(f\"Threshold: {THRESHOLD} (calibrated for SOEN)\")\nprint(f\"Label scale: {LABEL_SCALE}\")\nprint(f\"\\nHardware-compatible features:\")\nprint(f\"  ✓ Goodness = mean(activations²) = power measurement\")\nprint(f\"  ✓ Label embedding = optical input modulation\")\nprint(f\"  ✓ Inference = compare goodness across class hypotheses\")\nprint(f\"  ✓ No normalization (purely local computation)\")\nprint(\"=\" * 60)\n\ntorch.manual_seed(42)\nmodel = build_ff_soen_model(HIDDEN_DIMS, input_dim=4)\n\nhistory = train_forward_forward(\n    model, X_pos_seq, X_neg_seq,\n    n_epochs=N_EPOCHS, lr=LR, threshold=THRESHOLD, verbose=True\n)\n\nprint(\"=\" * 60)\nprint(f\"Final accuracy: {history['accuracy'][-1]:.4f}\")\nprint(f\"Final separation (G_pos - G_neg): {history['separation'][-1]:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-15",
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-16",
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Loss\nax1 = axes[0, 0]\nax1.plot(history['loss'], color='steelblue', lw=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Forward-Forward Loss')\nax1.set_title('Training Loss')\nax1.grid(True, alpha=0.3)\n\n# Goodness\nax2 = axes[0, 1]\nax2.plot(history['goodness_pos'], label='Positive', color='green', lw=2)\nax2.plot(history['goodness_neg'], label='Negative', color='red', lw=2)\nax2.axhline(y=THRESHOLD, color='black', linestyle='--', label=f'Threshold={THRESHOLD}')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Mean Normalized Goodness')\nax2.set_title('Goodness Separation')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Separation\nax3 = axes[1, 0]\nax3.plot(history['separation'], color='purple', lw=2)\nax3.axhline(y=0, color='black', linestyle='--', alpha=0.5)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('G_pos - G_neg')\nax3.set_title('Goodness Separation (should be positive)')\nax3.grid(True, alpha=0.3)\n\n# Accuracy\nax4 = axes[1, 1]\nax4.plot(history['accuracy'], color='coral', lw=2)\nax4.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\nax4.set_xlabel('Epoch')\nax4.set_ylabel('Accuracy')\nax4.set_title('Classification Accuracy')\nax4.set_ylim(0.4, 1.05)\nax4.legend()\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-17",
   "source": [
    "## 8. Visualize Goodness Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-18",
   "outputs": [],
   "source": "# Compute final goodness for positive and negative samples\nmodel.eval()\nwith torch.no_grad():\n    _, states_pos = model(X_pos_seq)\n    _, states_neg = model(X_neg_seq)\n    \n    # Average goodness across all layers\n    g_pos_final = torch.zeros(len(X_data))\n    g_neg_final = torch.zeros(len(X_data))\n    n_layers = 0\n    \n    for layer_idx in range(1, len(model.layers)):\n        g_pos_final += compute_goodness(states_pos[layer_idx][:, -1, :])\n        g_neg_final += compute_goodness(states_neg[layer_idx][:, -1, :])\n        n_layers += 1\n    \n    g_pos_final /= n_layers\n    g_neg_final /= n_layers\n\n# Plot distributions\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\nax1 = axes[0]\nax1.hist(g_pos_final.numpy(), bins=30, alpha=0.7, label='Positive', color='green')\nax1.hist(g_neg_final.numpy(), bins=30, alpha=0.7, label='Negative', color='red')\nax1.axvline(x=THRESHOLD, color='black', linestyle='--', label=f'Threshold={THRESHOLD}')\nax1.set_xlabel('Goodness (mean power)')\nax1.set_ylabel('Count')\nax1.set_title('Goodness Distribution: Positive vs Negative')\nax1.legend()\n\nax2 = axes[1]\ngoodness_diff = g_pos_final - g_neg_final\ncolors = ['blue' if y == 0 else 'red' for y in y_data]\nax2.scatter(range(len(goodness_diff)), goodness_diff.numpy(), c=colors, alpha=0.5, s=10)\nax2.axhline(y=0, color='black', linestyle='--')\nax2.set_xlabel('Sample Index')\nax2.set_ylabel('Goodness(Positive) - Goodness(Negative)')\nax2.set_title('Per-Sample Goodness Margin (should be > 0)')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Mean positive goodness: {g_pos_final.mean():.4f}\")\nprint(f\"Mean negative goodness: {g_neg_final.mean():.4f}\")\nprint(f\"Separation: {(g_pos_final.mean() - g_neg_final.mean()):.4f}\")\nprint(f\"Samples with correct margin (pos > neg): {(goodness_diff > 0).sum().item()}/{len(goodness_diff)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-19",
   "source": [
    "## 9. Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-20",
   "outputs": [],
   "source": "def plot_ff_decision_boundary(model, X_data, y_data, n_classes=2, label_scale=LABEL_SCALE, \n                               resolution=80, ax=None):\n    \"\"\"Plot decision boundary for Forward-Forward classifier.\"\"\"\n    if ax is None:\n        fig, ax = plt.subplots(figsize=(8, 8))\n    \n    x_min, x_max = X_data[:, 0].min() - 0.02, X_data[:, 0].max() + 0.02\n    y_min, y_max = X_data[:, 1].min() - 0.02, X_data[:, 1].max() + 0.02\n    \n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, resolution),\n        np.linspace(y_min, y_max, resolution)\n    )\n    \n    grid_points = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n    N_grid = len(grid_points)\n    \n    model.eval()\n    all_goodness = []\n    \n    for c in range(n_classes):\n        y_test = torch.full((N_grid,), c, dtype=torch.long)\n        X_test = embed_label(grid_points, y_test, n_classes, label_scale)\n        X_test_seq = X_test.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n        \n        with torch.no_grad():\n            _, layer_states = model(X_test_seq)\n            total_goodness = torch.zeros(N_grid)\n            for layer_idx in range(1, len(model.layers)):\n                act = layer_states[layer_idx][:, -1, :]\n                total_goodness += compute_goodness(act)\n        \n        all_goodness.append(total_goodness)\n    \n    goodness_matrix = torch.stack(all_goodness, dim=1)\n    probs = torch.softmax(goodness_matrix * 10, dim=1)[:, 1].numpy()  # Scale for sharper boundary\n    Z = probs.reshape(xx.shape)\n    \n    ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.7)\n    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n    \n    for c, color in enumerate(['blue', 'red']):\n        mask = y_data == c\n        ax.scatter(X_data[mask, 0].numpy(), X_data[mask, 1].numpy(), c=color, \n                   s=15, alpha=0.6, edgecolors='white', linewidths=0.3)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title('Forward-Forward Decision Boundary (Hardware-Compatible)')\n    ax.set_aspect('equal')\n    \n    return ax\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_ff_decision_boundary(model, X_data, y_data, ax=ax)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-21",
   "source": [
    "## 10. Hyperparameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-22",
   "outputs": [],
   "source": "# Try different architectures\nexperiments = [\n    {'hidden_dims': [8]},\n    {'hidden_dims': [16]},\n    {'hidden_dims': [32]},\n    {'hidden_dims': [8, 8]},\n    {'hidden_dims': [16, 16]},\n]\n\n# Threshold calibrated for SOEN dynamics\nSOEN_THRESHOLD = 0.1\n\nresults = []\n\nprint(\"Hyperparameter exploration (HARDWARE-COMPATIBLE)...\")\nprint(f\"Threshold: {SOEN_THRESHOLD} (calibrated for SOEN)\")\nprint(f\"Label scale: {LABEL_SCALE}\")\nprint(\"=\" * 60)\n\nfor exp in experiments:\n    torch.manual_seed(42)\n    \n    model = build_ff_soen_model(exp['hidden_dims'], input_dim=4)\n    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    \n    X_pos_exp, X_neg_exp = create_positive_negative_pairs(X_data, y_data, N_CLASSES, LABEL_SCALE)\n    X_pos_seq_exp = X_pos_exp.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n    X_neg_seq_exp = X_neg_exp.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n    \n    history_exp = train_forward_forward(\n        model, X_pos_seq_exp, X_neg_seq_exp,\n        n_epochs=200, lr=0.01, threshold=SOEN_THRESHOLD, verbose=False\n    )\n    \n    final_acc = history_exp['accuracy'][-1]\n    final_sep = history_exp['separation'][-1]\n    results.append({\n        'hidden_dims': str(exp['hidden_dims']),\n        'params': n_params,\n        'accuracy': final_acc,\n        'separation': final_sep,\n    })\n    \n    print(f\"Hidden={str(exp['hidden_dims']):12s} | Params={n_params:5d} | \"\n          f\"Acc={final_acc:.4f} | Sep={final_sep:.4f}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-23",
   "source": [
    "## 11. Compare with Backprop Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-24",
   "outputs": [],
   "source": [
    "def train_backprop_baseline(X_data, y_data, hidden_dims=[16, 16], n_epochs=300, lr=0.02):\n",
    "    \"\"\"Train a SOEN model with standard backpropagation for comparison.\"\"\"\n",
    "    \n",
    "    # Build model (without label embedding, just 2D input)\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=50.0,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = [LayerConfig(layer_id=0, layer_type=\"Input\", params={\"dim\": 2})]\n",
    "    connections = []\n",
    "    \n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "            },\n",
    "        ))\n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "    \n",
    "    # Output layer (2 neurons for classification)\n",
    "    output_id = len(hidden_dims) + 1\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=output_id,\n",
    "        layer_type=\"SingleDendrite\",\n",
    "        params={\n",
    "            \"dim\": 2,\n",
    "            \"solver\": \"FE\",\n",
    "            \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "            \"phi_offset\": 0.2,\n",
    "            \"bias_current\": 1.98,\n",
    "            \"gamma_plus\": 0.0005,\n",
    "            \"gamma_minus\": 1e-6,\n",
    "        },\n",
    "    ))\n",
    "    connections.append(ConnectionConfig(\n",
    "        from_layer=output_id - 1,\n",
    "        to_layer=output_id,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"xavier_uniform\"},\n",
    "    ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    # Prepare data\n",
    "    X_seq = X_data.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "    \n",
    "    # Train with BCE on s1 - s0\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    y_target = y_data.float().unsqueeze(1)\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        final_hist, _ = model(X_seq)\n",
    "        output = final_hist[:, -1, :]\n",
    "        logits = (output[:, 1] - output[:, 0]).unsqueeze(1)\n",
    "        \n",
    "        loss = criterion(logits, y_target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            acc = (preds == y_target).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "\n",
    "# Train backprop baseline\n",
    "print(\"Training backprop baseline...\")\n",
    "torch.manual_seed(42)\n",
    "backprop_accs = train_backprop_baseline(X_data, y_data, hidden_dims=[16, 16], n_epochs=300)\n",
    "print(f\"Backprop final accuracy: {backprop_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-25",
   "outputs": [],
   "source": [
    "# Compare training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(history['accuracy'], label='Forward-Forward', color='coral', lw=2)\n",
    "ax.plot(backprop_accs, label='Backpropagation', color='steelblue', lw=2)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Forward-Forward vs Backpropagation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Comparison:\")\n",
    "print(f\"  Forward-Forward: {history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Backpropagation: {backprop_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-26",
   "source": [
    "## 12. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-27",
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"CONCLUSIONS: HARDWARE-COMPATIBLE FORWARD-FORWARD FOR SOEN\")\nprint(\"=\" * 70)\n\nprint(f\"\\n1. PERFORMANCE:\")\nprint(f\"   Forward-Forward accuracy: {history['accuracy'][-1]:.4f}\")\nprint(f\"   Backpropagation accuracy: {backprop_accs[-1]:.4f}\")\n\nprint(f\"\\n2. GOODNESS METRICS:\")\nprint(f\"   Mean positive goodness: {history['goodness_pos'][-1]:.4f}\")\nprint(f\"   Mean negative goodness: {history['goodness_neg'][-1]:.4f}\")\nprint(f\"   Separation: {history['separation'][-1]:.4f}\")\nprint(f\"   Threshold: {THRESHOLD}\")\n\nprint(f\"\\n3. HARDWARE COMPATIBILITY:\")\nprint(f\"   ✓ Goodness = mean(I²) = power measurement (local)\")\nprint(f\"   ✓ Label embedding = optical input modulation\")\nprint(f\"   ✓ No normalization (no global computation)\")\nprint(f\"   ✓ Inference = compare goodness across hypotheses\")\n\nprint(f\"\\n4. WHAT TRANSFERS TO HARDWARE:\")\nprint(f\"   ✓ Trained weights (synaptic strengths)\")\nprint(f\"   ✓ Network architecture (layer connectivity)\")\nprint(f\"   ✓ Inference procedure (two forward passes)\")\nprint(f\"   ✗ Loss function (simulation only)\")\nprint(f\"   ✗ Optimizer (simulation only)\")\n\nprint(f\"\\n5. INFERENCE PROCEDURE (Hardware-Implementable):\")\nprint(f\"   1. Input sample with class-0 label → measure total power\")\nprint(f\"   2. Input sample with class-1 label → measure total power\")\nprint(f\"   3. Predict class with higher power\")\n\nprint(\"\\n\" + \"=\" * 70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}