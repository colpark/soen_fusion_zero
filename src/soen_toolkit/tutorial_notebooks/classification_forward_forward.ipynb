{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-0"
  },
   "source": [
    "# Classification with Forward-Forward Algorithm\n",
    "\n",
    "## The Problem with Backpropagation\n",
    "\n",
    "Standard backpropagation requires:\n",
    "- Global error signal propagated backwards through all layers\n",
    "- Surrogate gradients for non-differentiable SOEN dynamics\n",
    "- Weight transport (knowing downstream weights for gradient computation)\n",
    "\n",
    "**None of these are hardware-compatible.**\n",
    "\n",
    "## The Forward-Forward Algorithm (Hinton, 2022)\n",
    "\n",
    "Replace backprop with **two forward passes**:\n",
    "\n",
    "```\n",
    "POSITIVE PASS: Real data + correct label → maximize \"goodness\"\n",
    "NEGATIVE PASS: Real data + wrong label  → minimize \"goodness\"\n",
    "```\n",
    "\n",
    "**Goodness** = sum of squared activations (or any local measure)\n",
    "\n",
    "```\n",
    "                    ┌─────────────────────────────────────┐\n",
    "                    │     FORWARD-FORWARD LEARNING        │\n",
    "                    ├─────────────────────────────────────┤\n",
    "                    │                                     │\n",
    "                    │  Positive: x ⊕ y_correct            │\n",
    "                    │     → Layer 1 → goodness₁ ↑         │\n",
    "                    │     → Layer 2 → goodness₂ ↑         │\n",
    "                    │                                     │\n",
    "                    │  Negative: x ⊕ y_wrong              │\n",
    "                    │     → Layer 1 → goodness₁ ↓         │\n",
    "                    │     → Layer 2 → goodness₂ ↓         │\n",
    "                    │                                     │\n",
    "                    │  Each layer learns LOCALLY!         │\n",
    "                    └─────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "## Why This is Hardware-Compatible\n",
    "\n",
    "| Property | Backprop | Forward-Forward |\n",
    "|----------|----------|------------------|\n",
    "| Error propagation | Global, backwards | None |\n",
    "| Weight updates | Requires downstream info | Local only |\n",
    "| Gradient computation | Through all layers | Per-layer |\n",
    "| Hardware feasibility | Difficult | Possible |\n",
    "\n",
    "## Our Implementation\n",
    "\n",
    "1. **Label embedding**: Concatenate one-hot label to input\n",
    "2. **Goodness metric**: Sum of squared SOEN neuron outputs\n",
    "3. **Per-layer loss**: Maximize goodness for positive, minimize for negative\n",
    "4. **Threshold**: Goodness > θ means \"positive\" prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-1",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-2"
  },
   "source": [
    "## 1. Generate Circle-in-Ring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-3",
   "outputs": [],
   "source": [
    "def generate_circle_ring_data(n_samples=500, inner_radius=0.3, outer_radius_min=0.5, \n",
    "                               outer_radius_max=0.8, noise=0.05):\n",
    "    \"\"\"\n",
    "    Generate 2D classification data: circle inside a ring.\n",
    "    \"\"\"\n",
    "    n_each = n_samples // 2\n",
    "    \n",
    "    # Class 0: Inner circle\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_inner = np.random.uniform(0, inner_radius, n_each)\n",
    "    x_inner = r_inner * np.cos(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    y_inner = r_inner * np.sin(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    # Class 1: Outer ring\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_outer = np.random.uniform(outer_radius_min, outer_radius_max, n_each)\n",
    "    x_outer = r_outer * np.cos(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    y_outer = r_outer * np.sin(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    X = np.vstack([\n",
    "        np.column_stack([x_inner, y_inner]),\n",
    "        np.column_stack([x_outer, y_outer])\n",
    "    ])\n",
    "    y = np.array([0] * n_each + [1] * n_each)\n",
    "    \n",
    "    idx = np.random.permutation(len(y))\n",
    "    X, y = X[idx], y[idx]\n",
    "    \n",
    "    # Scale to SOEN operating range\n",
    "    X = (X + 1) / 2 * 0.25 + 0.025\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.LongTensor(y)\n",
    "\n",
    "\n",
    "N_SAMPLES = 500\n",
    "X_data, y_data = generate_circle_ring_data(N_SAMPLES)\n",
    "\n",
    "print(f\"Dataset shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "print(f\"Class distribution: {(y_data == 0).sum().item()} inner, {(y_data == 1).sum().item()} outer\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "for c, color in enumerate(['blue', 'red']):\n",
    "    mask = y_data == c\n",
    "    plt.scatter(X_data[mask, 0], X_data[mask, 1], c=color, alpha=0.6, s=20)\n",
    "plt.title('Circle vs Ring Dataset')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-4"
  },
   "source": [
    "## 2. Forward-Forward Data Preparation\n",
    "\n",
    "Key idea: Embed the label into the input!\n",
    "\n",
    "```\n",
    "Original input: [x₁, x₂]           (2D)\n",
    "With label:     [x₁, x₂, l₀, l₁]   (4D, one-hot label appended)\n",
    "\n",
    "Positive sample: x with CORRECT label\n",
    "Negative sample: x with WRONG label\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-5",
   "outputs": [],
   "source": [
    "N_CLASSES = 2\n",
    "SEQ_LEN = 50\n",
    "\n",
    "def embed_label(X, y, n_classes=2, label_scale=0.15):\n",
    "    \"\"\"\n",
    "    Embed one-hot label into input.\n",
    "    \n",
    "    Args:\n",
    "        X: [N, input_dim] input features\n",
    "        y: [N] class labels (integers)\n",
    "        n_classes: number of classes\n",
    "        label_scale: scale for label embedding (in SOEN operating range)\n",
    "    \n",
    "    Returns:\n",
    "        X_embedded: [N, input_dim + n_classes]\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    one_hot = torch.zeros(N, n_classes)\n",
    "    one_hot.scatter_(1, y.unsqueeze(1), label_scale)  # Scale to SOEN range\n",
    "    return torch.cat([X, one_hot], dim=1)\n",
    "\n",
    "\n",
    "def create_positive_negative_pairs(X, y, n_classes=2, label_scale=0.15):\n",
    "    \"\"\"\n",
    "    Create positive and negative samples for Forward-Forward.\n",
    "    \n",
    "    Positive: x with correct label\n",
    "    Negative: x with random wrong label\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Positive: correct labels\n",
    "    X_pos = embed_label(X, y, n_classes, label_scale)\n",
    "    \n",
    "    # Negative: random wrong labels\n",
    "    y_wrong = (y + torch.randint(1, n_classes, (N,))) % n_classes\n",
    "    X_neg = embed_label(X, y_wrong, n_classes, label_scale)\n",
    "    \n",
    "    return X_pos, X_neg\n",
    "\n",
    "\n",
    "# Create positive and negative samples\n",
    "X_pos, X_neg = create_positive_negative_pairs(X_data, y_data, N_CLASSES)\n",
    "\n",
    "print(f\"Positive samples shape: {X_pos.shape}\")\n",
    "print(f\"Negative samples shape: {X_neg.shape}\")\n",
    "print(f\"\\nExample positive sample (class 0): {X_pos[y_data == 0][0]}\")\n",
    "print(f\"Example negative sample (class 0): {X_neg[y_data == 0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-6",
   "outputs": [],
   "source": [
    "# Expand to sequence for SOEN\n",
    "X_pos_seq = X_pos.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "X_neg_seq = X_neg.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "\n",
    "print(f\"SOEN input shapes:\")\n",
    "print(f\"  Positive: {X_pos_seq.shape}\")\n",
    "print(f\"  Negative: {X_neg_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-7"
  },
   "source": [
    "## 3. Forward-Forward SOEN Model\n",
    "\n",
    "Architecture:\n",
    "- Input: 4D (2D features + 2D one-hot label)\n",
    "- Hidden layers: SingleDendrite neurons\n",
    "- Goodness: Sum of squared outputs per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-8",
   "outputs": [],
   "source": [
    "def build_ff_soen_model(hidden_dims, input_dim=4, dt=50.0):\n",
    "    \"\"\"\n",
    "    Build a SOEN model for Forward-Forward training.\n",
    "    \n",
    "    Args:\n",
    "        hidden_dims: List of hidden layer dimensions\n",
    "        input_dim: Input dimension (features + label embedding)\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=dt,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = []\n",
    "    connections = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": input_dim},\n",
    "    ))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        \n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "                \"learnable_params\": {\n",
    "                    \"phi_offset\": False,\n",
    "                    \"bias_current\": False,\n",
    "                    \"gamma_plus\": False,\n",
    "                    \"gamma_minus\": False,\n",
    "                },\n",
    "            },\n",
    "        ))\n",
    "        \n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Test model\n",
    "HIDDEN_DIMS = [16, 16]\n",
    "test_model = build_ff_soen_model(HIDDEN_DIMS, input_dim=4)\n",
    "print(f\"Model layers: {[l.dim for l in test_model.layers]}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in test_model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-9"
  },
   "source": [
    "## 4. Goodness Function and Forward-Forward Loss\n",
    "\n",
    "**Goodness** of a layer = sum of squared activations\n",
    "\n",
    "$$G = \\sum_j h_j^2$$\n",
    "\n",
    "**Forward-Forward Loss** (per layer):\n",
    "\n",
    "$$\\mathcal{L} = \\log(1 + e^{-(G_{pos} - \\theta)}) + \\log(1 + e^{(G_{neg} - \\theta)})$$\n",
    "\n",
    "Where θ is a threshold. This pushes:\n",
    "- Positive samples to have goodness > θ\n",
    "- Negative samples to have goodness < θ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-10",
   "outputs": [],
   "source": [
    "def compute_goodness(activations):\n",
    "    \"\"\"\n",
    "    Compute goodness as sum of squared activations.\n",
    "    \n",
    "    Args:\n",
    "        activations: [N, dim] layer activations\n",
    "    \n",
    "    Returns:\n",
    "        goodness: [N] goodness score per sample\n",
    "    \"\"\"\n",
    "    return (activations ** 2).sum(dim=1)\n",
    "\n",
    "\n",
    "def forward_forward_loss(goodness_pos, goodness_neg, threshold=2.0):\n",
    "    \"\"\"\n",
    "    Forward-Forward loss function.\n",
    "    \n",
    "    Encourages:\n",
    "    - goodness_pos > threshold (positive samples have high goodness)\n",
    "    - goodness_neg < threshold (negative samples have low goodness)\n",
    "    \n",
    "    Uses softplus for smooth optimization.\n",
    "    \"\"\"\n",
    "    # Loss for positive: want goodness > threshold\n",
    "    loss_pos = F.softplus(threshold - goodness_pos).mean()\n",
    "    \n",
    "    # Loss for negative: want goodness < threshold\n",
    "    loss_neg = F.softplus(goodness_neg - threshold).mean()\n",
    "    \n",
    "    return loss_pos + loss_neg\n",
    "\n",
    "\n",
    "# Test goodness computation\n",
    "test_activations = torch.randn(5, 16)\n",
    "print(f\"Test activations shape: {test_activations.shape}\")\n",
    "print(f\"Goodness: {compute_goodness(test_activations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-11"
  },
   "source": [
    "## 5. Layer-wise Forward-Forward Training\n",
    "\n",
    "Key insight: Each layer is trained **independently**!\n",
    "\n",
    "```\n",
    "For each layer:\n",
    "    1. Forward pass positive samples → get activations\n",
    "    2. Forward pass negative samples → get activations\n",
    "    3. Compute goodness for both\n",
    "    4. Update weights to maximize pos goodness, minimize neg goodness\n",
    "    5. Detach activations before passing to next layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-12",
   "outputs": [],
   "source": [
    "def get_layer_activations(model, X_seq, layer_idx):\n",
    "    \"\"\"\n",
    "    Get activations from a specific layer.\n",
    "    \n",
    "    Args:\n",
    "        model: SOEN model\n",
    "        X_seq: [N, T, input_dim] input sequence\n",
    "        layer_idx: which layer to get activations from (1-indexed for hidden layers)\n",
    "    \n",
    "    Returns:\n",
    "        activations: [N, dim] final timestep activations\n",
    "    \"\"\"\n",
    "    _, layer_states = model(X_seq)\n",
    "    # Get final timestep activations from the specified layer\n",
    "    return layer_states[layer_idx][:, -1, :]\n",
    "\n",
    "\n",
    "def train_forward_forward(model, X_pos_seq, X_neg_seq, n_epochs=200, lr=0.01, \n",
    "                          threshold=2.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Train SOEN model with Forward-Forward algorithm.\n",
    "    \n",
    "    Each layer is trained to:\n",
    "    - Maximize goodness for positive samples\n",
    "    - Minimize goodness for negative samples\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    n_hidden_layers = len([l for l in model.layers if l.layer_type != 'Input'])\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'goodness_pos': [],\n",
    "        'goodness_neg': [],\n",
    "        'accuracy': [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss = 0\n",
    "        all_goodness_pos = []\n",
    "        all_goodness_neg = []\n",
    "        \n",
    "        # Forward pass for all layers\n",
    "        _, layer_states_pos = model(X_pos_seq)\n",
    "        _, layer_states_neg = model(X_neg_seq)\n",
    "        \n",
    "        # Compute loss for each hidden layer\n",
    "        for layer_idx in range(1, n_hidden_layers + 1):\n",
    "            # Get activations (final timestep)\n",
    "            act_pos = layer_states_pos[layer_idx][:, -1, :]\n",
    "            act_neg = layer_states_neg[layer_idx][:, -1, :]\n",
    "            \n",
    "            # Compute goodness\n",
    "            g_pos = compute_goodness(act_pos)\n",
    "            g_neg = compute_goodness(act_neg)\n",
    "            \n",
    "            all_goodness_pos.append(g_pos.mean().item())\n",
    "            all_goodness_neg.append(g_neg.mean().item())\n",
    "            \n",
    "            # Forward-Forward loss for this layer\n",
    "            layer_loss = forward_forward_loss(g_pos, g_neg, threshold)\n",
    "            total_loss = total_loss + layer_loss\n",
    "        \n",
    "        # Backward and update\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute accuracy using final layer goodness\n",
    "        with torch.no_grad():\n",
    "            # For inference, we need to test both label hypotheses\n",
    "            acc = evaluate_ff_accuracy(model, X_data, y_data, threshold)\n",
    "        \n",
    "        history['loss'].append(total_loss.item())\n",
    "        history['goodness_pos'].append(np.mean(all_goodness_pos))\n",
    "        history['goodness_neg'].append(np.mean(all_goodness_neg))\n",
    "        history['accuracy'].append(acc)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={total_loss.item():.4f}, \"\n",
    "                  f\"G_pos={np.mean(all_goodness_pos):.2f}, \"\n",
    "                  f\"G_neg={np.mean(all_goodness_neg):.2f}, \"\n",
    "                  f\"Acc={acc:.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_ff_accuracy(model, X, y, threshold, n_classes=2, label_scale=0.15):\n",
    "    \"\"\"\n",
    "    Evaluate Forward-Forward model accuracy.\n",
    "    \n",
    "    For each sample, try all possible labels and pick the one with highest goodness.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # For each class, embed the label and compute goodness\n",
    "    all_goodness = []\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        # Create label tensor for this class\n",
    "        y_test = torch.full((N,), c, dtype=torch.long)\n",
    "        X_test = embed_label(X, y_test, n_classes, label_scale)\n",
    "        X_test_seq = X_test.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "        \n",
    "        # Get final layer goodness\n",
    "        with torch.no_grad():\n",
    "            _, layer_states = model(X_test_seq)\n",
    "            # Sum goodness across all hidden layers\n",
    "            total_goodness = torch.zeros(N)\n",
    "            for layer_idx in range(1, len(model.layers)):\n",
    "                act = layer_states[layer_idx][:, -1, :]\n",
    "                total_goodness += compute_goodness(act)\n",
    "        \n",
    "        all_goodness.append(total_goodness)\n",
    "    \n",
    "    # Stack and get predictions (class with highest goodness)\n",
    "    goodness_matrix = torch.stack(all_goodness, dim=1)  # [N, n_classes]\n",
    "    predictions = goodness_matrix.argmax(dim=1)\n",
    "    \n",
    "    accuracy = (predictions == y).float().mean().item()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-13"
  },
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-14",
   "outputs": [],
   "source": [
    "# Build model\n",
    "HIDDEN_DIMS = [16, 16]\n",
    "THRESHOLD = 2.0\n",
    "N_EPOCHS = 300\n",
    "LR = 0.02\n",
    "\n",
    "print(f\"Training Forward-Forward SOEN classifier...\")\n",
    "print(f\"Hidden dimensions: {HIDDEN_DIMS}\")\n",
    "print(f\"Goodness threshold: {THRESHOLD}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = build_ff_soen_model(HIDDEN_DIMS, input_dim=4)\n",
    "\n",
    "history = train_forward_forward(\n",
    "    model, X_pos_seq, X_neg_seq,\n",
    "    n_epochs=N_EPOCHS, lr=LR, threshold=THRESHOLD, verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final accuracy: {history['accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-15"
  },
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-16",
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0]\n",
    "ax1.plot(history['loss'], color='steelblue', lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Forward-Forward Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Goodness\n",
    "ax2 = axes[1]\n",
    "ax2.plot(history['goodness_pos'], label='Positive', color='green', lw=2)\n",
    "ax2.plot(history['goodness_neg'], label='Negative', color='red', lw=2)\n",
    "ax2.axhline(y=THRESHOLD, color='black', linestyle='--', label=f'Threshold={THRESHOLD}')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Goodness')\n",
    "ax2.set_title('Goodness Separation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax3 = axes[2]\n",
    "ax3.plot(history['accuracy'], color='coral', lw=2)\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Classification Accuracy')\n",
    "ax3.set_ylim(0.4, 1.05)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-17"
  },
   "source": [
    "## 8. Visualize Goodness Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-18",
   "outputs": [],
   "source": [
    "# Compute final goodness for positive and negative samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, states_pos = model(X_pos_seq)\n",
    "    _, states_neg = model(X_neg_seq)\n",
    "    \n",
    "    # Sum goodness across all layers\n",
    "    g_pos_final = torch.zeros(len(X_data))\n",
    "    g_neg_final = torch.zeros(len(X_data))\n",
    "    \n",
    "    for layer_idx in range(1, len(model.layers)):\n",
    "        g_pos_final += compute_goodness(states_pos[layer_idx][:, -1, :])\n",
    "        g_neg_final += compute_goodness(states_neg[layer_idx][:, -1, :])\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.hist(g_pos_final.numpy(), bins=30, alpha=0.7, label='Positive', color='green')\n",
    "ax1.hist(g_neg_final.numpy(), bins=30, alpha=0.7, label='Negative', color='red')\n",
    "ax1.axvline(x=THRESHOLD * len(HIDDEN_DIMS), color='black', linestyle='--', \n",
    "            label=f'Threshold (×{len(HIDDEN_DIMS)} layers)')\n",
    "ax1.set_xlabel('Total Goodness')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.set_title('Goodness Distribution: Positive vs Negative')\n",
    "ax1.legend()\n",
    "\n",
    "# Scatter plot: goodness difference\n",
    "ax2 = axes[1]\n",
    "goodness_diff = g_pos_final - g_neg_final\n",
    "colors = ['blue' if y == 0 else 'red' for y in y_data]\n",
    "ax2.scatter(range(len(goodness_diff)), goodness_diff.numpy(), c=colors, alpha=0.5, s=10)\n",
    "ax2.axhline(y=0, color='black', linestyle='--')\n",
    "ax2.set_xlabel('Sample Index')\n",
    "ax2.set_ylabel('Goodness(Positive) - Goodness(Negative)')\n",
    "ax2.set_title('Per-Sample Goodness Margin')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean positive goodness: {g_pos_final.mean():.2f}\")\n",
    "print(f\"Mean negative goodness: {g_neg_final.mean():.2f}\")\n",
    "print(f\"Separation: {(g_pos_final.mean() - g_neg_final.mean()):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-19"
  },
   "source": [
    "## 9. Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-20",
   "outputs": [],
   "source": [
    "def plot_ff_decision_boundary(model, X_data, y_data, n_classes=2, label_scale=0.15, \n",
    "                               resolution=80, ax=None):\n",
    "    \"\"\"Plot decision boundary for Forward-Forward classifier.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    x_min, x_max = X_data[:, 0].min() - 0.02, X_data[:, 0].max() + 0.02\n",
    "    y_min, y_max = X_data[:, 1].min() - 0.02, X_data[:, 1].max() + 0.02\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution),\n",
    "        np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "    \n",
    "    grid_points = torch.FloatTensor(np.c_[xx.ravel(), yy.ravel()])\n",
    "    N_grid = len(grid_points)\n",
    "    \n",
    "    # Compute goodness for each class hypothesis\n",
    "    model.eval()\n",
    "    all_goodness = []\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        y_test = torch.full((N_grid,), c, dtype=torch.long)\n",
    "        X_test = embed_label(grid_points, y_test, n_classes, label_scale)\n",
    "        X_test_seq = X_test.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, layer_states = model(X_test_seq)\n",
    "            total_goodness = torch.zeros(N_grid)\n",
    "            for layer_idx in range(1, len(model.layers)):\n",
    "                act = layer_states[layer_idx][:, -1, :]\n",
    "                total_goodness += compute_goodness(act)\n",
    "        \n",
    "        all_goodness.append(total_goodness)\n",
    "    \n",
    "    goodness_matrix = torch.stack(all_goodness, dim=1)\n",
    "    \n",
    "    # Softmax over goodness for probability-like values\n",
    "    probs = torch.softmax(goodness_matrix, dim=1)[:, 1].numpy()\n",
    "    Z = probs.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.7)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    for c, color in enumerate(['blue', 'red']):\n",
    "        mask = y_data == c\n",
    "        ax.scatter(X_data[mask, 0].numpy(), X_data[mask, 1].numpy(), c=color, \n",
    "                   s=15, alpha=0.6, edgecolors='white', linewidths=0.3)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Forward-Forward Decision Boundary')\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "plot_ff_decision_boundary(model, X_data, y_data, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-21"
  },
   "source": [
    "## 10. Hyperparameter Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-22",
   "outputs": [],
   "source": [
    "# Try different architectures and thresholds\n",
    "experiments = [\n",
    "    {'hidden_dims': [8], 'threshold': 1.0},\n",
    "    {'hidden_dims': [16], 'threshold': 2.0},\n",
    "    {'hidden_dims': [8, 8], 'threshold': 1.0},\n",
    "    {'hidden_dims': [16, 16], 'threshold': 2.0},\n",
    "    {'hidden_dims': [32], 'threshold': 3.0},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Hyperparameter exploration...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for exp in experiments:\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = build_ff_soen_model(exp['hidden_dims'], input_dim=4)\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    history = train_forward_forward(\n",
    "        model, X_pos_seq, X_neg_seq,\n",
    "        n_epochs=200, lr=0.02, threshold=exp['threshold'], verbose=False\n",
    "    )\n",
    "    \n",
    "    final_acc = history['accuracy'][-1]\n",
    "    results.append({\n",
    "        'hidden_dims': str(exp['hidden_dims']),\n",
    "        'threshold': exp['threshold'],\n",
    "        'params': n_params,\n",
    "        'accuracy': final_acc,\n",
    "    })\n",
    "    \n",
    "    print(f\"Hidden={exp['hidden_dims']}, θ={exp['threshold']}: \"\n",
    "          f\"Acc={final_acc:.4f}, Params={n_params}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-23"
  },
   "source": [
    "## 11. Compare with Backprop Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-24",
   "outputs": [],
   "source": [
    "def train_backprop_baseline(X_data, y_data, hidden_dims=[16, 16], n_epochs=300, lr=0.02):\n",
    "    \"\"\"Train a SOEN model with standard backpropagation for comparison.\"\"\"\n",
    "    \n",
    "    # Build model (without label embedding, just 2D input)\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=50.0,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = [LayerConfig(layer_id=0, layer_type=\"Input\", params={\"dim\": 2})]\n",
    "    connections = []\n",
    "    \n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "            },\n",
    "        ))\n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "    \n",
    "    # Output layer (2 neurons for classification)\n",
    "    output_id = len(hidden_dims) + 1\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=output_id,\n",
    "        layer_type=\"SingleDendrite\",\n",
    "        params={\n",
    "            \"dim\": 2,\n",
    "            \"solver\": \"FE\",\n",
    "            \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "            \"phi_offset\": 0.2,\n",
    "            \"bias_current\": 1.98,\n",
    "            \"gamma_plus\": 0.0005,\n",
    "            \"gamma_minus\": 1e-6,\n",
    "        },\n",
    "    ))\n",
    "    connections.append(ConnectionConfig(\n",
    "        from_layer=output_id - 1,\n",
    "        to_layer=output_id,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"xavier_uniform\"},\n",
    "    ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    # Prepare data\n",
    "    X_seq = X_data.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "    \n",
    "    # Train with BCE on s1 - s0\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    y_target = y_data.float().unsqueeze(1)\n",
    "    accuracies = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        final_hist, _ = model(X_seq)\n",
    "        output = final_hist[:, -1, :]\n",
    "        logits = (output[:, 1] - output[:, 0]).unsqueeze(1)\n",
    "        \n",
    "        loss = criterion(logits, y_target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            acc = (preds == y_target).float().mean().item()\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "\n",
    "# Train backprop baseline\n",
    "print(\"Training backprop baseline...\")\n",
    "torch.manual_seed(42)\n",
    "backprop_accs = train_backprop_baseline(X_data, y_data, hidden_dims=[16, 16], n_epochs=300)\n",
    "print(f\"Backprop final accuracy: {backprop_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-25",
   "outputs": [],
   "source": [
    "# Compare training curves\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.plot(history['accuracy'], label='Forward-Forward', color='coral', lw=2)\n",
    "ax.plot(backprop_accs, label='Backpropagation', color='steelblue', lw=2)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Forward-Forward vs Backpropagation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Comparison:\")\n",
    "print(f\"  Forward-Forward: {history['accuracy'][-1]:.4f}\")\n",
    "print(f\"  Backpropagation: {backprop_accs[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "id": "cell-26"
  },
   "source": [
    "## 12. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "id": "cell-27",
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS: FORWARD-FORWARD FOR SOEN CLASSIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. PERFORMANCE:\")\n",
    "print(f\"   Forward-Forward accuracy: {history['accuracy'][-1]:.4f}\")\n",
    "print(f\"   Backpropagation accuracy: {backprop_accs[-1]:.4f}\")\n",
    "\n",
    "print(f\"\\n2. GOODNESS SEPARATION:\")\n",
    "print(f\"   Mean positive goodness: {history['goodness_pos'][-1]:.2f}\")\n",
    "print(f\"   Mean negative goodness: {history['goodness_neg'][-1]:.2f}\")\n",
    "print(f\"   Threshold: {THRESHOLD}\")\n",
    "\n",
    "print(f\"\\n3. HARDWARE COMPATIBILITY:\")\n",
    "print(f\"   ✓ No backward pass required\")\n",
    "print(f\"   ✓ Local learning rules (per-layer)\")\n",
    "print(f\"   ✓ No surrogate gradients through SOEN dynamics\")\n",
    "print(f\"   ✓ Goodness = sum of squared currents (measurable!)\")\n",
    "\n",
    "print(f\"\\n4. INFERENCE COST:\")\n",
    "print(f\"   ⚠ Requires {N_CLASSES} forward passes (one per class hypothesis)\")\n",
    "print(f\"   ⚠ Compare goodness across hypotheses to classify\")\n",
    "\n",
    "print(f\"\\n5. POTENTIAL HARDWARE IMPLEMENTATION:\")\n",
    "print(f\"   - Goodness = power measurement (I²R)\")\n",
    "print(f\"   - Label embedding = optical input modulation\")\n",
    "print(f\"   - Local updates = per-synapse plasticity rules\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
