{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 02-FF ‚Äî Train a SOEN Model with Forward-Forward Algorithm\n",
    "\n",
    "This tutorial demonstrates training a SOEN model using the **Forward-Forward (FF) algorithm** (Hinton, 2022) instead of backpropagation.\n",
    "\n",
    "## Why Forward-Forward for SOEN?\n",
    "\n",
    "The FF algorithm is remarkably well-suited for SOEN hardware because:\n",
    "- **No backward pass needed** ‚Äî Only forward passes through the network\n",
    "- **Local objectives** ‚Äî Each layer learns independently\n",
    "- **Designed for analog hardware** ‚Äî Works with unknown non-linearities\n",
    "- **Potential for on-chip learning** ‚Äî Could enable hardware-based training\n",
    "\n",
    "## Hardware/Software Split\n",
    "\n",
    "**IMPORTANT**: This notebook uses FF as a SOFTWARE-FLEXIBLE training method.\n",
    "The HARDWARE-FIXED SOEN dynamics (`ds/dt = Œ≥‚Å∫g(œÜ) - Œ≥‚Åªs`) remain unchanged.\n",
    "\n",
    "| Component | Classification |\n",
    "|-----------|----------------|\n",
    "| SOEN dynamics (ODE) | üîí Hardware-Fixed |\n",
    "| Source function g(œÜ) | üîí Hardware-Fixed |\n",
    "| Goodness function (Œ£s¬≤) | üü¢ Software-Flexible |\n",
    "| Layer normalization | üü¢ Software-Flexible |\n",
    "| FF training loop | üü¢ Software-Flexible |\n",
    "\n",
    "## ML Task Overview\n",
    "\n",
    "Same as Tutorial 02: Binary classification on time-series inputs:\n",
    "- Class 0: Input contains a single pulse\n",
    "- Class 1: Input contains two distinct pulses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# Setup: Ensure soen_toolkit is importable\nimport sys\nfrom pathlib import Path\n\n# Add src directory to path if running from notebook location\nnotebook_dir = Path.cwd()\nfor parent in [notebook_dir] + list(notebook_dir.parents):\n    candidate = parent / \"src\"\n    if (candidate / \"soen_toolkit\").exists():\n        sys.path.insert(0, str(candidate))\n        break\n\nfrom typing import Dict, List, Tuple, Optional\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nfrom tqdm.auto import tqdm\n\n# HARDWARE-FIXED: Import existing SOEN model (do NOT modify these)\nfrom soen_toolkit.core import SOENModelCore\nfrom soen_toolkit.core.model_yaml import build_model_from_yaml\n\n# Set random seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n\nprint(\"Setup complete!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Forward-Forward Algorithm Components (Software-Flexible)\n",
    "\n",
    "These are the FF-specific components that we ADD to the existing SOEN framework.\n",
    "They do NOT modify any hardware-fixed code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SOFTWARE-FLEXIBLE: Forward-Forward Components\n",
    "# These are training constructs, not physics.\n",
    "# ============================================================================\n",
    "\n",
    "class GoodnessFunction(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute goodness from SOEN layer states.\n",
    "    \n",
    "    Goodness = sum of squared activities (Hinton, 2022)\n",
    "    This is a SOFTWARE-FLEXIBLE training objective.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def forward(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute goodness from layer states.\n",
    "        \n",
    "        Args:\n",
    "            states: [batch, time, neurons] or [batch, neurons]\n",
    "        \n",
    "        Returns:\n",
    "            goodness: [batch] - sum of squared activities\n",
    "        \"\"\"\n",
    "        # If we have time dimension, use final timestep\n",
    "        if states.dim() == 3:\n",
    "            states = states[:, -1, :]  # [batch, neurons]\n",
    "        \n",
    "        # Goodness = sum of squared states\n",
    "        goodness = torch.sum(states ** 2, dim=-1)  # [batch]\n",
    "        return goodness\n",
    "    \n",
    "    def loss(self, goodness: torch.Tensor, is_positive: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        FF loss: maximize goodness for positive, minimize for negative.\n",
    "        \n",
    "        Args:\n",
    "            goodness: [batch] - computed goodness values\n",
    "            is_positive: [batch] - 1.0 for positive samples, 0.0 for negative\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar BCE loss\n",
    "        \"\"\"\n",
    "        # p(positive) = sigmoid(goodness - threshold)\n",
    "        logits = goodness - self.threshold\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, is_positive)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FFLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization for Forward-Forward algorithm.\n",
    "    \n",
    "    Normalizes to unit length, removing goodness information\n",
    "    while preserving orientation (relative activities).\n",
    "    \n",
    "    This is a SOFTWARE-FLEXIBLE training technique.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize input to unit length.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, neurons] - layer activations\n",
    "        \n",
    "        Returns:\n",
    "            normalized: [batch, neurons] - unit length vectors\n",
    "        \"\"\"\n",
    "        norm = torch.sqrt(torch.sum(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return x / norm\n",
    "\n",
    "\n",
    "print(\"FF components defined (Software-Flexible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "Load the same pulse classification dataset used in Tutorial 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "DATA_PATH = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n",
    "\n",
    "def load_pulse_dataset(path: Path, split: str = \"train\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Load pulse classification dataset.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to HDF5 file\n",
    "        split: 'train', 'val', or 'test'\n",
    "    \n",
    "    Returns:\n",
    "        data: [N, T, D] input sequences\n",
    "        labels: [N] class labels (0 or 1)\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        data = torch.tensor(f[split]['data'][:], dtype=torch.float32)\n",
    "        labels = torch.tensor(f[split]['labels'][:], dtype=torch.long)\n",
    "    return data, labels\n",
    "\n",
    "# Load datasets\n",
    "if DATA_PATH.exists():\n",
    "    train_data, train_labels = load_pulse_dataset(DATA_PATH, 'train')\n",
    "    val_data, val_labels = load_pulse_dataset(DATA_PATH, 'val')\n",
    "    \n",
    "    print(f\"Train: {train_data.shape}, labels: {train_labels.shape}\")\n",
    "    print(f\"Val: {val_data.shape}, labels: {val_labels.shape}\")\n",
    "    print(f\"Unique labels: {torch.unique(train_labels).tolist()}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {DATA_PATH}\")\n",
    "    print(\"Please ensure the dataset exists or update the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Build SOEN Model (Hardware-Fixed)\n",
    "\n",
    "Load the same model architecture used in Tutorial 02.\n",
    "**The model dynamics are HARDWARE-FIXED and unchanged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HARDWARE-FIXED: Load existing SOEN model\n",
    "# We use the model AS-IS, only adding FF training on top\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_PATH = Path(\"training/test_models/model_specs/1D_5D_2D_PulseNetSpec.yaml\")\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    # Load the model - this uses hardware-fixed SOEN dynamics\n",
    "    model = build_model_from_yaml(MODEL_PATH)\n",
    "    \n",
    "    # Update dt to match training config\n",
    "    model.dt = 779\n",
    "    \n",
    "    print(\"Model Architecture:\")\n",
    "    print(f\"  Layers: {len(model.layers)}\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(f\"    Layer {i}: {type(layer).__name__}\")\n",
    "    print(f\"  Connections: {list(model.connections.keys())}\")\n",
    "    \n",
    "    # The model uses these hardware-fixed components:\n",
    "    # - SingleDendrite dynamics: ds/dt = gamma_plus * g(phi) - gamma_minus * s\n",
    "    # - Source function g(phi): Heaviside_fit_state_dep\n",
    "    # These are NOT modified by FF training\n",
    "else:\n",
    "    print(f\"Model spec not found at {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Forward-Forward Training Implementation\n",
    "\n",
    "This is the core FF training logic. Key points:\n",
    "1. **Embed label into input** - Following Hinton's supervised FF approach\n",
    "2. **Positive pass** - Run with correct label, maximize goodness\n",
    "3. **Negative pass** - Run with wrong label, minimize goodness\n",
    "4. **Layer-wise learning** - Each layer learns independently\n",
    "\n",
    "**CRITICAL**: We WRAP the existing SOEN model, we don't modify its dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFSOENTrainer:\n",
    "    \"\"\"\n",
    "    Forward-Forward trainer for SOEN models.\n",
    "    \n",
    "    This trainer:\n",
    "    - WRAPS the existing SOEN model (hardware-fixed dynamics unchanged)\n",
    "    - ADDS FF-specific components (goodness, layer norm) for training\n",
    "    - Uses two forward passes instead of backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SOENModelCore,\n",
    "        num_classes: int = 2,\n",
    "        goodness_threshold: float = 2.0,\n",
    "        learning_rate: float = 0.001,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # SOFTWARE-FLEXIBLE: FF components\n",
    "        self.goodness_fn = GoodnessFunction(threshold=goodness_threshold)\n",
    "        self.layer_norm = FFLayerNorm()\n",
    "        \n",
    "        # Optimizer for model weights\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Track metrics\n",
    "        self.train_losses = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def create_ff_input(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        use_correct_label: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create FF input by embedding label into the input signal.\n",
    "        \n",
    "        For this task, we scale the input based on the label:\n",
    "        - This embeds label information into the input magnitude\n",
    "        - Positive data: input with correct label scaling\n",
    "        - Negative data: input with wrong label scaling\n",
    "        \n",
    "        Args:\n",
    "            data: [batch, time, 1] - raw input signals\n",
    "            labels: [batch] - true class labels\n",
    "            use_correct_label: True for positive pass, False for negative\n",
    "        \n",
    "        Returns:\n",
    "            ff_input: [batch, time, 1] - input with embedded label\n",
    "        \"\"\"\n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        if use_correct_label:\n",
    "            # Positive: use true labels\n",
    "            label_scale = labels.float()\n",
    "        else:\n",
    "            # Negative: use wrong labels (flip 0<->1 for binary)\n",
    "            label_scale = 1.0 - labels.float()\n",
    "        \n",
    "        # Embed label as a bias added to the input\n",
    "        # This is a simple embedding; more sophisticated methods exist\n",
    "        label_bias = label_scale.view(batch_size, 1, 1) * 0.1\n",
    "        ff_input = data + label_bias\n",
    "        \n",
    "        return ff_input\n",
    "    \n",
    "    def forward_pass(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run forward pass through SOEN model.\n",
    "        \n",
    "        HARDWARE-FIXED: Uses existing model dynamics (unchanged)\n",
    "        SOFTWARE-FLEXIBLE: Extracts states for goodness computation\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, time, input_dim] - input sequences\n",
    "        \n",
    "        Returns:\n",
    "            hidden_states: [batch, neurons] - final hidden layer states\n",
    "            output: [batch, output_dim] - model output\n",
    "        \"\"\"\n",
    "        # Run through existing SOEN model (HARDWARE-FIXED dynamics)\n",
    "        self.model.set_tracking(track_s=True)  # Track states for goodness\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            output, all_histories = self.model(x)\n",
    "        \n",
    "        # Extract hidden layer states (layer 1 = SingleDendrite)\n",
    "        # all_histories[0] = input layer, all_histories[1] = hidden layer\n",
    "        if len(all_histories) > 1:\n",
    "            hidden_states = all_histories[1][:, -1, :]  # Final timestep\n",
    "        else:\n",
    "            hidden_states = output[:, -1, :] if output.dim() == 3 else output\n",
    "        \n",
    "        return hidden_states, output\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        One FF training step.\n",
    "        \n",
    "        1. Positive pass: input with correct label -> maximize goodness\n",
    "        2. Negative pass: input with wrong label -> minimize goodness\n",
    "        \n",
    "        Args:\n",
    "            data: [batch, time, 1] - input sequences\n",
    "            labels: [batch] - class labels\n",
    "        \n",
    "        Returns:\n",
    "            metrics: dict with loss values\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        # === POSITIVE PASS ===\n",
    "        pos_input = self.create_ff_input(data, labels, use_correct_label=True)\n",
    "        pos_hidden, _ = self.forward_pass(pos_input)\n",
    "        pos_goodness = self.goodness_fn(pos_hidden)\n",
    "        \n",
    "        # === NEGATIVE PASS ===\n",
    "        neg_input = self.create_ff_input(data, labels, use_correct_label=False)\n",
    "        neg_hidden, _ = self.forward_pass(neg_input)\n",
    "        neg_goodness = self.goodness_fn(neg_hidden)\n",
    "        \n",
    "        # === COMPUTE FF LOSS ===\n",
    "        # Combine positive and negative samples\n",
    "        all_goodness = torch.cat([pos_goodness, neg_goodness], dim=0)\n",
    "        is_positive = torch.cat([\n",
    "            torch.ones(batch_size, device=self.device),\n",
    "            torch.zeros(batch_size, device=self.device)\n",
    "        ], dim=0)\n",
    "        \n",
    "        loss = self.goodness_fn.loss(all_goodness, is_positive)\n",
    "        \n",
    "        # === BACKWARD (through goodness, not through layers) ===\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"pos_goodness\": pos_goodness.mean().item(),\n",
    "            \"neg_goodness\": neg_goodness.mean().item(),\n",
    "        }\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate using FF classification strategy.\n",
    "        \n",
    "        For each sample, run with each possible label and\n",
    "        pick the label that gives highest goodness.\n",
    "        \n",
    "        Args:\n",
    "            data: [batch, time, 1] - input sequences\n",
    "            labels: [batch] - true labels\n",
    "        \n",
    "        Returns:\n",
    "            accuracy: float\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(data)):\n",
    "                sample = data[i:i+1]  # [1, time, 1]\n",
    "                true_label = labels[i].item()\n",
    "                \n",
    "                # Test each possible label\n",
    "                goodness_per_label = []\n",
    "                for test_label in range(self.num_classes):\n",
    "                    test_labels = torch.tensor([test_label], device=self.device)\n",
    "                    ff_input = self.create_ff_input(\n",
    "                        sample,\n",
    "                        test_labels,\n",
    "                        use_correct_label=True  # Treat as if this is the \"correct\" label\n",
    "                    )\n",
    "                    hidden, _ = self.forward_pass(ff_input)\n",
    "                    goodness = self.goodness_fn(hidden).item()\n",
    "                    goodness_per_label.append(goodness)\n",
    "                \n",
    "                # Predict label with highest goodness\n",
    "                predicted = np.argmax(goodness_per_label)\n",
    "                \n",
    "                if predicted == true_label:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"FFSOENTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"goodness_threshold\": 2.0,\n",
    "    \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "def create_dataloader(data, labels, batch_size, shuffle=True):\n",
    "    \"\"\"Simple dataloader generator.\"\"\"\n",
    "    dataset_size = len(data)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in range(0, dataset_size, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_size)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield data[batch_indices], labels[batch_indices]\n",
    "\n",
    "print(f\"Training config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer (only if data and model are available)\n",
    "if 'model' in dir() and 'train_data' in dir():\n",
    "    # Rebuild model for fresh training\n",
    "    model = build_model_from_yaml(MODEL_PATH)\n",
    "    model.dt = 779\n",
    "    \n",
    "    trainer = FFSOENTrainer(\n",
    "        model=model,\n",
    "        num_classes=2,\n",
    "        goodness_threshold=CONFIG[\"goodness_threshold\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        device=CONFIG[\"device\"],\n",
    "    )\n",
    "    \n",
    "    # Move data to device\n",
    "    train_data_device = train_data.to(CONFIG[\"device\"])\n",
    "    train_labels_device = train_labels.to(CONFIG[\"device\"])\n",
    "    val_data_device = val_data.to(CONFIG[\"device\"])\n",
    "    val_labels_device = val_labels.to(CONFIG[\"device\"])\n",
    "    \n",
    "    print(\"Trainer initialized\")\n",
    "    print(f\"Training samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "else:\n",
    "    print(\"Model or data not available. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if 'trainer' in dir():\n",
    "    print(\"Starting Forward-Forward Training\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Note: FF uses local objectives per layer, not global backprop.\")\n",
    "    print(\"The SOEN dynamics (ds/dt = gamma+ * g(phi) - gamma- * s) are UNCHANGED.\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"pos_goodness\": [],\n",
    "        \"neg_goodness\": [],\n",
    "        \"val_accuracy\": [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        epoch_losses = []\n",
    "        epoch_pos_goodness = []\n",
    "        epoch_neg_goodness = []\n",
    "        \n",
    "        # Training\n",
    "        for batch_data, batch_labels in create_dataloader(\n",
    "            train_data_device, train_labels_device, CONFIG[\"batch_size\"]\n",
    "        ):\n",
    "            metrics = trainer.train_step(batch_data, batch_labels)\n",
    "            epoch_losses.append(metrics[\"loss\"])\n",
    "            epoch_pos_goodness.append(metrics[\"pos_goodness\"])\n",
    "            epoch_neg_goodness.append(metrics[\"neg_goodness\"])\n",
    "        \n",
    "        # Validation (subsample for speed)\n",
    "        val_subset = min(200, len(val_data_device))\n",
    "        val_acc = trainer.evaluate(\n",
    "            val_data_device[:val_subset],\n",
    "            val_labels_device[:val_subset]\n",
    "        )\n",
    "        \n",
    "        # Record history\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        avg_pos = np.mean(epoch_pos_goodness)\n",
    "        avg_neg = np.mean(epoch_neg_goodness)\n",
    "        \n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"pos_goodness\"].append(avg_pos)\n",
    "        history[\"neg_goodness\"].append(avg_neg)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']} | \"\n",
    "              f\"Loss: {avg_loss:.4f} | \"\n",
    "              f\"Pos G: {avg_pos:.2f} | \"\n",
    "              f\"Neg G: {avg_neg:.2f} | \"\n",
    "              f\"Val Acc: {val_acc*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "else:\n",
    "    print(\"Trainer not initialized. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'history' in dir() and len(history[\"train_loss\"]) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(history[\"train_loss\"], 'b-', linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"FF Loss\")\n",
    "    ax1.set_title(\"Training Loss\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Goodness\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(history[\"pos_goodness\"], 'g-', linewidth=2, label=\"Positive\")\n",
    "    ax2.plot(history[\"neg_goodness\"], 'r-', linewidth=2, label=\"Negative\")\n",
    "    ax2.axhline(y=CONFIG[\"goodness_threshold\"], color='k', linestyle='--', \n",
    "                label=f\"Threshold ({CONFIG['goodness_threshold']})\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Goodness\")\n",
    "    ax2.set_title(\"Goodness (Positive vs Negative)\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Accuracy\n",
    "    ax3 = axes[2]\n",
    "    ax3.plot([a * 100 for a in history[\"val_accuracy\"]], 'purple', linewidth=2)\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Accuracy (%)\")\n",
    "    ax3.set_title(\"Validation Accuracy\")\n",
    "    ax3.set_ylim([0, 100])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Final Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {history['val_accuracy'][-1]*100:.1f}%\")\n",
    "    print(f\"Final Positive Goodness: {history['pos_goodness'][-1]:.2f}\")\n",
    "    print(f\"Final Negative Goodness: {history['neg_goodness'][-1]:.2f}\")\n",
    "    print(f\"Goodness Gap: {history['pos_goodness'][-1] - history['neg_goodness'][-1]:.2f}\")\n",
    "else:\n",
    "    print(\"No training history available.\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "wsunaz73jbi",
   "source": "# ============================================================================\n# DETAILED EVALUATION: Sample predictions and confusion matrix\n# ============================================================================\n\nif 'trainer' in dir() and 'val_data_device' in dir():\n    from sklearn.metrics import confusion_matrix\n    \n    # Evaluate on full validation set (or subset for speed)\n    eval_size = min(200, len(val_data_device))\n    eval_data = val_data_device[:eval_size]\n    eval_labels = val_labels_device[:eval_size]\n    \n    # Get predictions using FF strategy (highest goodness wins)\n    predictions = []\n    goodness_values = []\n    \n    trainer.model.eval()\n    with torch.no_grad():\n        for i in range(len(eval_data)):\n            sample = eval_data[i:i+1]\n            \n            # Test each label\n            sample_goodness = []\n            for test_label in range(trainer.num_classes):\n                test_labels = torch.tensor([test_label], device=trainer.device)\n                ff_input = trainer.create_ff_input(sample, test_labels, use_correct_label=True)\n                hidden, _ = trainer.forward_pass(ff_input)\n                goodness = trainer.goodness_fn(hidden).item()\n                sample_goodness.append(goodness)\n            \n            predictions.append(np.argmax(sample_goodness))\n            goodness_values.append(sample_goodness)\n    \n    predictions = np.array(predictions)\n    true_labels = eval_labels.cpu().numpy()\n    \n    # Calculate accuracy\n    accuracy = (predictions == true_labels).mean() * 100\n    print(f\"Evaluation Accuracy: {accuracy:.1f}% ({(predictions == true_labels).sum()}/{len(predictions)})\")\n    \n    # ---- Visualize sample predictions ----\n    fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n    \n    for i, ax in enumerate(axes.flatten()):\n        if i >= len(eval_data):\n            break\n        \n        # Plot input signal\n        signal = eval_data[i, :, 0].cpu().numpy()\n        ax.plot(signal, 'b-', linewidth=1.5)\n        \n        true_label = true_labels[i]\n        pred_label = predictions[i]\n        \n        color = 'green' if true_label == pred_label else 'red'\n        ax.set_title(f\"True: {true_label}, Pred: {pred_label}\\nG0={goodness_values[i][0]:.2f}, G1={goodness_values[i][1]:.2f}\", \n                    color=color, fontsize=10)\n        ax.set_xlabel(\"Time\")\n        ax.set_ylabel(\"Input\")\n        ax.grid(True, alpha=0.3)\n    \n    plt.suptitle(f\"FF Sample Predictions (Accuracy: {accuracy:.1f}%)\", fontsize=14)\n    plt.tight_layout()\n    plt.show()\n    \n    # ---- Confusion Matrix ----\n    cm = confusion_matrix(true_labels, predictions)\n    \n    fig, ax = plt.subplots(figsize=(6, 5))\n    im = ax.imshow(cm, cmap='Blues')\n    ax.set_xlabel('Predicted Label')\n    ax.set_ylabel('True Label')\n    ax.set_title('FF Confusion Matrix')\n    ax.set_xticks([0, 1])\n    ax.set_yticks([0, 1])\n    ax.set_xticklabels(['0 (1 pulse)', '1 (2 pulses)'])\n    ax.set_yticklabels(['0 (1 pulse)', '1 (2 pulses)'])\n    \n    # Add text annotations\n    for i in range(2):\n        for j in range(2):\n            ax.text(j, i, str(cm[i, j]), ha='center', va='center', \n                   color='white' if cm[i, j] > cm.max()/2 else 'black', fontsize=16)\n    \n    plt.colorbar(im)\n    plt.tight_layout()\n    plt.show()\n    \n    # ---- Goodness Distribution ----\n    fig, ax = plt.subplots(figsize=(10, 4))\n    \n    goodness_arr = np.array(goodness_values)\n    \n    # Split by true label\n    g0_class0 = goodness_arr[true_labels == 0, 0]  # Goodness for label 0, true class 0\n    g1_class0 = goodness_arr[true_labels == 0, 1]  # Goodness for label 1, true class 0\n    g0_class1 = goodness_arr[true_labels == 1, 0]  # Goodness for label 0, true class 1\n    g1_class1 = goodness_arr[true_labels == 1, 1]  # Goodness for label 1, true class 1\n    \n    x = np.arange(2)\n    width = 0.35\n    \n    ax.bar(x - width/2, [g0_class0.mean(), g0_class1.mean()], width, label='G(label=0)', color='blue', alpha=0.7)\n    ax.bar(x + width/2, [g1_class0.mean(), g1_class1.mean()], width, label='G(label=1)', color='orange', alpha=0.7)\n    \n    ax.axhline(y=CONFIG[\"goodness_threshold\"], color='red', linestyle='--', label=f'Threshold ({CONFIG[\"goodness_threshold\"]})')\n    \n    ax.set_xlabel('True Class')\n    ax.set_ylabel('Mean Goodness')\n    ax.set_title('FF Goodness by True Class and Tested Label')\n    ax.set_xticks(x)\n    ax.set_xticklabels(['Class 0 (1 pulse)', 'Class 1 (2 pulses)'])\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nInterpretation:\")\n    print(\"- For correct classification: G(correct_label) > G(wrong_label)\")\n    print(\"- Class 0 samples should have higher G(label=0)\")\n    print(\"- Class 1 samples should have higher G(label=1)\")\nelse:\n    print(\"Trainer not available. Run training first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Comparison: FF vs Backpropagation\n",
    "\n",
    "| Aspect | Backpropagation (Tutorial 02) | Forward-Forward (This Tutorial) |\n",
    "|--------|------------------------------|--------------------------------|\n",
    "| **Training Algorithm** | Global loss + backward pass | Local goodness + two forward passes |\n",
    "| **Gradient Computation** | Requires surrogate gradients | No gradients through spike |\n",
    "| **Hardware Compatibility** | Low (needs backprop circuit) | High (forward passes only) |\n",
    "| **SOEN Dynamics** | Unchanged | Unchanged |\n",
    "| **On-Chip Learning** | Not possible | Potentially possible |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### Hardware/Software Split Preserved\n",
    "\n",
    "1. **HARDWARE-FIXED (Unchanged)**:\n",
    "   - Dendritic dynamics: `ds/dt = Œ≥‚Å∫¬∑g(œÜ) - Œ≥‚Åª¬∑s`\n",
    "   - Source function: `g(œÜ)` lookup\n",
    "   - Spike mechanism\n",
    "   - Physical constants\n",
    "\n",
    "2. **SOFTWARE-FLEXIBLE (New for FF)**:\n",
    "   - Goodness function: `Œ£s¬≤`\n",
    "   - Layer normalization\n",
    "   - Positive/negative data generation\n",
    "   - FF training loop\n",
    "\n",
    "### FF Advantages for SOEN\n",
    "\n",
    "- No backpropagation through complex g(œÜ)\n",
    "- Local learning per layer\n",
    "- Could enable on-chip learning\n",
    "- Works with unknown non-linearities\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Tune goodness threshold for your task\n",
    "- Try different label embedding strategies\n",
    "- Compare accuracy with backprop baseline\n",
    "- Explore FF for larger SOEN networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Hinton, G. (2022). The Forward-Forward Algorithm: Some Preliminary Investigations. arXiv:2212.13345\n",
    "- See `reports/forward_forward_soen_analysis.md` for detailed analysis\n",
    "- Compare with `02_train_a_model.ipynb` for backpropagation approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}