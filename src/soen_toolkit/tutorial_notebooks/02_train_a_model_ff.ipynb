{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Tutorial 02-FF \u2014 Train a SOEN Model with Forward-Forward Algorithm\n",
    "\n",
    "This tutorial demonstrates training a SOEN model using the **Forward-Forward (FF) algorithm** (Hinton, 2022) instead of backpropagation.\n",
    "\n",
    "## Why Forward-Forward for SOEN?\n",
    "\n",
    "The FF algorithm is remarkably well-suited for SOEN hardware because:\n",
    "- **No backward pass needed** \u2014 Only forward passes through the network\n",
    "- **Local objectives** \u2014 Each layer learns independently\n",
    "- **Designed for analog hardware** \u2014 Works with unknown non-linearities\n",
    "- **Potential for on-chip learning** \u2014 Could enable hardware-based training\n",
    "\n",
    "## Hardware/Software Split\n",
    "\n",
    "**IMPORTANT**: This notebook uses FF as a SOFTWARE-FLEXIBLE training method.\n",
    "The HARDWARE-FIXED SOEN dynamics (`ds/dt = \u03b3\u207ag(\u03c6) - \u03b3\u207bs`) remain unchanged.\n",
    "\n",
    "| Component | Classification |\n",
    "|-----------|----------------|\n",
    "| SOEN dynamics (ODE) | \ud83d\udd12 Hardware-Fixed |\n",
    "| Source function g(\u03c6) | \ud83d\udd12 Hardware-Fixed |\n",
    "| Goodness function (\u03a3s\u00b2) | \ud83d\udfe2 Software-Flexible |\n",
    "| Layer normalization | \ud83d\udfe2 Software-Flexible |\n",
    "| FF training loop | \ud83d\udfe2 Software-Flexible |\n",
    "\n",
    "## ML Task Overview\n",
    "\n",
    "Same as Tutorial 02: Binary classification on time-series inputs:\n",
    "- Class 0: Input contains a single pulse\n",
    "- Class 1: Input contains two distinct pulses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# HARDWARE-FIXED: Import existing SOEN model (do NOT modify these)\n",
    "from soen_toolkit.core import SOENModelCore\n",
    "from soen_toolkit.core.model_yaml import build_model_from_yaml\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 2. Forward-Forward Algorithm Components (Software-Flexible)\n",
    "\n",
    "These are the FF-specific components that we ADD to the existing SOEN framework.\n",
    "They do NOT modify any hardware-fixed code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SOFTWARE-FLEXIBLE: Forward-Forward Components\n",
    "# These are training constructs, not physics.\n",
    "# ============================================================================\n",
    "\n",
    "class GoodnessFunction(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute goodness from SOEN layer states.\n",
    "    \n",
    "    Goodness = sum of squared activities (Hinton, 2022)\n",
    "    This is a SOFTWARE-FLEXIBLE training objective.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def forward(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute goodness from layer states.\n",
    "        \n",
    "        Args:\n",
    "            states: [batch, time, neurons] or [batch, neurons]\n",
    "        \n",
    "        Returns:\n",
    "            goodness: [batch] - sum of squared activities\n",
    "        \"\"\"\n",
    "        # If we have time dimension, use final timestep\n",
    "        if states.dim() == 3:\n",
    "            states = states[:, -1, :]  # [batch, neurons]\n",
    "        \n",
    "        # Goodness = sum of squared states\n",
    "        goodness = torch.sum(states ** 2, dim=-1)  # [batch]\n",
    "        return goodness\n",
    "    \n",
    "    def loss(self, goodness: torch.Tensor, is_positive: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        FF loss: maximize goodness for positive, minimize for negative.\n",
    "        \n",
    "        Args:\n",
    "            goodness: [batch] - computed goodness values\n",
    "            is_positive: [batch] - 1.0 for positive samples, 0.0 for negative\n",
    "        \n",
    "        Returns:\n",
    "            loss: scalar BCE loss\n",
    "        \"\"\"\n",
    "        # p(positive) = sigmoid(goodness - threshold)\n",
    "        logits = goodness - self.threshold\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, is_positive)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class FFLayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer normalization for Forward-Forward algorithm.\n",
    "    \n",
    "    Normalizes to unit length, removing goodness information\n",
    "    while preserving orientation (relative activities).\n",
    "    \n",
    "    This is a SOFTWARE-FLEXIBLE training technique.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Normalize input to unit length.\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, neurons] - layer activations\n",
    "        \n",
    "        Returns:\n",
    "            normalized: [batch, neurons] - unit length vectors\n",
    "        \"\"\"\n",
    "        norm = torch.sqrt(torch.sum(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        return x / norm\n",
    "\n",
    "\n",
    "print(\"FF components defined (Software-Flexible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "Load the same pulse classification dataset used in Tutorial 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dataset\n",
    "DATA_PATH = Path(\"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\")\n",
    "\n",
    "def load_pulse_dataset(path: Path, split: str = \"train\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Load pulse classification dataset.\n",
    "    \n",
    "    Args:\n",
    "        path: Path to HDF5 file\n",
    "        split: 'train', 'val', or 'test'\n",
    "    \n",
    "    Returns:\n",
    "        data: [N, T, D] input sequences\n",
    "        labels: [N] class labels (0 or 1)\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        data = torch.tensor(f[split]['data'][:], dtype=torch.float32)\n",
    "        labels = torch.tensor(f[split]['labels'][:], dtype=torch.long)\n",
    "    return data, labels\n",
    "\n",
    "# Load datasets\n",
    "if DATA_PATH.exists():\n",
    "    train_data, train_labels = load_pulse_dataset(DATA_PATH, 'train')\n",
    "    val_data, val_labels = load_pulse_dataset(DATA_PATH, 'val')\n",
    "    \n",
    "    print(f\"Train: {train_data.shape}, labels: {train_labels.shape}\")\n",
    "    print(f\"Val: {val_data.shape}, labels: {val_labels.shape}\")\n",
    "    print(f\"Unique labels: {torch.unique(train_labels).tolist()}\")\n",
    "else:\n",
    "    print(f\"Dataset not found at {DATA_PATH}\")\n",
    "    print(\"Please ensure the dataset exists or update the path.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 4. Build SOEN Model (Hardware-Fixed)\n",
    "\n",
    "Load the same model architecture used in Tutorial 02.\n",
    "**The model dynamics are HARDWARE-FIXED and unchanged.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HARDWARE-FIXED: Load existing SOEN model\n",
    "# We use the model AS-IS, only adding FF training on top\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_PATH = Path(\"training/test_models/model_specs/1D_5D_2D_PulseNetSpec.yaml\")\n",
    "\n",
    "if MODEL_PATH.exists():\n",
    "    # Load the model - this uses hardware-fixed SOEN dynamics\n",
    "    model = build_model_from_yaml(MODEL_PATH)\n",
    "    \n",
    "    # Update dt to match training config\n",
    "    model.dt = 779\n",
    "    \n",
    "    print(\"Model Architecture:\")\n",
    "    print(f\"  Layers: {len(model.layers)}\")\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        print(f\"    Layer {i}: {type(layer).__name__}\")\n",
    "    print(f\"  Connections: {list(model.connections.keys())}\")\n",
    "    \n",
    "    # The model uses these hardware-fixed components:\n",
    "    # - SingleDendrite dynamics: ds/dt = gamma_plus * g(phi) - gamma_minus * s\n",
    "    # - Source function g(phi): Heaviside_fit_state_dep\n",
    "    # These are NOT modified by FF training\n",
    "else:\n",
    "    print(f\"Model spec not found at {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 5. Forward-Forward Training Implementation\n",
    "\n",
    "This is the core FF training logic. Key points:\n",
    "1. **Embed label into input** - Following Hinton's supervised FF approach\n",
    "2. **Positive pass** - Run with correct label, maximize goodness\n",
    "3. **Negative pass** - Run with wrong label, minimize goodness\n",
    "4. **Layer-wise learning** - Each layer learns independently\n",
    "\n",
    "**CRITICAL**: We WRAP the existing SOEN model, we don't modify its dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFSOENTrainer:\n",
    "    \"\"\"\n",
    "    Forward-Forward trainer for SOEN models.\n",
    "    \n",
    "    This trainer:\n",
    "    - WRAPS the existing SOEN model (hardware-fixed dynamics unchanged)\n",
    "    - ADDS FF-specific components (goodness, layer norm) for training\n",
    "    - Uses two forward passes instead of backprop\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: SOENModelCore,\n",
    "        num_classes: int = 2,\n",
    "        goodness_threshold: float = 2.0,\n",
    "        learning_rate: float = 0.001,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        \n",
    "        # SOFTWARE-FLEXIBLE: FF components\n",
    "        self.goodness_fn = GoodnessFunction(threshold=goodness_threshold)\n",
    "        self.layer_norm = FFLayerNorm()\n",
    "        \n",
    "        # Optimizer for model weights\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Track metrics\n",
    "        self.train_losses = []\n",
    "        self.val_accuracies = []\n",
    "    \n",
    "    def create_ff_input(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "        use_correct_label: bool = True,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create FF input by embedding label into the input signal.\n",
    "        \n",
    "        For this task, we scale the input based on the label:\n",
    "        - This embeds label information into the input magnitude\n",
    "        - Positive data: input with correct label scaling\n",
    "        - Negative data: input with wrong label scaling\n",
    "        \n",
    "        Args:\n",
    "            data: [batch, time, 1] - raw input signals\n",
    "            labels: [batch] - true class labels\n",
    "            use_correct_label: True for positive pass, False for negative\n",
    "        \n",
    "        Returns:\n",
    "            ff_input: [batch, time, 1] - input with embedded label\n",
    "        \"\"\"\n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        if use_correct_label:\n",
    "            # Positive: use true labels\n",
    "            label_scale = labels.float()\n",
    "        else:\n",
    "            # Negative: use wrong labels (flip 0<->1 for binary)\n",
    "            label_scale = 1.0 - labels.float()\n",
    "        \n",
    "        # Embed label as a bias added to the input\n",
    "        # This is a simple embedding; more sophisticated methods exist\n",
    "        label_bias = label_scale.view(batch_size, 1, 1) * 0.1\n",
    "        ff_input = data + label_bias\n",
    "        \n",
    "        return ff_input\n",
    "    \n",
    "    def forward_pass(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Run forward pass through SOEN model.\n",
    "        \n",
    "        HARDWARE-FIXED: Uses existing model dynamics (unchanged)\n",
    "        SOFTWARE-FLEXIBLE: Extracts states for goodness computation\n",
    "        \n",
    "        Args:\n",
    "            x: [batch, time, input_dim] - input sequences\n",
    "        \n",
    "        Returns:\n",
    "            hidden_states: [batch, neurons] - final hidden layer states\n",
    "            output: [batch, output_dim] - model output\n",
    "        \"\"\"\n",
    "        # Run through existing SOEN model (HARDWARE-FIXED dynamics)\n",
    "        self.model.set_tracking(track_s=True)  # Track states for goodness\n",
    "        \n",
    "        with torch.enable_grad():\n",
    "            output, all_histories = self.model(x)\n",
    "        \n",
    "        # Extract hidden layer states (layer 1 = SingleDendrite)\n",
    "        # all_histories[0] = input layer, all_histories[1] = hidden layer\n",
    "        if len(all_histories) > 1:\n",
    "            hidden_states = all_histories[1][:, -1, :]  # Final timestep\n",
    "        else:\n",
    "            hidden_states = output[:, -1, :] if output.dim() == 3 else output\n",
    "        \n",
    "        return hidden_states, output\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        One FF training step.\n",
    "        \n",
    "        1. Positive pass: input with correct label -> maximize goodness\n",
    "        2. Negative pass: input with wrong label -> minimize goodness\n",
    "        \n",
    "        Args:\n",
    "            data: [batch, time, 1] - input sequences\n",
    "            labels: [batch] - class labels\n",
    "        \n",
    "        Returns:\n",
    "            metrics: dict with loss values\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = data.shape[0]\n",
    "        \n",
    "        # === POSITIVE PASS ===\n",
    "        pos_input = self.create_ff_input(data, labels, use_correct_label=True)\n",
    "        pos_hidden, _ = self.forward_pass(pos_input)\n",
    "        pos_goodness = self.goodness_fn(pos_hidden)\n",
    "        \n",
    "        # === NEGATIVE PASS ===\n",
    "        neg_input = self.create_ff_input(data, labels, use_correct_label=False)\n",
    "        neg_hidden, _ = self.forward_pass(neg_input)\n",
    "        neg_goodness = self.goodness_fn(neg_hidden)\n",
    "        \n",
    "        # === COMPUTE FF LOSS ===\n",
    "        # Combine positive and negative samples\n",
    "        all_goodness = torch.cat([pos_goodness, neg_goodness], dim=0)\n",
    "        is_positive = torch.cat([\n",
    "            torch.ones(batch_size, device=self.device),\n",
    "            torch.zeros(batch_size, device=self.device)\n",
    "        ], dim=0)\n",
    "        \n",
    "        loss = self.goodness_fn.loss(all_goodness, is_positive)\n",
    "        \n",
    "        # === BACKWARD (through goodness, not through layers) ===\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            \"loss\": loss.item(),\n",
    "            \"pos_goodness\": pos_goodness.mean().item(),\n",
    "            \"neg_goodness\": neg_goodness.mean().item(),\n",
    "        }\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        data: torch.Tensor,\n",
    "        labels: torch.Tensor,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate using FF classification strategy.\n",
    "        \n",
    "        For each sample, run with each possible label and\n",
    "        pick the label that gives highest goodness.\n",
    "        \n",
    "        Args:\n",
    "            data: [batch, time, 1] - input sequences\n",
    "            labels: [batch] - true labels\n",
    "        \n",
    "        Returns:\n",
    "            accuracy: float\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(data)):\n",
    "                sample = data[i:i+1]  # [1, time, 1]\n",
    "                true_label = labels[i].item()\n",
    "                \n",
    "                # Test each possible label\n",
    "                goodness_per_label = []\n",
    "                for test_label in range(self.num_classes):\n",
    "                    test_labels = torch.tensor([test_label], device=self.device)\n",
    "                    ff_input = self.create_ff_input(\n",
    "                        sample,\n",
    "                        test_labels,\n",
    "                        use_correct_label=True  # Treat as if this is the \"correct\" label\n",
    "                    )\n",
    "                    hidden, _ = self.forward_pass(ff_input)\n",
    "                    goodness = self.goodness_fn(hidden).item()\n",
    "                    goodness_per_label.append(goodness)\n",
    "                \n",
    "                # Predict label with highest goodness\n",
    "                predicted = np.argmax(goodness_per_label)\n",
    "                \n",
    "                if predicted == true_label:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        \n",
    "        return correct / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "print(\"FFSOENTrainer defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"goodness_threshold\": 2.0,\n",
    "    \"device\": \"cpu\",\n",
    "}\n",
    "\n",
    "def create_dataloader(data, labels, batch_size, shuffle=True):\n",
    "    \"\"\"Simple dataloader generator.\"\"\"\n",
    "    dataset_size = len(data)\n",
    "    indices = list(range(dataset_size))\n",
    "    \n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in range(0, dataset_size, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, dataset_size)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield data[batch_indices], labels[batch_indices]\n",
    "\n",
    "print(f\"Training config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer (only if data and model are available)\n",
    "if 'model' in dir() and 'train_data' in dir():\n",
    "    # Rebuild model for fresh training\n",
    "    model = build_model_from_yaml(MODEL_PATH)\n",
    "    model.dt = 779\n",
    "    \n",
    "    trainer = FFSOENTrainer(\n",
    "        model=model,\n",
    "        num_classes=2,\n",
    "        goodness_threshold=CONFIG[\"goodness_threshold\"],\n",
    "        learning_rate=CONFIG[\"learning_rate\"],\n",
    "        device=CONFIG[\"device\"],\n",
    "    )\n",
    "    \n",
    "    # Move data to device\n",
    "    train_data_device = train_data.to(CONFIG[\"device\"])\n",
    "    train_labels_device = train_labels.to(CONFIG[\"device\"])\n",
    "    val_data_device = val_data.to(CONFIG[\"device\"])\n",
    "    val_labels_device = val_labels.to(CONFIG[\"device\"])\n",
    "    \n",
    "    print(\"Trainer initialized\")\n",
    "    print(f\"Training samples: {len(train_data)}\")\n",
    "    print(f\"Validation samples: {len(val_data)}\")\n",
    "else:\n",
    "    print(\"Model or data not available. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "if 'trainer' in dir():\n",
    "    print(\"Starting Forward-Forward Training\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Note: FF uses local objectives per layer, not global backprop.\")\n",
    "    print(\"The SOEN dynamics (ds/dt = gamma+ * g(phi) - gamma- * s) are UNCHANGED.\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"pos_goodness\": [],\n",
    "        \"neg_goodness\": [],\n",
    "        \"val_accuracy\": [],\n",
    "    }\n",
    "    \n",
    "    for epoch in range(CONFIG[\"num_epochs\"]):\n",
    "        epoch_losses = []\n",
    "        epoch_pos_goodness = []\n",
    "        epoch_neg_goodness = []\n",
    "        \n",
    "        # Training\n",
    "        for batch_data, batch_labels in create_dataloader(\n",
    "            train_data_device, train_labels_device, CONFIG[\"batch_size\"]\n",
    "        ):\n",
    "            metrics = trainer.train_step(batch_data, batch_labels)\n",
    "            epoch_losses.append(metrics[\"loss\"])\n",
    "            epoch_pos_goodness.append(metrics[\"pos_goodness\"])\n",
    "            epoch_neg_goodness.append(metrics[\"neg_goodness\"])\n",
    "        \n",
    "        # Validation (subsample for speed)\n",
    "        val_subset = min(200, len(val_data_device))\n",
    "        val_acc = trainer.evaluate(\n",
    "            val_data_device[:val_subset],\n",
    "            val_labels_device[:val_subset]\n",
    "        )\n",
    "        \n",
    "        # Record history\n",
    "        avg_loss = np.mean(epoch_losses)\n",
    "        avg_pos = np.mean(epoch_pos_goodness)\n",
    "        avg_neg = np.mean(epoch_neg_goodness)\n",
    "        \n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"pos_goodness\"].append(avg_pos)\n",
    "        history[\"neg_goodness\"].append(avg_neg)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{CONFIG['num_epochs']} | \"\n",
    "              f\"Loss: {avg_loss:.4f} | \"\n",
    "              f\"Pos G: {avg_pos:.2f} | \"\n",
    "              f\"Neg G: {avg_neg:.2f} | \"\n",
    "              f\"Val Acc: {val_acc*100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "else:\n",
    "    print(\"Trainer not initialized. Please run previous cells.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'history' in dir() and len(history[\"train_loss\"]) > 0:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(history[\"train_loss\"], 'b-', linewidth=2)\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"FF Loss\")\n",
    "    ax1.set_title(\"Training Loss\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Goodness\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(history[\"pos_goodness\"], 'g-', linewidth=2, label=\"Positive\")\n",
    "    ax2.plot(history[\"neg_goodness\"], 'r-', linewidth=2, label=\"Negative\")\n",
    "    ax2.axhline(y=CONFIG[\"goodness_threshold\"], color='k', linestyle='--', \n",
    "                label=f\"Threshold ({CONFIG['goodness_threshold']})\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Goodness\")\n",
    "    ax2.set_title(\"Goodness (Positive vs Negative)\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Accuracy\n",
    "    ax3 = axes[2]\n",
    "    ax3.plot([a * 100 for a in history[\"val_accuracy\"]], 'purple', linewidth=2)\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Accuracy (%)\")\n",
    "    ax3.set_title(\"Validation Accuracy\")\n",
    "    ax3.set_ylim([0, 100])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final results\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Final Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Validation Accuracy: {history['val_accuracy'][-1]*100:.1f}%\")\n",
    "    print(f\"Final Positive Goodness: {history['pos_goodness'][-1]:.2f}\")\n",
    "    print(f\"Final Negative Goodness: {history['neg_goodness'][-1]:.2f}\")\n",
    "    print(f\"Goodness Gap: {history['pos_goodness'][-1] - history['neg_goodness'][-1]:.2f}\")\n",
    "else:\n",
    "    print(\"No training history available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Comparison: FF vs Backpropagation\n",
    "\n",
    "| Aspect | Backpropagation (Tutorial 02) | Forward-Forward (This Tutorial) |\n",
    "|--------|------------------------------|--------------------------------|\n",
    "| **Training Algorithm** | Global loss + backward pass | Local goodness + two forward passes |\n",
    "| **Gradient Computation** | Requires surrogate gradients | No gradients through spike |\n",
    "| **Hardware Compatibility** | Low (needs backprop circuit) | High (forward passes only) |\n",
    "| **SOEN Dynamics** | Unchanged | Unchanged |\n",
    "| **On-Chip Learning** | Not possible | Potentially possible |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Key Takeaways\n",
    "\n",
    "### Hardware/Software Split Preserved\n",
    "\n",
    "1. **HARDWARE-FIXED (Unchanged)**:\n",
    "   - Dendritic dynamics: `ds/dt = \u03b3\u207a\u00b7g(\u03c6) - \u03b3\u207b\u00b7s`\n",
    "   - Source function: `g(\u03c6)` lookup\n",
    "   - Spike mechanism\n",
    "   - Physical constants\n",
    "\n",
    "2. **SOFTWARE-FLEXIBLE (New for FF)**:\n",
    "   - Goodness function: `\u03a3s\u00b2`\n",
    "   - Layer normalization\n",
    "   - Positive/negative data generation\n",
    "   - FF training loop\n",
    "\n",
    "### FF Advantages for SOEN\n",
    "\n",
    "- No backpropagation through complex g(\u03c6)\n",
    "- Local learning per layer\n",
    "- Could enable on-chip learning\n",
    "- Works with unknown non-linearities\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Tune goodness threshold for your task\n",
    "- Try different label embedding strategies\n",
    "- Compare accuracy with backprop baseline\n",
    "- Explore FF for larger SOEN networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- Hinton, G. (2022). The Forward-Forward Algorithm: Some Preliminary Investigations. arXiv:2212.13345\n",
    "- See `reports/forward_forward_soen_analysis.md` for detailed analysis\n",
    "- Compare with `02_train_a_model.ipynb` for backpropagation approach"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
