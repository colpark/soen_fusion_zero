{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Tutorial 02-FF â€” Train a SOEN Model with Forward-Forward Learning\n\nThis tutorial demonstrates **Forward-Forward (FF) learning** as an alternative to backpropagation for training SOEN networks.\n\n**Architecture**: Matches `02_train_a_model.ipynb` with **5 hidden neurons** (same as SOEN SingleDendrite layer).\n\n---\n\n## What is Forward-Forward Learning?\n\nForward-Forward (Hinton, 2022) is a **local learning algorithm** that eliminates backpropagation:\n\n| Aspect | Backpropagation | Forward-Forward |\n|--------|-----------------|------------------|\n| **Gradient flow** | Through all layers | None between layers |\n| **Learning signal** | Global error | Local \"goodness\" |\n| **Weight transport** | Required | Not needed |\n| **Hardware fit** | Poor | Good |\n\n---\n\n## How Forward-Forward Works\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    FORWARD-FORWARD LEARNING                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚                                                                 â”‚\nâ”‚  POSITIVE PASS: Input + CORRECT label â†’ Maximize \"goodness\"    â”‚\nâ”‚  NEGATIVE PASS: Input + WRONG label   â†’ Minimize \"goodness\"    â”‚\nâ”‚                                                                 â”‚\nâ”‚  Each layer learns INDEPENDENTLY with its own local objective  â”‚\nâ”‚                                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---\n\n## ML Task: Pulse Classification\n\nSame task as Tutorial 02:\n- **Class 0**: Input contains a single pulse\n- **Class 1**: Input contains two distinct pulses\n\n---\n\n## Architecture Matching\n\n| Component | SOEN (02_train_a_model) | FF (This Tutorial) |\n|-----------|-------------------------|---------------------|\n| Hidden neurons | 5 | **5** |\n| Input | 64 timesteps Ã— 1 | 64 + 2 (label) = 66 |\n| Output | 2 classes | 2 classes |\n\n---\n\n## ðŸ”Š Noise Configuration\n\n| Parameter | Default | Description |\n|-----------|---------|-------------|\n| `phi` | **0.01** | Noise on input flux |\n| `s` | **0.005** | Noise on state |\n| `relative` | **False** | Absolute scaling |"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "We import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 1: IMPORTS AND SETUP\n",
    "# ==============================================================================\n",
    "# This cell sets up the Python environment and imports all required libraries.\n",
    "#\n",
    "# Key imports:\n",
    "#   - torch: PyTorch for tensor operations and neural networks\n",
    "#   - numpy: Numerical operations\n",
    "#   - h5py: Reading HDF5 dataset files\n",
    "#   - matplotlib: Visualization\n",
    "# ==============================================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path if running from notebook location\n",
    "notebook_dir = Path.cwd()\n",
    "for parent in [notebook_dir] + list(notebook_dir.parents):\n",
    "    candidate = parent / \"src\"\n",
    "    if (candidate / \"soen_toolkit\").exists():\n",
    "        sys.path.insert(0, str(candidate))\n",
    "        break\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "We define all hyperparameters and settings in a single configuration class.\n",
    "\n",
    "### Hardware Mode Options\n",
    "\n",
    "| Mode | Description | Accuracy | Hardware Ready |\n",
    "|------|-------------|----------|----------------|\n",
    "| `HARDWARE_MODE = False` | Uses autograd + Adam | Higher | No |\n",
    "| `HARDWARE_MODE = True` | Uses Hebbian learning | Lower | Yes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 2: CONFIGURATION\n# ==============================================================================\n# All hyperparameters and settings are defined here.\n# This makes it easy to experiment with different configurations.\n#\n# IMPORTANT: Architecture matches the non-FF version (02_train_a_model.ipynb)\n#   - Hidden dim = 5 (same as 1D_5D_2D_PulseNetSpec_trainable.yaml)\n#   - Single hidden layer (matching SOEN architecture)\n#\n# Key settings:\n#   - HARDWARE_MODE: If True, uses hardware-compatible Hebbian learning\n#   - NOISE_ENABLED: If True, adds noise during training (more realistic)\n#   - hidden_dim: Number of neurons in hidden layer (5 to match SOEN)\n#   - threshold: Goodness threshold for FF learning\n# ==============================================================================\n\n# Toggle between standard FF (autograd) and hardware-compatible FF (Hebbian)\nHARDWARE_MODE = False  # Set to True for hardware-compatible mode\n\n# Toggle noise injection\nNOISE_ENABLED = True\n\n@dataclass\nclass FFConfig:\n    \"\"\"Configuration for Forward-Forward training on pulse classification.\n    \n    Architecture matches the SOEN model from 02_train_a_model.ipynb:\n        Input (1) â†’ Hidden (5) â†’ Output (2)\n    \n    For FF, we flatten the sequence and concatenate the label:\n        Input: seq_len * input_dim + num_classes = 64 * 1 + 2 = 66\n    \"\"\"\n    \n    # Dataset\n    data_path: str = \"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\"\n    seq_len: int = 64          # Sequence length (timesteps)\n    input_dim: int = 1         # Input features per timestep\n    num_classes: int = 2       # Binary classification (one vs two pulses)\n    \n    # Architecture - MATCHING THE SOEN MODEL (1D â†’ 5D â†’ 2D)\n    # The SOEN model uses dim=5 for the hidden SingleDendrite layer\n    hidden_dim: int = 5        # Hidden layer size (matches SOEN!)\n    num_layers: int = 1        # Number of FF layers (matches single SOEN hidden layer)\n    \n    # Training\n    batch_size: int = 32\n    num_epochs: int = 100      # More epochs needed for smaller network\n    learning_rate: float = 0.03     # For standard mode (Adam)\n    hebbian_lr: float = 0.1         # For hardware mode (Hebbian)\n    \n    # Forward-Forward specific\n    threshold: float = 2.0     # Goodness threshold\n    \n    # Hardware mode settings\n    hardware_mode: bool = HARDWARE_MODE\n    inhibition_strength: float = 0.1  # Lateral inhibition for hardware mode\n    \n    # Noise\n    noise_enabled: bool = NOISE_ENABLED\n    noise_phi: float = 0.01    # Noise on input flux\n    noise_s: float = 0.005     # Noise on state\n    \n    @property\n    def flattened_input_dim(self) -> int:\n        \"\"\"Total input dimension after flattening sequence + adding label.\"\"\"\n        return self.seq_len * self.input_dim + self.num_classes  # 64 + 2 = 66\n\n\n# Create configuration\nconfig = FFConfig()\n\n# Calculate parameter counts for comparison\nff_params = (config.flattened_input_dim * config.hidden_dim +  # Input weights\n             config.hidden_dim +                                # Input biases\n             config.hidden_dim * config.hidden_dim +            # Recurrent (if num_layers > 1)\n             config.hidden_dim)                                 # Additional biases\nsoen_params = (1 * 5 +        # J_0_to_1: 1â†’5\n               5 * 5 - 5 +    # J_1_to_1: 5â†’5 no self = 20\n               2)             # J_1_to_2: one_to_one â‰ˆ 2\n\n# Print configuration\nprint(\"=\"*60)\nprint(\"FORWARD-FORWARD CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"Hardware mode: {config.hardware_mode}\")\nprint(f\"Noise enabled: {config.noise_enabled}\")\nprint(f\"\")\nprint(f\"Dataset: {config.data_path}\")\nprint(f\"Sequence length: {config.seq_len}\")\nprint(f\"Input dim: {config.input_dim}\")\nprint(f\"Flattened + label dim: {config.flattened_input_dim}\")\nprint(f\"\")\nprint(f\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\nprint(f\"â•‘  ARCHITECTURE (matching SOEN model)                      â•‘\")\nprint(f\"â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\")\nprint(f\"â•‘  Hidden dim: {config.hidden_dim:3d} neurons (same as SOEN SingleDendrite) â•‘\")\nprint(f\"â•‘  Num layers: {config.num_layers:3d} (same as SOEN hidden layers)         â•‘\")\nprint(f\"â•‘  Num classes: {config.num_classes:2d}                                      â•‘\")\nprint(f\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\nprint(f\"\")\nprint(f\"Batch size: {config.batch_size}\")\nprint(f\"Epochs: {config.num_epochs}\")\nprint(f\"Learning rate: {config.hebbian_lr if config.hardware_mode else config.learning_rate}\")\nprint(f\"Threshold: {config.threshold}\")\nprint(f\"\")\nprint(f\"Approx SOEN params: ~{soen_params} connection weights\")\nprint(f\"Approx FF params:   ~{(config.flattened_input_dim + 1) * config.hidden_dim} weights\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Load and Visualize Dataset\n",
    "\n",
    "We load the pulse classification dataset and visualize examples from each class.\n",
    "\n",
    "### Dataset Structure\n",
    "- **Shape**: `(N, T, D)` = `(samples, timesteps, features)`\n",
    "- **Class 0**: Single pulse in the signal\n",
    "- **Class 1**: Two pulses in the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 3: LOAD AND VISUALIZE DATASET\n",
    "# ==============================================================================\n",
    "# This cell loads the HDF5 dataset and visualizes examples from each class.\n",
    "#\n",
    "# The dataset contains:\n",
    "#   - Class 0: Signals with ONE pulse\n",
    "#   - Class 1: Signals with TWO pulses\n",
    "#\n",
    "# Dataset format:\n",
    "#   - data: shape (N, T, D) where N=samples, T=timesteps, D=features\n",
    "#   - labels: shape (N,) with integer class labels\n",
    "# ==============================================================================\n",
    "\n",
    "def load_dataset(data_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load train and validation data from HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to HDF5 dataset file\n",
    "        \n",
    "    Returns:\n",
    "        train_data, train_labels, val_data, val_labels\n",
    "    \"\"\"\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        # Load training data\n",
    "        train_data = np.array(f['train']['data'])\n",
    "        train_labels = np.array(f['train']['labels'])\n",
    "        \n",
    "        # Load validation data (or use part of train if not available)\n",
    "        if 'val' in f:\n",
    "            val_data = np.array(f['val']['data'])\n",
    "            val_labels = np.array(f['val']['labels'])\n",
    "        else:\n",
    "            # Split train data\n",
    "            split_idx = int(len(train_data) * 0.8)\n",
    "            val_data = train_data[split_idx:]\n",
    "            val_labels = train_labels[split_idx:]\n",
    "            train_data = train_data[:split_idx]\n",
    "            train_labels = train_labels[:split_idx]\n",
    "    \n",
    "    return train_data, train_labels, val_data, val_labels\n",
    "\n",
    "\n",
    "def visualize_dataset(data: np.ndarray, labels: np.ndarray, n_examples: int = 4):\n",
    "    \"\"\"\n",
    "    Visualize examples from each class.\n",
    "    \n",
    "    Args:\n",
    "        data: Input data of shape (N, T, D)\n",
    "        labels: Class labels of shape (N,)\n",
    "        n_examples: Number of examples per class to show\n",
    "    \"\"\"\n",
    "    print(f\"Dataset shape: {data.shape} (N samples, T timesteps, D features)\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
    "    \n",
    "    # Find examples of each class\n",
    "    class_0_idx = np.where(labels == 0)[0][:n_examples]\n",
    "    class_1_idx = np.where(labels == 1)[0][:n_examples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 5))\n",
    "    fig.suptitle(\"Input Signals: One-Pulse (Class 0) vs Two-Pulse (Class 1)\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot Class 0 (single pulse)\n",
    "    for i, idx in enumerate(class_0_idx):\n",
    "        axes[0, i].plot(data[idx, :, 0], 'b-', linewidth=1.5)\n",
    "        axes[0, i].set_title(f\"Sample {idx}\", fontsize=10)\n",
    "        axes[0, i].set_ylim(-0.1, 1.1)\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel(\"Class 0\\n(One Pulse)\", fontsize=10)\n",
    "    \n",
    "    # Plot Class 1 (two pulses)\n",
    "    for i, idx in enumerate(class_1_idx):\n",
    "        axes[1, i].plot(data[idx, :, 0], 'r-', linewidth=1.5)\n",
    "        axes[1, i].set_title(f\"Sample {idx}\", fontsize=10)\n",
    "        axes[1, i].set_ylim(-0.1, 1.1)\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel(\"Class 1\\n(Two Pulses)\", fontsize=10)\n",
    "        axes[1, i].set_xlabel(\"Time step\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "train_data, train_labels, val_data, val_labels = load_dataset(config.data_path)\n",
    "\n",
    "print(f\"\\nTrain set: {train_data.shape[0]} samples\")\n",
    "print(f\"Val set: {val_data.shape[0]} samples\")\n",
    "\n",
    "# Visualize\n",
    "visualize_dataset(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 4: Create Forward-Forward Dataset\n",
    "\n",
    "For Forward-Forward learning, we need to create **positive** and **negative** examples:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  POSITIVE: [flattened signal (64)] + [correct label one-hot]   â”‚\n",
    "â”‚  NEGATIVE: [flattened signal (64)] + [wrong label one-hot]     â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Total dimension: 64 + 2 = 66                                   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "The label is **concatenated** (not replacing signal values) to preserve all information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 4: CREATE FORWARD-FORWARD DATASET\n",
    "# ==============================================================================\n",
    "# This cell creates a PyTorch Dataset that returns positive and negative pairs.\n",
    "#\n",
    "# For Forward-Forward learning:\n",
    "#   - POSITIVE: signal + correct label â†’ should have HIGH goodness\n",
    "#   - NEGATIVE: signal + wrong label â†’ should have LOW goodness\n",
    "#\n",
    "# Label embedding:\n",
    "#   - We CONCATENATE the one-hot label to the flattened signal\n",
    "#   - This preserves all signal information (unlike pixel replacement)\n",
    "#   - Input dim = seq_len * input_dim + num_classes = 64 + 2 = 66\n",
    "# ==============================================================================\n",
    "\n",
    "class PulseFFDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Pulse classification dataset for Forward-Forward learning.\n",
    "    \n",
    "    Each sample returns:\n",
    "        - positive: flattened signal + correct label one-hot\n",
    "        - negative: flattened signal + wrong label one-hot\n",
    "        - label: true class label\n",
    "        - signal: original signal for visualization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data: np.ndarray, labels: np.ndarray, num_classes: int = 2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Signal data of shape (N, T, D)\n",
    "            labels: Class labels of shape (N,)\n",
    "            num_classes: Number of classes (2 for pulse classification)\n",
    "        \"\"\"\n",
    "        # Convert to tensors\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Pre-flatten signals for efficiency\n",
    "        # Shape: (N, T, D) -> (N, T*D)\n",
    "        self.flat_data = self.data.view(len(data), -1)\n",
    "        \n",
    "        print(f\"Created FF Dataset:\")\n",
    "        print(f\"  Original shape: {self.data.shape}\")\n",
    "        print(f\"  Flattened shape: {self.flat_data.shape}\")\n",
    "        print(f\"  + label one-hot: {num_classes}\")\n",
    "        print(f\"  = Total input dim: {self.flat_data.shape[1] + num_classes}\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _embed_label(self, flat_signal: torch.Tensor, label: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Embed label by CONCATENATING one-hot to flattened signal.\n",
    "        \n",
    "        Args:\n",
    "            flat_signal: Flattened signal of shape (T*D,)\n",
    "            label: Integer class label\n",
    "            \n",
    "        Returns:\n",
    "            Embedded tensor of shape (T*D + num_classes,)\n",
    "        \"\"\"\n",
    "        # Create one-hot label\n",
    "        one_hot = F.one_hot(torch.tensor(label), num_classes=self.num_classes).float()\n",
    "        \n",
    "        # Concatenate signal + label\n",
    "        return torch.cat([flat_signal, one_hot], dim=0)\n",
    "    \n",
    "    def _get_wrong_label(self, true_label: int) -> int:\n",
    "        \"\"\"\n",
    "        Get a wrong label for negative example.\n",
    "        \n",
    "        For binary classification: just flip the label.\n",
    "        For multi-class: randomly select a different label.\n",
    "        \"\"\"\n",
    "        if self.num_classes == 2:\n",
    "            return 1 - true_label  # Flip: 0->1, 1->0\n",
    "        else:\n",
    "            wrong = torch.randint(0, self.num_classes - 1, (1,)).item()\n",
    "            if wrong >= true_label:\n",
    "                wrong += 1\n",
    "            return wrong\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get a sample with positive and negative versions.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with:\n",
    "                - positive: signal + correct label (should have HIGH goodness)\n",
    "                - negative: signal + wrong label (should have LOW goodness)\n",
    "                - label: true class label\n",
    "                - signal: original signal for visualization\n",
    "        \"\"\"\n",
    "        flat_signal = self.flat_data[idx]\n",
    "        true_label = self.labels[idx].item()\n",
    "        wrong_label = self._get_wrong_label(true_label)\n",
    "        \n",
    "        return {\n",
    "            \"positive\": self._embed_label(flat_signal, true_label),\n",
    "            \"negative\": self._embed_label(flat_signal, wrong_label),\n",
    "            \"label\": self.labels[idx],\n",
    "            \"signal\": self.data[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = PulseFFDataset(train_data, train_labels, num_classes=config.num_classes)\n",
    "\n",
    "print(\"\\nCreating validation dataset...\")\n",
    "val_dataset = PulseFFDataset(val_data, val_labels, num_classes=config.num_classes)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# Verify a sample\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "sample = train_dataset[0]\n",
    "print(f\"Positive shape: {sample['positive'].shape}\")\n",
    "print(f\"Negative shape: {sample['negative'].shape}\")\n",
    "print(f\"True label: {sample['label'].item()}\")\n",
    "print(f\"\")\n",
    "print(f\"Signal part (first 5 values): {sample['positive'][:5].tolist()}\")\n",
    "print(f\"Label part (last 2 values):\")\n",
    "print(f\"  Positive (correct): {sample['positive'][-2:].tolist()}\")\n",
    "print(f\"  Negative (wrong):   {sample['negative'][-2:].tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 5: Define Forward-Forward Layer\n",
    "\n",
    "Each FF layer has its own **local learning objective**:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  GOODNESS = measure of layer activity                          â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Goal: positive_goodness > threshold > negative_goodness       â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  Standard mode:  goodness = mean(xÂ²),     loss = softplus      â”‚\n",
    "â”‚  Hardware mode:  goodness = mean(|x|),    loss = hinge         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 5: DEFINE FORWARD-FORWARD LAYER\n",
    "# ==============================================================================\n",
    "# This cell defines the FFLayer class - the building block of our network.\n",
    "#\n",
    "# Each layer has:\n",
    "#   - A linear transformation (weights and biases)\n",
    "#   - A ReLU activation\n",
    "#   - A normalization step\n",
    "#   - A \"goodness\" computation for the FF objective\n",
    "#\n",
    "# The layer learns to:\n",
    "#   - Have HIGH goodness for positive data (correct label)\n",
    "#   - Have LOW goodness for negative data (wrong label)\n",
    "#\n",
    "# Two modes are supported:\n",
    "#   - Standard: Uses autograd for gradient computation\n",
    "#   - Hardware: Uses Hebbian learning (gradient-free)\n",
    "# ==============================================================================\n",
    "\n",
    "class FFLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Forward-Forward Layer with local learning objective.\n",
    "    \n",
    "    This layer learns to distinguish between positive (correct label)\n",
    "    and negative (wrong label) inputs using a local \"goodness\" measure.\n",
    "    \n",
    "    Architecture:\n",
    "        Input -> Linear -> ReLU -> Normalize -> Output\n",
    "                                      |\n",
    "                                 Compute Goodness\n",
    "                                      |\n",
    "                                 Local Learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        threshold: float = 2.0,\n",
    "        hardware_mode: bool = False,\n",
    "        inhibition_strength: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_features: Input dimension\n",
    "            out_features: Output dimension (number of neurons)\n",
    "            threshold: Goodness threshold for FF learning\n",
    "            hardware_mode: If True, use hardware-compatible operations\n",
    "            inhibition_strength: Strength of lateral inhibition (hardware mode)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "        self.hardware_mode = hardware_mode\n",
    "        self.inhibition_strength = inhibition_strength\n",
    "        \n",
    "        # Linear transformation: y = Wx + b\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "        # ReLU activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initialize weights\n",
    "        if hardware_mode:\n",
    "            # Larger initialization for hardware mode\n",
    "            nn.init.uniform_(self.linear.weight, -0.3, 0.3)\n",
    "            nn.init.zeros_(self.linear.bias)\n",
    "        \n",
    "        # Storage for activations (needed for Hebbian learning)\n",
    "        self._raw_activations = None\n",
    "        self._pos_input = None\n",
    "        self._pos_activations = None\n",
    "        self._neg_input = None\n",
    "        self._neg_activations = None\n",
    "        \n",
    "        # Running estimates for adaptive threshold\n",
    "        self._running_pos_goodness = 0.1\n",
    "        self._running_neg_goodness = 0.1\n",
    "        self._momentum = 0.95\n",
    "    \n",
    "    def forward(self, x: torch.Tensor, store_as: str = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, in_features)\n",
    "            store_as: If 'positive' or 'negative', store activations for Hebbian learning\n",
    "            \n",
    "        Returns:\n",
    "            Normalized output tensor of shape (batch, out_features)\n",
    "        \"\"\"\n",
    "        # Store input for Hebbian learning\n",
    "        if store_as == 'positive':\n",
    "            self._pos_input = x.detach()\n",
    "        elif store_as == 'negative':\n",
    "            self._neg_input = x.detach()\n",
    "        \n",
    "        # Linear transformation\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        # ReLU activation\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Store raw activations for goodness computation\n",
    "        self._raw_activations = x\n",
    "        \n",
    "        # Store activations for Hebbian learning\n",
    "        if store_as == 'positive':\n",
    "            self._pos_activations = x.detach()\n",
    "        elif store_as == 'negative':\n",
    "            self._neg_activations = x.detach()\n",
    "        \n",
    "        # Normalization\n",
    "        if self.hardware_mode:\n",
    "            # Hardware-compatible: Lateral inhibition\n",
    "            # Subtract mean activity (can be implemented with differential amplifier)\n",
    "            mean_activity = x.mean(dim=1, keepdim=True)\n",
    "            x_norm = x - self.inhibition_strength * mean_activity\n",
    "            x_norm = torch.clamp(x_norm, min=0)\n",
    "        else:\n",
    "            # Standard: L2 normalization\n",
    "            x_norm = x / (x.norm(dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        return x_norm\n",
    "    \n",
    "    def compute_goodness(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute \"goodness\" of the current activations.\n",
    "        \n",
    "        Goodness measures how \"active\" the layer is:\n",
    "            - Standard mode: mean(xÂ²) - requires squaring circuit\n",
    "            - Hardware mode: mean(|x|) - uses rectifiers only\n",
    "            \n",
    "        Returns:\n",
    "            Goodness value per sample, shape (batch,)\n",
    "        \"\"\"\n",
    "        activations = self._raw_activations\n",
    "        \n",
    "        if self.hardware_mode:\n",
    "            # Hardware-compatible: mean absolute value\n",
    "            # Can be computed with full-wave rectifier\n",
    "            return activations.abs().mean(dim=1)\n",
    "        else:\n",
    "            # Standard: mean squared value\n",
    "            return (activations ** 2).mean(dim=1)\n",
    "    \n",
    "    def ff_loss(self, pos_goodness: torch.Tensor, neg_goodness: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute Forward-Forward loss.\n",
    "        \n",
    "        Goal: Push positive goodness ABOVE threshold, negative BELOW.\n",
    "        \n",
    "        Args:\n",
    "            pos_goodness: Goodness for positive data, shape (batch,)\n",
    "            neg_goodness: Goodness for negative data, shape (batch,)\n",
    "            \n",
    "        Returns:\n",
    "            Scalar loss value\n",
    "        \"\"\"\n",
    "        # Use adaptive threshold in hardware mode\n",
    "        if self.hardware_mode:\n",
    "            threshold = (self._running_pos_goodness + self._running_neg_goodness) / 2\n",
    "        else:\n",
    "            threshold = self.threshold\n",
    "        \n",
    "        if self.hardware_mode:\n",
    "            # Hardware-compatible: Hinge loss\n",
    "            # Uses comparators and ReLU (threshold circuits)\n",
    "            pos_loss = F.relu(threshold - pos_goodness)  # Want pos > threshold\n",
    "            neg_loss = F.relu(neg_goodness - threshold)  # Want neg < threshold\n",
    "        else:\n",
    "            # Standard: Softplus loss (smooth approximation)\n",
    "            pos_loss = F.softplus(-(pos_goodness - threshold))\n",
    "            neg_loss = F.softplus(neg_goodness - threshold)\n",
    "        \n",
    "        return (pos_loss + neg_loss).mean()\n",
    "    \n",
    "    def hebbian_update(self, pos_goodness: torch.Tensor, neg_goodness: torch.Tensor,\n",
    "                       lr: float = 0.01) -> None:\n",
    "        \"\"\"\n",
    "        Perform Hebbian weight update (gradient-free learning).\n",
    "        \n",
    "        This implements \"differential Hebbian learning\":\n",
    "            - Compare activations for positive vs negative inputs\n",
    "            - Strengthen connections that help distinguish them\n",
    "        \n",
    "        Args:\n",
    "            pos_goodness: Goodness for positive data\n",
    "            neg_goodness: Goodness for negative data\n",
    "            lr: Hebbian learning rate\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            if self._pos_input is None or self._neg_input is None:\n",
    "                return\n",
    "            \n",
    "            batch_size = self._pos_input.shape[0]\n",
    "            \n",
    "            # Update running goodness estimates\n",
    "            pos_g_mean = pos_goodness.mean().item()\n",
    "            neg_g_mean = neg_goodness.mean().item()\n",
    "            self._running_pos_goodness = self._momentum * self._running_pos_goodness + (1 - self._momentum) * pos_g_mean\n",
    "            self._running_neg_goodness = self._momentum * self._running_neg_goodness + (1 - self._momentum) * neg_g_mean\n",
    "            \n",
    "            # Compute differences between positive and negative\n",
    "            diff_acts = self._pos_activations - self._neg_activations  # Output difference\n",
    "            diff_input = self._pos_input - self._neg_input              # Input difference (label part)\n",
    "            avg_acts = (self._pos_activations + self._neg_activations) / 2\n",
    "            avg_input = (self._pos_input + self._neg_input) / 2\n",
    "            \n",
    "            # Differential Hebbian update\n",
    "            update1 = diff_acts.T @ avg_input / batch_size\n",
    "            update2 = avg_acts.T @ diff_input / batch_size\n",
    "            delta_w = lr * (update1 + 2.0 * update2)\n",
    "            \n",
    "            # Apply update\n",
    "            self.linear.weight.data += delta_w\n",
    "            \n",
    "            # Update bias\n",
    "            goodness_diff = pos_g_mean - neg_g_mean\n",
    "            self.linear.bias.data += lr * 0.5 * goodness_diff\n",
    "            \n",
    "            # Clamp weights to hardware constraints\n",
    "            self.linear.weight.data.clamp_(-0.5, 0.5)\n",
    "            self.linear.bias.data.clamp_(-1.0, 1.0)\n",
    "\n",
    "\n",
    "# Test the layer\n",
    "print(\"Testing FFLayer...\")\n",
    "test_layer = FFLayer(66, 128, threshold=config.threshold, hardware_mode=config.hardware_mode)\n",
    "test_input = torch.randn(32, 66)  # Batch of 32, input dim 66\n",
    "test_output = test_layer(test_input)\n",
    "test_goodness = test_layer.compute_goodness()\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {test_output.shape}\")\n",
    "print(f\"Goodness shape: {test_goodness.shape}\")\n",
    "print(f\"Mean goodness: {test_goodness.mean().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Step 6: Define Forward-Forward Network\n\nWe stack FF layers to match the SOEN architecture from `02_train_a_model.ipynb`.\n\n### Architecture Comparison\n\n| Component | SOEN (Backprop) | FF (This Tutorial) |\n|-----------|-----------------|---------------------|\n| **Input processing** | Temporal (64 steps Ã— 1 feature) | Flattened (64 + 2 label = 66) |\n| **Hidden layer** | 5 SingleDendrite neurons | 5 FF neurons |\n| **Output** | 2 classes | 2 classes |\n| **Recurrence** | Yes (J_1_to_1) | No (feedforward only) |\n\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  INPUT: [flattened signal (64)] + [label one-hot (2)] = 66     â”‚\nâ”‚                          â”‚                                      â”‚\nâ”‚                          â–¼                                      â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\nâ”‚  â”‚ FF Layer 1: 66 â†’ 5 neurons                                â”‚ â”‚\nâ”‚  â”‚   Linear â†’ ReLU â†’ Normalize â†’ Goodness                   â”‚ â”‚\nâ”‚  â”‚   LOCAL OBJECTIVE: pos_goodness > Î¸ > neg_goodness       â”‚ â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\nâ”‚                          â”‚                                      â”‚\nâ”‚                          â–¼                                      â”‚\nâ”‚  INFERENCE: Test each label, select highest total goodness     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n**Key Difference**: SOEN processes the signal temporally with recurrence, while FF flattens\nthe entire sequence. This loses temporal structure but enables local learning."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 6: DEFINE FORWARD-FORWARD NETWORK\n# ==============================================================================\n# This cell defines the complete FF network by stacking multiple FF layers.\n#\n# Key concepts:\n#   - Each layer learns INDEPENDENTLY (no gradient flow between layers)\n#   - Inference requires multiple forward passes (one per class)\n#   - Prediction = class with highest total goodness across all layers\n#\n# Architecture for pulse classification (matching SOEN):\n#   Input (66) â†’ FFLayer (5) â†’ Prediction\n#\n# Note: The SOEN model processes temporally (64 timesteps Ã— 1 feature),\n# while FF flattens the sequence (64 features + 2 label = 66 input dim).\n# ==============================================================================\n\nclass FFNetwork(nn.Module):\n    \"\"\"\n    Forward-Forward Network for classification.\n    \n    This network stacks multiple FF layers, each trained with its own\n    local objective. During inference, it performs multiple forward passes\n    (one per class) and selects the class with highest total goodness.\n    \"\"\"\n    \n    def __init__(\n        self,\n        input_dim: int,\n        hidden_dim: int,\n        num_classes: int,\n        num_layers: int = 1,\n        threshold: float = 2.0,\n        hardware_mode: bool = False,\n    ):\n        \"\"\"\n        Args:\n            input_dim: Dimension of flattened input (without label)\n            hidden_dim: Hidden layer dimension\n            num_classes: Number of classes\n            num_layers: Number of FF layers\n            threshold: Goodness threshold\n            hardware_mode: Use hardware-compatible operations\n        \"\"\"\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.num_classes = num_classes\n        self.hardware_mode = hardware_mode\n        \n        # Total input dim = signal + label\n        total_input_dim = input_dim + num_classes\n        \n        # Build FF layers\n        self.layers = nn.ModuleList()\n        \n        # First layer: input -> hidden\n        self.layers.append(FFLayer(\n            total_input_dim, hidden_dim,\n            threshold=threshold,\n            hardware_mode=hardware_mode\n        ))\n        \n        # Additional hidden layers (if any)\n        for _ in range(num_layers - 1):\n            self.layers.append(FFLayer(\n                hidden_dim, hidden_dim,\n                threshold=threshold,\n                hardware_mode=hardware_mode\n            ))\n        \n        print(f\"\\nCreated FF Network:\")\n        print(f\"  Input dim: {input_dim} (signal) + {num_classes} (label) = {total_input_dim}\")\n        print(f\"  Layers: {num_layers} x {hidden_dim} neurons\")\n        print(f\"  Hardware mode: {hardware_mode}\")\n        print(f\"  Total params: {sum(p.numel() for p in self.parameters()):,}\")\n    \n    def forward_with_label(self, flat_signal: torch.Tensor, label: int,\n                           store_as: str = None) -> torch.Tensor:\n        \"\"\"\n        Forward pass with a specific label embedded.\n        \n        Args:\n            flat_signal: Flattened input signal, shape (batch, input_dim)\n            label: Class label to embed\n            store_as: 'positive' or 'negative' for Hebbian learning\n            \n        Returns:\n            Final layer output\n        \"\"\"\n        batch_size = flat_signal.shape[0]\n        device = flat_signal.device\n        \n        # Create one-hot label\n        one_hot = F.one_hot(torch.tensor([label], device=device), self.num_classes)\n        one_hot = one_hot.float().expand(batch_size, -1)\n        \n        # Concatenate signal + label\n        x = torch.cat([flat_signal, one_hot], dim=1)\n        \n        # Forward through all layers\n        for layer in self.layers:\n            x = layer(x, store_as=store_as)\n            if store_as is None:\n                x = x.detach()  # No gradient flow between layers during inference\n        \n        return x\n    \n    def compute_total_goodness(self, flat_signal: torch.Tensor, label: int) -> torch.Tensor:\n        \"\"\"\n        Compute total goodness across all layers for a given label.\n        \n        Args:\n            flat_signal: Flattened input signal\n            label: Class label to test\n            \n        Returns:\n            Total goodness per sample, shape (batch,)\n        \"\"\"\n        batch_size = flat_signal.shape[0]\n        device = flat_signal.device\n        \n        # Create one-hot label\n        one_hot = F.one_hot(torch.tensor([label], device=device), self.num_classes)\n        one_hot = one_hot.float().expand(batch_size, -1)\n        \n        # Concatenate signal + label\n        x = torch.cat([flat_signal, one_hot], dim=1)\n        \n        # Sum goodness across all layers\n        total_goodness = torch.zeros(batch_size, device=device)\n        \n        for layer in self.layers:\n            x = layer(x)\n            total_goodness += layer.compute_goodness()\n            x = x.detach()  # No gradient flow between layers\n        \n        return total_goodness\n    \n    def predict(self, flat_signal: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Predict class labels using multi-pass inference.\n        \n        For each class, we compute total goodness and select the class\n        with the highest goodness.\n        \n        Args:\n            flat_signal: Flattened input signal, shape (batch, input_dim)\n            \n        Returns:\n            Predicted class labels, shape (batch,)\n        \"\"\"\n        batch_size = flat_signal.shape[0]\n        device = flat_signal.device\n        \n        # Compute goodness for each class\n        all_goodness = torch.zeros(batch_size, self.num_classes, device=device)\n        \n        for label in range(self.num_classes):\n            all_goodness[:, label] = self.compute_total_goodness(flat_signal, label)\n        \n        # Return class with highest goodness\n        return all_goodness.argmax(dim=1)\n    \n    def get_layer_optimizer(self, layer_idx: int, lr: float) -> torch.optim.Optimizer:\n        \"\"\"\n        Get optimizer for a specific layer (standard mode only).\n        \n        Args:\n            layer_idx: Index of layer\n            lr: Learning rate\n            \n        Returns:\n            Adam optimizer for the layer's parameters\n        \"\"\"\n        return torch.optim.Adam(self.layers[layer_idx].parameters(), lr=lr)\n\n\n# Create network - using config.num_layers for architecture matching\nmodel = FFNetwork(\n    input_dim=config.seq_len * config.input_dim,  # 64\n    hidden_dim=config.hidden_dim,                  # 5 (matching SOEN)\n    num_classes=config.num_classes,                # 2\n    num_layers=config.num_layers,                  # 1 (matching SOEN)\n    threshold=config.threshold,\n    hardware_mode=config.hardware_mode,\n).to(DEVICE)\n\n# Count parameters\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nTotal parameters: {total_params:,}\")\nprint(f\"\\nNote: SOEN processes 64 timesteps sequentially with recurrence.\")\nprint(f\"      FF flattens the sequence, losing temporal structure.\")\nprint(f\"      This is a key architectural difference.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 7: Training Loop\n",
    "\n",
    "The key difference from backpropagation:\n",
    "\n",
    "| Backpropagation | Forward-Forward |\n",
    "|-----------------|------------------|\n",
    "| Global loss â†’ backward() | Local loss per layer |\n",
    "| One forward pass | Two passes (positive + negative) |\n",
    "| Gradients flow through all layers | No gradients between layers |\n",
    "| Single optimizer | One optimizer per layer (or Hebbian) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 7: TRAINING LOOP\n",
    "# ==============================================================================\n",
    "# This cell implements the Forward-Forward training algorithm.\n",
    "#\n",
    "# Key differences from backpropagation:\n",
    "#   1. Each layer is trained INDEPENDENTLY\n",
    "#   2. Two forward passes per batch: positive and negative\n",
    "#   3. No gradient flow between layers (outputs are detached)\n",
    "#   4. Local loss function based on \"goodness\"\n",
    "#\n",
    "# Training flow:\n",
    "#   For each batch:\n",
    "#     For each layer:\n",
    "#       1. Forward positive data â†’ compute positive goodness\n",
    "#       2. Forward negative data â†’ compute negative goodness\n",
    "#       3. Compute local loss: push pos > threshold > neg\n",
    "#       4. Update layer weights (Adam or Hebbian)\n",
    "#       5. Pass output to next layer (detached)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_ff(\n",
    "    model: FFNetwork,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    lr: float,\n",
    "    hebbian_lr: float,\n",
    "    device: torch.device,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Train using Forward-Forward algorithm.\n",
    "    \n",
    "    Args:\n",
    "        model: FFNetwork to train\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Learning rate for Adam (standard mode)\n",
    "        hebbian_lr: Learning rate for Hebbian (hardware mode)\n",
    "        device: Device to train on\n",
    "        \n",
    "    Returns:\n",
    "        Training history dictionary\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    hardware_mode = model.hardware_mode\n",
    "    \n",
    "    # Create optimizers for each layer (standard mode only)\n",
    "    if not hardware_mode:\n",
    "        optimizers = [model.get_layer_optimizer(i, lr) for i in range(len(model.layers))]\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"layer_losses\": [[] for _ in range(len(model.layers))],\n",
    "        \"layer_pos_goodness\": [[] for _ in range(len(model.layers))],\n",
    "        \"layer_neg_goodness\": [[] for _ in range(len(model.layers))],\n",
    "    }\n",
    "    \n",
    "    # Print training info\n",
    "    mode_str = \"HARDWARE (Hebbian)\" if hardware_mode else \"STANDARD (Autograd)\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FORWARD-FORWARD TRAINING - {mode_str}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Epochs: {num_epochs}\")\n",
    "    print(f\"Learning rate: {hebbian_lr if hardware_mode else lr}\")\n",
    "    print(f\"Layers: {len(model.layers)}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Track metrics\n",
    "        epoch_losses = [0.0 for _ in range(len(model.layers))]\n",
    "        epoch_pos_goodness = [0.0 for _ in range(len(model.layers))]\n",
    "        epoch_neg_goodness = [0.0 for _ in range(len(model.layers))]\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Progress bar\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in pbar:\n",
    "            # Get positive and negative data\n",
    "            pos_data = batch[\"positive\"].to(device)\n",
    "            neg_data = batch[\"negative\"].to(device)\n",
    "            \n",
    "            # Current inputs for each layer\n",
    "            pos_input = pos_data\n",
    "            neg_input = neg_data\n",
    "            \n",
    "            batch_loss = 0.0\n",
    "            \n",
    "            # Train each layer independently\n",
    "            for layer_idx, layer in enumerate(model.layers):\n",
    "                \n",
    "                if hardware_mode:\n",
    "                    # ========================================\n",
    "                    # HARDWARE MODE: Hebbian learning\n",
    "                    # ========================================\n",
    "                    with torch.no_grad():\n",
    "                        # Forward positive data\n",
    "                        pos_output = layer(pos_input, store_as='positive')\n",
    "                        pos_goodness = layer.compute_goodness()\n",
    "                        \n",
    "                        # Forward negative data\n",
    "                        neg_output = layer(neg_input, store_as='negative')\n",
    "                        neg_goodness = layer.compute_goodness()\n",
    "                        \n",
    "                        # Compute loss (for monitoring only)\n",
    "                        loss = layer.ff_loss(pos_goodness, neg_goodness)\n",
    "                    \n",
    "                    # Apply Hebbian update\n",
    "                    layer.hebbian_update(pos_goodness, neg_goodness, lr=hebbian_lr)\n",
    "                    \n",
    "                    # Prepare inputs for next layer\n",
    "                    pos_input = pos_output.detach()\n",
    "                    neg_input = neg_output.detach()\n",
    "                    \n",
    "                else:\n",
    "                    # ========================================\n",
    "                    # STANDARD MODE: Autograd + Adam\n",
    "                    # ========================================\n",
    "                    optimizer = optimizers[layer_idx]\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward positive data\n",
    "                    pos_output = layer(pos_input)\n",
    "                    pos_goodness = layer.compute_goodness()\n",
    "                    \n",
    "                    # Forward negative data\n",
    "                    neg_output = layer(neg_input)\n",
    "                    neg_goodness = layer.compute_goodness()\n",
    "                    \n",
    "                    # Compute loss\n",
    "                    loss = layer.ff_loss(pos_goodness, neg_goodness)\n",
    "                    \n",
    "                    # Backward pass (only for this layer!)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    # Prepare inputs for next layer (detached = no gradient flow)\n",
    "                    pos_input = pos_output.detach()\n",
    "                    neg_input = neg_output.detach()\n",
    "                \n",
    "                # Track metrics\n",
    "                epoch_losses[layer_idx] += loss.item()\n",
    "                epoch_pos_goodness[layer_idx] += pos_goodness.mean().item()\n",
    "                epoch_neg_goodness[layer_idx] += neg_goodness.mean().item()\n",
    "                batch_loss += loss.item()\n",
    "            \n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({\"loss\": f\"{batch_loss/len(model.layers):.4f}\"})\n",
    "        \n",
    "        # Compute epoch averages\n",
    "        for i in range(len(model.layers)):\n",
    "            history[\"layer_losses\"][i].append(epoch_losses[i] / num_batches)\n",
    "            history[\"layer_pos_goodness\"][i].append(epoch_pos_goodness[i] / num_batches)\n",
    "            history[\"layer_neg_goodness\"][i].append(epoch_neg_goodness[i] / num_batches)\n",
    "        \n",
    "        avg_loss = sum(epoch_losses) / (num_batches * len(model.layers))\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        \n",
    "        # Evaluate\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0 or epoch == num_epochs - 1:\n",
    "            train_acc = evaluate_ff(model, train_loader, device)\n",
    "            val_acc = evaluate_ff(model, val_loader, device)\n",
    "            history[\"train_acc\"].append(train_acc)\n",
    "            history[\"val_acc\"].append(val_acc)\n",
    "            \n",
    "            print(f\"\\nEpoch {epoch+1}: Loss={avg_loss:.4f}, Train Acc={train_acc:.1%}, Val Acc={val_acc:.1%}\")\n",
    "            \n",
    "            # Print layer-wise goodness\n",
    "            for i in range(len(model.layers)):\n",
    "                pos_g = history[\"layer_pos_goodness\"][i][-1]\n",
    "                neg_g = history[\"layer_neg_goodness\"][i][-1]\n",
    "                sep = pos_g - neg_g\n",
    "                print(f\"  Layer {i}: pos={pos_g:.3f}, neg={neg_g:.3f}, separation={sep:+.3f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_ff(model: FFNetwork, data_loader: DataLoader, device: torch.device) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate Forward-Forward model accuracy.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained FFNetwork\n",
    "        data_loader: Data loader for evaluation\n",
    "        device: Device\n",
    "        \n",
    "    Returns:\n",
    "        Accuracy (0 to 1)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            # Get signal and labels\n",
    "            signals = batch[\"signal\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            # Flatten signals\n",
    "            flat_signals = signals.view(signals.shape[0], -1)\n",
    "            \n",
    "            # Predict\n",
    "            predictions = model.predict(flat_signals)\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TRAIN THE MODEL\n",
    "# ==============================================================================\n",
    "# This cell runs the Forward-Forward training.\n",
    "#\n",
    "# Watch for:\n",
    "#   - Layer-wise goodness separation (pos should be > neg)\n",
    "#   - Validation accuracy improvement over epochs\n",
    "#   - Loss decreasing over time\n",
    "# ==============================================================================\n",
    "\n",
    "history = train_ff(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    num_epochs=config.num_epochs,\n",
    "    lr=config.learning_rate,\n",
    "    hebbian_lr=config.hebbian_lr,\n",
    "    device=DEVICE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Training Progress\n",
    "\n",
    "We plot the training metrics to understand how learning progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 8: VISUALIZE TRAINING PROGRESS\n",
    "# ==============================================================================\n",
    "# This cell plots training metrics to visualize learning progress.\n",
    "#\n",
    "# Key plots:\n",
    "#   1. Training loss over epochs\n",
    "#   2. Validation accuracy over epochs\n",
    "#   3. Layer-wise goodness (positive vs negative)\n",
    "#   4. Layer-wise loss\n",
    "# ==============================================================================\n",
    "\n",
    "def plot_training_history(history: Dict, config: FFConfig):\n",
    "    \"\"\"Plot training metrics.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Forward-Forward Training Progress', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 1. Training loss\n",
    "    axes[0, 0].plot(history[\"train_loss\"], 'b-', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy\n",
    "    epochs_with_acc = [0] + list(range(4, len(history[\"train_loss\"]), 5)) + [len(history[\"train_loss\"])-1]\n",
    "    epochs_with_acc = sorted(set(epochs_with_acc))\n",
    "    if len(history[\"val_acc\"]) > 0:\n",
    "        axes[0, 1].plot(epochs_with_acc[:len(history[\"val_acc\"])], history[\"val_acc\"], \n",
    "                        'g-o', linewidth=2, markersize=6, label='Validation')\n",
    "        axes[0, 1].plot(epochs_with_acc[:len(history[\"train_acc\"])], history[\"train_acc\"], \n",
    "                        'b-s', linewidth=2, markersize=6, label='Training')\n",
    "        axes[0, 1].legend()\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Classification Accuracy')\n",
    "    axes[0, 1].set_ylim([0, 1.05])\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Layer-wise goodness\n",
    "    colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(history[\"layer_pos_goodness\"])))\n",
    "    for i, (pos_g, neg_g) in enumerate(zip(history[\"layer_pos_goodness\"], \n",
    "                                           history[\"layer_neg_goodness\"])):\n",
    "        axes[1, 0].plot(pos_g, color=colors[i], linestyle='-', \n",
    "                        linewidth=2, label=f'Layer {i} (pos)')\n",
    "        axes[1, 0].plot(neg_g, color=colors[i], linestyle='--', \n",
    "                        linewidth=2, alpha=0.7)\n",
    "    axes[1, 0].axhline(y=config.threshold, color='red', linestyle=':', \n",
    "                       linewidth=2, label=f'Threshold ({config.threshold})')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Goodness')\n",
    "    axes[1, 0].set_title('Layer-wise Goodness (solid=pos, dashed=neg)')\n",
    "    axes[1, 0].legend(loc='best', fontsize=8)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Layer-wise loss\n",
    "    for i, layer_loss in enumerate(history[\"layer_losses\"]):\n",
    "        axes[1, 1].plot(layer_loss, color=colors[i], linewidth=2, label=f'Layer {i}')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Layer-wise FF Loss')\n",
    "    axes[1, 1].legend(loc='best')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    if len(history[\"val_acc\"]) > 0:\n",
    "        print(f\"Final validation accuracy: {history['val_acc'][-1]:.1%}\")\n",
    "        print(f\"Best validation accuracy: {max(history['val_acc']):.1%}\")\n",
    "    print(f\"\\nFinal layer-wise goodness separation:\")\n",
    "    for i in range(len(history['layer_pos_goodness'])):\n",
    "        pos = history['layer_pos_goodness'][i][-1]\n",
    "        neg = history['layer_neg_goodness'][i][-1]\n",
    "        sep = pos - neg\n",
    "        print(f\"  Layer {i}: pos={pos:.3f}, neg={neg:.3f}, separation={sep:+.3f}\")\n",
    "\n",
    "\n",
    "plot_training_history(history, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Predictions\n",
    "\n",
    "Let's see how the model performs on individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 9: VISUALIZE PREDICTIONS\n",
    "# ==============================================================================\n",
    "# This cell shows model predictions on individual samples.\n",
    "#\n",
    "# For each sample, we show:\n",
    "#   - The input signal\n",
    "#   - The true label\n",
    "#   - The predicted label\n",
    "#   - Whether the prediction is correct (green) or wrong (red)\n",
    "# ==============================================================================\n",
    "\n",
    "def visualize_predictions(model: FFNetwork, dataset: Dataset, n_samples: int = 8):\n",
    "    \"\"\"Visualize model predictions on samples.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Select random samples\n",
    "    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    n_cols = min(4, n_samples)\n",
    "    n_rows = (n_samples + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5*n_cols, 3*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    fig.suptitle('Forward-Forward Predictions', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    class_names = [\"One Pulse\", \"Two Pulses\"]\n",
    "    correct_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            sample = dataset[idx]\n",
    "            signal = sample[\"signal\"].unsqueeze(0).to(DEVICE)\n",
    "            label = sample[\"label\"].item()\n",
    "            \n",
    "            # Flatten and predict\n",
    "            flat_signal = signal.view(1, -1)\n",
    "            prediction = model.predict(flat_signal).item()\n",
    "            \n",
    "            is_correct = prediction == label\n",
    "            correct_count += is_correct\n",
    "            \n",
    "            # Plot\n",
    "            row, col = i // n_cols, i % n_cols\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            ax.plot(sample[\"signal\"][:, 0].numpy(), 'b-', linewidth=1.5)\n",
    "            ax.set_ylim(-0.1, 1.1)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            color = 'green' if is_correct else 'red'\n",
    "            symbol = 'âœ“' if is_correct else 'âœ—'\n",
    "            \n",
    "            ax.set_title(\n",
    "                f\"{symbol} Pred: {class_names[prediction]}\\nTrue: {class_names[label]}\",\n",
    "                fontsize=9,\n",
    "                color=color,\n",
    "                fontweight='bold' if not is_correct else 'normal'\n",
    "            )\n",
    "            \n",
    "            if col == 0:\n",
    "                ax.set_ylabel(\"Signal\")\n",
    "            if row == n_rows - 1:\n",
    "                ax.set_xlabel(\"Time step\")\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_samples, n_rows * n_cols):\n",
    "        row, col = i // n_cols, i % n_cols\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSample accuracy: {correct_count}/{n_samples} ({correct_count/n_samples:.1%})\")\n",
    "\n",
    "\n",
    "visualize_predictions(model, val_dataset, n_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 10: Analyze Goodness Distributions\n",
    "\n",
    "Let's see how well the layers separate positive and negative goodness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 10: ANALYZE GOODNESS DISTRIBUTIONS\n",
    "# ==============================================================================\n",
    "# This cell analyzes how well each layer separates positive vs negative goodness.\n",
    "#\n",
    "# A well-trained layer should have:\n",
    "#   - Positive goodness distribution shifted to the RIGHT\n",
    "#   - Negative goodness distribution shifted to the LEFT\n",
    "#   - Minimal overlap between the two distributions\n",
    "# ==============================================================================\n",
    "\n",
    "def analyze_goodness_distributions(model: FFNetwork, data_loader: DataLoader):\n",
    "    \"\"\"Analyze goodness distributions for positive vs negative data.\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_pos_goodness = [[] for _ in range(len(model.layers))]\n",
    "    all_neg_goodness = [[] for _ in range(len(model.layers))]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            pos_data = batch[\"positive\"].to(DEVICE)\n",
    "            neg_data = batch[\"negative\"].to(DEVICE)\n",
    "            \n",
    "            pos_input = pos_data\n",
    "            neg_input = neg_data\n",
    "            \n",
    "            for layer_idx, layer in enumerate(model.layers):\n",
    "                # Forward and compute goodness\n",
    "                pos_output = layer(pos_input)\n",
    "                pos_g = layer.compute_goodness()\n",
    "                \n",
    "                neg_output = layer(neg_input)\n",
    "                neg_g = layer.compute_goodness()\n",
    "                \n",
    "                all_pos_goodness[layer_idx].extend(pos_g.cpu().numpy())\n",
    "                all_neg_goodness[layer_idx].extend(neg_g.cpu().numpy())\n",
    "                \n",
    "                pos_input = pos_output\n",
    "                neg_input = neg_output\n",
    "    \n",
    "    # Plot distributions\n",
    "    fig, axes = plt.subplots(1, len(model.layers), figsize=(6*len(model.layers), 5))\n",
    "    if len(model.layers) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    fig.suptitle('Goodness Distributions by Layer', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for i, ax in enumerate(axes):\n",
    "        pos_g = np.array(all_pos_goodness[i])\n",
    "        neg_g = np.array(all_neg_goodness[i])\n",
    "        \n",
    "        ax.hist(pos_g, bins=50, alpha=0.6, color='green', \n",
    "                label=f'Positive (Î¼={pos_g.mean():.2f})', density=True)\n",
    "        ax.hist(neg_g, bins=50, alpha=0.6, color='red', \n",
    "                label=f'Negative (Î¼={neg_g.mean():.2f})', density=True)\n",
    "        \n",
    "        ax.axvline(x=config.threshold, color='black', linestyle='--', \n",
    "                   linewidth=2, label=f'Threshold ({config.threshold})')\n",
    "        \n",
    "        ax.set_xlabel('Goodness', fontsize=12)\n",
    "        ax.set_ylabel('Density', fontsize=12)\n",
    "        ax.set_title(f'Layer {i}', fontsize=12)\n",
    "        ax.legend(loc='best')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Print separation stats\n",
    "        separation = pos_g.mean() - neg_g.mean()\n",
    "        print(f\"Layer {i}: separation = {separation:.3f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_goodness_distributions(model, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "## Summary\n\n### What We Learned\n\n1. **Forward-Forward eliminates backpropagation** by using local \"goodness\" objectives\n2. **Each layer learns independently** - no gradient flow between layers\n3. **Label embedding** concatenates the class label to the input\n4. **Inference requires multiple passes** (one per class)\n\n### Architecture Comparison: This Tutorial vs SOEN Backprop\n\n| Aspect | SOEN (02_train_a_model) | FF (This Tutorial) |\n|--------|--------------------------|---------------------|\n| Hidden neurons | 5 (SingleDendrite) | **5** (FFLayer) |\n| Input processing | Temporal (64 timesteps) | Flattened (66 dim) |\n| Recurrence | Yes (J_1_to_1) | No |\n| Learning | Backprop (global loss) | FF (local goodness) |\n| Params | ~27 connection weights | ~335 weights |\n\n**Note on parameter difference**: FF has more parameters because it processes the flattened\nsequence (66 inputs) rather than 1 input per timestep. The hidden dimension (5) matches.\n\n### Forward-Forward vs Backpropagation\n\n| Aspect | Backpropagation | Forward-Forward |\n|--------|-----------------|------------------|\n| Gradient flow | Through all layers | None between layers |\n| Learning signal | Global loss | Local goodness |\n| Hardware fit | Poor | Good |\n| Inference passes | 1 | N (one per class) |\n| Accuracy | Higher | Lower (trade-off) |\n\n### Hardware Compatibility\n\n| Operation | Standard Mode | Hardware Mode |\n|-----------|---------------|---------------|\n| Goodness | mean(xÂ²) | mean(\\|x\\|) |\n| Normalization | L2 norm | Lateral inhibition |\n| Loss | Softplus | Hinge |\n| Update | Adam + autograd | Hebbian |\n\n### Next Steps\n\n- Try `HARDWARE_MODE = True` to see hardware-compatible training\n- Experiment with different network sizes (hidden_dim)\n- Compare results with the backprop version in `02_train_a_model.ipynb`\n- Apply to MNIST (see Tutorial 04)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}