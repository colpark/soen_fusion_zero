{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Tutorial 02-FF — Train a SOEN Model with Forward-Forward Learning\n\nThis tutorial demonstrates **Forward-Forward (FF) learning** integrated with **actual SOEN dynamics**.\n\nUnlike the separate MLP-based FF implementation, this version:\n- Uses **real SOEN SingleDendrite layers** with temporal processing\n- Maintains **64-timestep sequential input** (not flattened)\n- Computes **goodness from SOEN neuron states**\n- Updates **J matrices** (connection weights) using local FF learning\n\n---\n\n## Architecture Comparison\n\n| Aspect | SOEN Backprop | **SOEN + FF (This Tutorial)** |\n|--------|---------------|-------------------------------|\n| Layers | SingleDendrite | **SingleDendrite** (same!) |\n| Input | 1D × 64 timesteps | **3D × 64 timesteps** (signal + label) |\n| Hidden | 5 neurons | **5 neurons** (same!) |\n| Recurrence | J_1_to_1 | **J_1_to_1** (same!) |\n| Learning | Backprop | **Forward-Forward** (local) |\n| Parameters | ~27 | **~45** (15 + 20 + 10) |\n\n---\n\n## How FF-SOEN Works\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    FORWARD-FORWARD + SOEN                       │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  Input at each timestep: [signal, label_0, label_1]             │\n│                                                                 │\n│  POSITIVE: signal + CORRECT label → SOEN dynamics → HIGH goodness│\n│  NEGATIVE: signal + WRONG label   → SOEN dynamics → LOW goodness │\n│                                                                 │\n│  Goodness = mean(s²) from SingleDendrite neuron states          │\n│  Local learning updates J matrices without backprop             │\n│                                                                 │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n---\n\n## ML Task: Pulse Classification (Same as Tutorial 02)\n- **Class 0**: Single pulse\n- **Class 1**: Two pulses"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "We import the necessary libraries and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 1: IMPORTS AND SETUP\n# ==============================================================================\n# Import SOEN toolkit and required libraries\n# ==============================================================================\n\nimport sys\nfrom pathlib import Path\n\n# Add src directory to path\nnotebook_dir = Path.cwd()\nfor parent in [notebook_dir] + list(notebook_dir.parents):\n    candidate = parent / \"src\"\n    if (candidate / \"soen_toolkit\").exists():\n        sys.path.insert(0, str(candidate))\n        break\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import Dict, Tuple, List\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\n# Import SOEN model builder\nfrom soen_toolkit.core.model_yaml import build_model_from_yaml\n\n# Set device\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Using device: {DEVICE}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "We define all hyperparameters and settings in a single configuration class.\n",
    "\n",
    "### Hardware Mode Options\n",
    "\n",
    "| Mode | Description | Accuracy | Hardware Ready |\n",
    "|------|-------------|----------|----------------|\n",
    "| `HARDWARE_MODE = False` | Uses autograd + Adam | Higher | No |\n",
    "| `HARDWARE_MODE = True` | Uses Hebbian learning | Lower | Yes |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 2: CONFIGURATION\n# ==============================================================================\n# Configuration for FF-SOEN training\n#\n# Key difference from standard FF:\n#   - Uses actual SOEN model with temporal processing\n#   - Label embedded in input at each timestep (3D input)\n#   - Goodness computed from SOEN neuron states\n# ==============================================================================\n\n@dataclass\nclass FFSOENConfig:\n    \"\"\"Configuration for Forward-Forward training with SOEN.\"\"\"\n    \n    # Model specification\n    model_spec: str = \"training/test_models/model_specs/3D_5D_2D_PulseNetSpec_FF.yaml\"\n    \n    # Dataset\n    data_path: str = \"training/datasets/soen_seq_task_one_or_two_pulses_seq64.hdf5\"\n    seq_len: int = 64\n    signal_dim: int = 1       # Original signal dimension\n    num_classes: int = 2      # Binary classification\n    input_dim: int = 3        # signal (1) + label one-hot (2)\n    \n    # Architecture (from model spec)\n    hidden_dim: int = 5       # SingleDendrite layer dimension\n    \n    # Training\n    batch_size: int = 32\n    num_epochs: int = 100\n    learning_rate: float = 0.01   # For J matrix updates\n    \n    # Forward-Forward specific\n    threshold: float = 0.5    # Goodness threshold (adjusted for SOEN states)\n    goodness_type: str = \"mean_squared\"  # \"mean_squared\" or \"mean_abs\"\n\n\nconfig = FFSOENConfig()\n\n# Print configuration\nprint(\"=\"*60)\nprint(\"FF-SOEN CONFIGURATION\")\nprint(\"=\"*60)\nprint(f\"Model spec: {config.model_spec}\")\nprint(f\"Dataset: {config.data_path}\")\nprint(f\"\")\nprint(f\"Input: {config.input_dim}D × {config.seq_len} timesteps\")\nprint(f\"  - Signal: {config.signal_dim}D\")\nprint(f\"  - Label: {config.num_classes}D one-hot\")\nprint(f\"Hidden: {config.hidden_dim} SingleDendrite neurons\")\nprint(f\"Output: {config.num_classes} classes\")\nprint(f\"\")\nprint(f\"Batch size: {config.batch_size}\")\nprint(f\"Epochs: {config.num_epochs}\")\nprint(f\"Learning rate: {config.learning_rate}\")\nprint(f\"Threshold: {config.threshold}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 3: Load and Visualize Dataset\n",
    "\n",
    "We load the pulse classification dataset and visualize examples from each class.\n",
    "\n",
    "### Dataset Structure\n",
    "- **Shape**: `(N, T, D)` = `(samples, timesteps, features)`\n",
    "- **Class 0**: Single pulse in the signal\n",
    "- **Class 1**: Two pulses in the signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# STEP 3: LOAD AND VISUALIZE DATASET\n",
    "# ==============================================================================\n",
    "# This cell loads the HDF5 dataset and visualizes examples from each class.\n",
    "#\n",
    "# The dataset contains:\n",
    "#   - Class 0: Signals with ONE pulse\n",
    "#   - Class 1: Signals with TWO pulses\n",
    "#\n",
    "# Dataset format:\n",
    "#   - data: shape (N, T, D) where N=samples, T=timesteps, D=features\n",
    "#   - labels: shape (N,) with integer class labels\n",
    "# ==============================================================================\n",
    "\n",
    "def load_dataset(data_path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load train and validation data from HDF5 file.\n",
    "    \n",
    "    Args:\n",
    "        data_path: Path to HDF5 dataset file\n",
    "        \n",
    "    Returns:\n",
    "        train_data, train_labels, val_data, val_labels\n",
    "    \"\"\"\n",
    "    with h5py.File(data_path, 'r') as f:\n",
    "        # Load training data\n",
    "        train_data = np.array(f['train']['data'])\n",
    "        train_labels = np.array(f['train']['labels'])\n",
    "        \n",
    "        # Load validation data (or use part of train if not available)\n",
    "        if 'val' in f:\n",
    "            val_data = np.array(f['val']['data'])\n",
    "            val_labels = np.array(f['val']['labels'])\n",
    "        else:\n",
    "            # Split train data\n",
    "            split_idx = int(len(train_data) * 0.8)\n",
    "            val_data = train_data[split_idx:]\n",
    "            val_labels = train_labels[split_idx:]\n",
    "            train_data = train_data[:split_idx]\n",
    "            train_labels = train_labels[:split_idx]\n",
    "    \n",
    "    return train_data, train_labels, val_data, val_labels\n",
    "\n",
    "\n",
    "def visualize_dataset(data: np.ndarray, labels: np.ndarray, n_examples: int = 4):\n",
    "    \"\"\"\n",
    "    Visualize examples from each class.\n",
    "    \n",
    "    Args:\n",
    "        data: Input data of shape (N, T, D)\n",
    "        labels: Class labels of shape (N,)\n",
    "        n_examples: Number of examples per class to show\n",
    "    \"\"\"\n",
    "    print(f\"Dataset shape: {data.shape} (N samples, T timesteps, D features)\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    print(f\"Class distribution: {np.bincount(labels)}\")\n",
    "    \n",
    "    # Find examples of each class\n",
    "    class_0_idx = np.where(labels == 0)[0][:n_examples]\n",
    "    class_1_idx = np.where(labels == 1)[0][:n_examples]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, n_examples, figsize=(3*n_examples, 5))\n",
    "    fig.suptitle(\"Input Signals: One-Pulse (Class 0) vs Two-Pulse (Class 1)\", \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Plot Class 0 (single pulse)\n",
    "    for i, idx in enumerate(class_0_idx):\n",
    "        axes[0, i].plot(data[idx, :, 0], 'b-', linewidth=1.5)\n",
    "        axes[0, i].set_title(f\"Sample {idx}\", fontsize=10)\n",
    "        axes[0, i].set_ylim(-0.1, 1.1)\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        if i == 0:\n",
    "            axes[0, i].set_ylabel(\"Class 0\\n(One Pulse)\", fontsize=10)\n",
    "    \n",
    "    # Plot Class 1 (two pulses)\n",
    "    for i, idx in enumerate(class_1_idx):\n",
    "        axes[1, i].plot(data[idx, :, 0], 'r-', linewidth=1.5)\n",
    "        axes[1, i].set_title(f\"Sample {idx}\", fontsize=10)\n",
    "        axes[1, i].set_ylim(-0.1, 1.1)\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "        if i == 0:\n",
    "            axes[1, i].set_ylabel(\"Class 1\\n(Two Pulses)\", fontsize=10)\n",
    "        axes[1, i].set_xlabel(\"Time step\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "train_data, train_labels, val_data, val_labels = load_dataset(config.data_path)\n",
    "\n",
    "print(f\"\\nTrain set: {train_data.shape[0]} samples\")\n",
    "print(f\"Val set: {val_data.shape[0]} samples\")\n",
    "\n",
    "# Visualize\n",
    "visualize_dataset(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Step 4: Create FF-SOEN Dataset\n\nFor FF-SOEN, we embed the label at **each timestep**:\n\n```\nOriginal signal: [batch, 64, 1]  →  shape (B, T, 1)\n\nWith label embedding:\n  POSITIVE: [batch, 64, 3]  →  [signal, correct_label_0, correct_label_1]\n  NEGATIVE: [batch, 64, 3]  →  [signal, wrong_label_0, wrong_label_1]\n```\n\nThe label is **repeated at every timestep** so the SOEN neurons receive consistent label information throughout the temporal processing."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 4: CREATE FF-SOEN DATASET\n# ==============================================================================\n# Dataset that embeds labels temporally for SOEN processing.\n#\n# Key difference from standard FF:\n#   - Labels are embedded at EACH TIMESTEP (not flattened)\n#   - Output shape: [batch, seq_len, 3] where 3 = signal + one-hot label\n#   - Preserves temporal structure for SOEN dynamics\n# ==============================================================================\n\nclass PulseFFSOENDataset(Dataset):\n    \"\"\"\n    Pulse classification dataset for FF-SOEN learning.\n    \n    Embeds labels temporally: at each timestep, the input is\n    [signal_value, label_0, label_1] where label is one-hot.\n    \"\"\"\n    \n    def __init__(self, data: np.ndarray, labels: np.ndarray, num_classes: int = 2):\n        \"\"\"\n        Args:\n            data: Signal data of shape (N, T, D) where D=1\n            labels: Class labels of shape (N,)\n            num_classes: Number of classes (2 for pulse classification)\n        \"\"\"\n        self.data = torch.tensor(data, dtype=torch.float32)\n        self.labels = torch.tensor(labels, dtype=torch.long)\n        self.num_classes = num_classes\n        self.seq_len = data.shape[1]\n        \n        print(f\"Created FF-SOEN Dataset:\")\n        print(f\"  Original shape: {self.data.shape} (N, T, D)\")\n        print(f\"  Output shape: (N, {self.seq_len}, {1 + num_classes})\")\n    \n    def __len__(self) -> int:\n        return len(self.labels)\n    \n    def _embed_label_temporal(self, signal: torch.Tensor, label: int) -> torch.Tensor:\n        \"\"\"\n        Embed label at each timestep.\n        \n        Args:\n            signal: Signal of shape (T, 1)\n            label: Integer class label\n            \n        Returns:\n            Embedded tensor of shape (T, 3) = [signal, label_one_hot]\n        \"\"\"\n        seq_len = signal.shape[0]\n        \n        # Create one-hot label and repeat for all timesteps\n        one_hot = F.one_hot(torch.tensor(label), num_classes=self.num_classes).float()\n        one_hot_repeated = one_hot.unsqueeze(0).expand(seq_len, -1)  # (T, 2)\n        \n        # Concatenate: [signal, label_0, label_1]\n        embedded = torch.cat([signal, one_hot_repeated], dim=1)  # (T, 3)\n        return embedded\n    \n    def _get_wrong_label(self, true_label: int) -> int:\n        \"\"\"Get a wrong label for negative example.\"\"\"\n        if self.num_classes == 2:\n            return 1 - true_label\n        else:\n            wrong = torch.randint(0, self.num_classes - 1, (1,)).item()\n            if wrong >= true_label:\n                wrong += 1\n            return wrong\n    \n    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Get a sample with positive and negative versions.\n        \n        Returns:\n            Dictionary with:\n                - positive: signal with correct label, shape (T, 3)\n                - negative: signal with wrong label, shape (T, 3)\n                - label: true class label\n                - signal: original signal (T, 1)\n        \"\"\"\n        signal = self.data[idx]  # (T, 1)\n        true_label = self.labels[idx].item()\n        wrong_label = self._get_wrong_label(true_label)\n        \n        return {\n            \"positive\": self._embed_label_temporal(signal, true_label),\n            \"negative\": self._embed_label_temporal(signal, wrong_label),\n            \"label\": self.labels[idx],\n            \"signal\": signal,\n        }\n\n\n# Create datasets\nprint(\"Creating training dataset...\")\ntrain_dataset = PulseFFSOENDataset(train_data, train_labels, num_classes=config.num_classes)\n\nprint(\"\\nCreating validation dataset...\")\nval_dataset = PulseFFSOENDataset(val_data, val_labels, num_classes=config.num_classes)\n\n# Create data loaders\ntrain_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n\n# Verify a sample\nprint(\"\\n\" + \"=\"*60)\nprint(\"SAMPLE VERIFICATION\")\nprint(\"=\"*60)\nsample = train_dataset[0]\nprint(f\"Positive shape: {sample['positive'].shape}  (T, 3)\")\nprint(f\"Negative shape: {sample['negative'].shape}  (T, 3)\")\nprint(f\"True label: {sample['label'].item()}\")\nprint(f\"\")\nprint(f\"Timestep 0:\")\nprint(f\"  Signal value: {sample['positive'][0, 0].item():.4f}\")\nprint(f\"  Positive label part: {sample['positive'][0, 1:].tolist()}\")\nprint(f\"  Negative label part: {sample['negative'][0, 1:].tolist()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Step 5: Load SOEN Model\n\nWe load the actual SOEN model from the YAML specification. This gives us:\n- **SingleDendrite layers** with real superconductor dynamics\n- **J matrices** (connection weights) that we'll train with FF\n- **Temporal processing** with recurrence"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 5: LOAD SOEN MODEL\n# ==============================================================================\n# Load the actual SOEN model from YAML specification.\n#\n# The model has:\n#   - Layer 0 (Input): dim=3 (signal + label)\n#   - Layer 1 (SingleDendrite): dim=5 (hidden layer with SOEN dynamics)\n#   - Layer 2 (Output): dim=2 (for readout)\n#\n# Connection matrices (J):\n#   - J_0_to_1: [5, 3] = 15 weights\n#   - J_1_to_1: [5, 5] - 5 diagonal = 20 weights (recurrent)\n#   - J_1_to_2: [2, 5] = 10 weights\n# ==============================================================================\n\n# Build SOEN model from YAML\nmodel_path = Path(config.model_spec)\nmodel = build_model_from_yaml(model_path)\nmodel = model.to(DEVICE)\n\n# Enable state tracking for goodness computation\nfor layer in model.layers:\n    if hasattr(layer, 'set_tracking_flags'):\n        layer.set_tracking_flags(phi=True, g=True, s=True)\n\nprint(\"=\"*60)\nprint(\"SOEN MODEL LOADED\")\nprint(\"=\"*60)\nprint(f\"\\nLayers:\")\nfor i, layer in enumerate(model.layers):\n    layer_type = type(layer).__name__\n    if hasattr(layer, 'dim'):\n        print(f\"  Layer {i}: {layer_type}, dim={layer.dim}\")\n    else:\n        print(f\"  Layer {i}: {layer_type}\")\n\nprint(f\"\\nConnections (J matrices):\")\nfor key in model.connections:\n    J = model.connections[key]\n    print(f\"  {key}: shape={list(J.shape)}, params={J.numel()}\")\n\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nTotal trainable parameters: {total_params}\")\nprint(\"=\"*60)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "## Step 6: Define FF-SOEN Training Functions\n\nWe define functions to:\n1. **Compute goodness** from SOEN neuron states\n2. **Run forward pass** through SOEN with label-embedded input\n3. **Compute FF loss** (push positive goodness up, negative down)\n4. **Update J matrices** based on local gradients\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│  SOEN Forward Pass with FF Learning                             │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                 │\n│  Input [B, T, 3] → Layer 0 → J_0_to_1 → Layer 1 (SingleDendrite)│\n│                                  ↺ J_1_to_1 (recurrent)         │\n│                                                                 │\n│  Goodness = mean(s²) from Layer 1 states over all timesteps     │\n│                                                                 │\n│  Loss = softplus(θ - g_pos) + softplus(g_neg - θ)               │\n│                                                                 │\n└─────────────────────────────────────────────────────────────────┘\n```"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 6: FF-SOEN TRAINING FUNCTIONS\n# ==============================================================================\n# Functions for Forward-Forward learning with SOEN model.\n#\n# Key functions:\n#   - compute_goodness: Extract goodness from SOEN layer states\n#   - ff_loss: Forward-Forward loss function\n#   - get_layer_states: Get neuron states from SOEN layers\n# ==============================================================================\n\ndef compute_goodness(states: torch.Tensor, goodness_type: str = \"mean_squared\") -> torch.Tensor:\n    \"\"\"\n    Compute goodness from SOEN neuron states.\n    \n    Args:\n        states: Neuron states of shape (batch, seq_len, dim)\n        goodness_type: \"mean_squared\" or \"mean_abs\"\n        \n    Returns:\n        Goodness per sample, shape (batch,)\n    \"\"\"\n    if goodness_type == \"mean_squared\":\n        # Sum of squared activities across neurons and time\n        return (states ** 2).mean(dim=(1, 2))\n    else:  # mean_abs\n        return states.abs().mean(dim=(1, 2))\n\n\ndef ff_loss(pos_goodness: torch.Tensor, neg_goodness: torch.Tensor, \n            threshold: float) -> torch.Tensor:\n    \"\"\"\n    Compute Forward-Forward loss.\n    \n    Goal: Push positive goodness ABOVE threshold, negative BELOW.\n    \n    Args:\n        pos_goodness: Goodness for positive data, shape (batch,)\n        neg_goodness: Goodness for negative data, shape (batch,)\n        threshold: Goodness threshold\n        \n    Returns:\n        Scalar loss value\n    \"\"\"\n    # Softplus loss: smooth approximation\n    pos_loss = F.softplus(-(pos_goodness - threshold))  # Want pos > threshold\n    neg_loss = F.softplus(neg_goodness - threshold)     # Want neg < threshold\n    return (pos_loss + neg_loss).mean()\n\n\ndef forward_and_get_states(model, x: torch.Tensor) -> Tuple[torch.Tensor, List[torch.Tensor]]:\n    \"\"\"\n    Run forward pass and extract layer states.\n    \n    Args:\n        model: SOEN model\n        x: Input tensor of shape (batch, seq_len, input_dim)\n        \n    Returns:\n        output: Model output\n        layer_states: List of state tensors for each layer\n    \"\"\"\n    # Reset stateful components before forward pass\n    if hasattr(model, 'reset_stateful_components'):\n        model.reset_stateful_components()\n    \n    # Forward pass\n    output, all_outputs = model(x)\n    \n    # Extract states from layers\n    layer_states = []\n    for i, layer in enumerate(model.layers):\n        if hasattr(layer, 'get_state_history'):\n            states = layer.get_state_history()  # (batch, seq_len+1, dim)\n            if states is not None:\n                # Remove initial state (first timestep)\n                states = states[:, 1:, :]  # (batch, seq_len, dim)\n                layer_states.append(states)\n        elif i < len(all_outputs):\n            # Fallback: use layer output as state\n            layer_states.append(all_outputs[i])\n    \n    return output, layer_states\n\n\ndef predict_multipass(model, signals: torch.Tensor, num_classes: int) -> torch.Tensor:\n    \"\"\"\n    Predict using multi-pass inference (one pass per class).\n    \n    For each class, embed that label and compute total goodness.\n    Predict the class with highest goodness.\n    \n    Args:\n        model: SOEN model\n        signals: Original signals of shape (batch, seq_len, 1)\n        num_classes: Number of classes\n        \n    Returns:\n        Predicted class labels, shape (batch,)\n    \"\"\"\n    batch_size = signals.shape[0]\n    seq_len = signals.shape[1]\n    device = signals.device\n    \n    all_goodness = torch.zeros(batch_size, num_classes, device=device)\n    \n    for label in range(num_classes):\n        # Create one-hot label repeated at each timestep\n        one_hot = F.one_hot(torch.tensor([label], device=device), num_classes).float()\n        one_hot_repeated = one_hot.unsqueeze(1).expand(batch_size, seq_len, -1)\n        \n        # Embed label in input\n        x = torch.cat([signals, one_hot_repeated], dim=2)  # (batch, seq_len, 3)\n        \n        # Forward pass\n        _, layer_states = forward_and_get_states(model, x)\n        \n        # Compute goodness from hidden layer (layer 1)\n        if len(layer_states) > 1:\n            hidden_states = layer_states[1]  # SingleDendrite layer states\n        else:\n            hidden_states = layer_states[0]\n        \n        goodness = compute_goodness(hidden_states, config.goodness_type)\n        all_goodness[:, label] = goodness\n    \n    return all_goodness.argmax(dim=1)\n\n\n# Test the functions\nprint(\"Testing FF-SOEN functions...\")\nsample = train_dataset[0]\ntest_pos = sample[\"positive\"].unsqueeze(0).to(DEVICE)  # (1, T, 3)\ntest_neg = sample[\"negative\"].unsqueeze(0).to(DEVICE)\n\n# Forward pass\noutput_pos, states_pos = forward_and_get_states(model, test_pos)\noutput_neg, states_neg = forward_and_get_states(model, test_neg)\n\nprint(f\"Output shape: {output_pos.shape}\")\nprint(f\"Number of layer states: {len(states_pos)}\")\nfor i, s in enumerate(states_pos):\n    print(f\"  Layer {i} states shape: {s.shape}\")\n\n# Compute goodness\nif len(states_pos) > 1:\n    g_pos = compute_goodness(states_pos[1], config.goodness_type)\n    g_neg = compute_goodness(states_neg[1], config.goodness_type)\nelse:\n    g_pos = compute_goodness(states_pos[0], config.goodness_type)\n    g_neg = compute_goodness(states_neg[0], config.goodness_type)\n\nprint(f\"\\nGoodness (positive): {g_pos.item():.4f}\")\nprint(f\"Goodness (negative): {g_neg.item():.4f}\")\nprint(f\"FF Loss: {ff_loss(g_pos, g_neg, config.threshold).item():.4f}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": "## Step 7: Training Loop\n\nThe FF-SOEN training loop:\n\n1. **Positive pass**: Forward with correct label → compute positive goodness\n2. **Negative pass**: Forward with wrong label → compute negative goodness  \n3. **FF loss**: Push positive above threshold, negative below\n4. **Update**: Gradient descent on J matrices (connection weights)\n\n**Key difference from backprop**: We use the same loss for all layers (goodness-based),\nnot a global cross-entropy loss at the output."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 7: FF-SOEN TRAINING LOOP\n# ==============================================================================\n# Train the SOEN model using Forward-Forward learning.\n#\n# Training flow:\n#   For each batch:\n#     1. Forward positive data → compute positive goodness\n#     2. Forward negative data → compute negative goodness\n#     3. Compute FF loss\n#     4. Backprop through goodness computation (NOT through all layers)\n#     5. Update J matrices\n# ==============================================================================\n\ndef train_ff_soen(\n    model,\n    train_loader: DataLoader,\n    val_loader: DataLoader,\n    config: FFSOENConfig,\n    device: torch.device,\n) -> Dict:\n    \"\"\"\n    Train SOEN model using Forward-Forward algorithm.\n    \n    Args:\n        model: SOEN model to train\n        train_loader: Training data loader\n        val_loader: Validation data loader\n        config: Training configuration\n        device: Device to train on\n        \n    Returns:\n        Training history dictionary\n    \"\"\"\n    model.to(device)\n    model.train()\n    \n    # Optimizer for all trainable parameters (J matrices)\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    \n    # Training history\n    history = {\n        \"train_loss\": [],\n        \"val_acc\": [],\n        \"pos_goodness\": [],\n        \"neg_goodness\": [],\n    }\n    \n    print(f\"\\n{'='*60}\")\n    print(\"FF-SOEN TRAINING\")\n    print(f\"{'='*60}\")\n    print(f\"Epochs: {config.num_epochs}\")\n    print(f\"Learning rate: {config.learning_rate}\")\n    print(f\"Threshold: {config.threshold}\")\n    print(f\"Goodness type: {config.goodness_type}\")\n    print(f\"{'='*60}\\n\")\n    \n    for epoch in range(config.num_epochs):\n        model.train()\n        \n        epoch_loss = 0.0\n        epoch_pos_g = 0.0\n        epoch_neg_g = 0.0\n        num_batches = 0\n        \n        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n        \n        for batch in pbar:\n            pos_data = batch[\"positive\"].to(device)  # (batch, seq_len, 3)\n            neg_data = batch[\"negative\"].to(device)\n            \n            optimizer.zero_grad()\n            \n            # ===== POSITIVE PASS =====\n            _, states_pos = forward_and_get_states(model, pos_data)\n            \n            # Get hidden layer states (layer 1 = SingleDendrite)\n            if len(states_pos) > 1:\n                hidden_pos = states_pos[1]\n            else:\n                hidden_pos = states_pos[0]\n            \n            pos_goodness = compute_goodness(hidden_pos, config.goodness_type)\n            \n            # ===== NEGATIVE PASS =====\n            _, states_neg = forward_and_get_states(model, neg_data)\n            \n            if len(states_neg) > 1:\n                hidden_neg = states_neg[1]\n            else:\n                hidden_neg = states_neg[0]\n            \n            neg_goodness = compute_goodness(hidden_neg, config.goodness_type)\n            \n            # ===== FF LOSS =====\n            loss = ff_loss(pos_goodness, neg_goodness, config.threshold)\n            \n            # ===== UPDATE =====\n            loss.backward()\n            \n            # Clip gradients for stability\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            \n            optimizer.step()\n            \n            # Track metrics\n            epoch_loss += loss.item()\n            epoch_pos_g += pos_goodness.mean().item()\n            epoch_neg_g += neg_goodness.mean().item()\n            num_batches += 1\n            \n            pbar.set_postfix({\n                \"loss\": f\"{loss.item():.4f}\",\n                \"g+\": f\"{pos_goodness.mean().item():.3f}\",\n                \"g-\": f\"{neg_goodness.mean().item():.3f}\",\n            })\n        \n        # Epoch averages\n        avg_loss = epoch_loss / num_batches\n        avg_pos_g = epoch_pos_g / num_batches\n        avg_neg_g = epoch_neg_g / num_batches\n        \n        history[\"train_loss\"].append(avg_loss)\n        history[\"pos_goodness\"].append(avg_pos_g)\n        history[\"neg_goodness\"].append(avg_neg_g)\n        \n        # Evaluate every 10 epochs\n        if (epoch + 1) % 10 == 0 or epoch == 0 or epoch == config.num_epochs - 1:\n            val_acc = evaluate_ff_soen(model, val_loader, config.num_classes, device)\n            history[\"val_acc\"].append(val_acc)\n            \n            sep = avg_pos_g - avg_neg_g\n            print(f\"\\nEpoch {epoch+1}: Loss={avg_loss:.4f}, \"\n                  f\"g+={avg_pos_g:.3f}, g-={avg_neg_g:.3f}, sep={sep:+.3f}, \"\n                  f\"Val Acc={val_acc:.1%}\")\n    \n    return history\n\n\ndef evaluate_ff_soen(model, data_loader: DataLoader, num_classes: int, \n                     device: torch.device) -> float:\n    \"\"\"\n    Evaluate FF-SOEN model accuracy using multi-pass inference.\n    \"\"\"\n    model.eval()\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            signals = batch[\"signal\"].to(device)  # (batch, seq_len, 1)\n            labels = batch[\"label\"].to(device)\n            \n            predictions = predict_multipass(model, signals, num_classes)\n            \n            correct += (predictions == labels).sum().item()\n            total += labels.size(0)\n    \n    return correct / total"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# TRAIN THE MODEL\n# ==============================================================================\n# Run FF-SOEN training.\n#\n# Watch for:\n#   - Goodness separation (g+ should become > g-)\n#   - Validation accuracy improvement\n#   - Loss decreasing\n# ==============================================================================\n\nhistory = train_ff_soen(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    config=config,\n    device=DEVICE,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 8: Visualize Training Progress\n",
    "\n",
    "We plot the training metrics to understand how learning progressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 8: VISUALIZE TRAINING PROGRESS\n# ==============================================================================\n# Plot training metrics for FF-SOEN.\n# ==============================================================================\n\ndef plot_ff_soen_history(history: Dict, config: FFSOENConfig):\n    \"\"\"Plot FF-SOEN training metrics.\"\"\"\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    fig.suptitle('FF-SOEN Training Progress', fontsize=14, fontweight='bold')\n    \n    # 1. Training loss\n    axes[0].plot(history[\"train_loss\"], 'b-', linewidth=2)\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('FF Loss')\n    axes[0].set_title('Training Loss')\n    axes[0].grid(True, alpha=0.3)\n    \n    # 2. Goodness separation\n    epochs = range(len(history[\"pos_goodness\"]))\n    axes[1].plot(epochs, history[\"pos_goodness\"], 'g-', linewidth=2, label='Positive')\n    axes[1].plot(epochs, history[\"neg_goodness\"], 'r-', linewidth=2, label='Negative')\n    axes[1].axhline(y=config.threshold, color='black', linestyle='--', \n                    linewidth=1, label=f'Threshold ({config.threshold})')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('Goodness')\n    axes[1].set_title('Goodness Separation')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    # 3. Validation accuracy\n    if len(history[\"val_acc\"]) > 0:\n        eval_epochs = [0] + list(range(9, len(history[\"train_loss\"]), 10))\n        eval_epochs = eval_epochs[:len(history[\"val_acc\"])]\n        if len(eval_epochs) < len(history[\"val_acc\"]):\n            eval_epochs.append(len(history[\"train_loss\"]) - 1)\n        axes[2].plot(eval_epochs[:len(history[\"val_acc\"])], history[\"val_acc\"], \n                     'go-', linewidth=2, markersize=6)\n        axes[2].set_xlabel('Epoch')\n        axes[2].set_ylabel('Accuracy')\n        axes[2].set_title('Validation Accuracy')\n        axes[2].set_ylim([0, 1.05])\n        axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print summary\n    print(f\"\\n{'='*60}\")\n    print(\"TRAINING SUMMARY\")\n    print(f\"{'='*60}\")\n    if len(history[\"val_acc\"]) > 0:\n        print(f\"Final validation accuracy: {history['val_acc'][-1]:.1%}\")\n        print(f\"Best validation accuracy: {max(history['val_acc']):.1%}\")\n    print(f\"Final goodness separation: {history['pos_goodness'][-1] - history['neg_goodness'][-1]:+.4f}\")\n    print(f\"  Positive: {history['pos_goodness'][-1]:.4f}\")\n    print(f\"  Negative: {history['neg_goodness'][-1]:.4f}\")\n\n\nplot_ff_soen_history(history, config)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Step 9: Visualize Predictions\n",
    "\n",
    "Let's see how the model performs on individual samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 9: VISUALIZE PREDICTIONS\n# ==============================================================================\n# Show model predictions on individual samples.\n# ==============================================================================\n\ndef visualize_ff_soen_predictions(model, dataset, num_classes: int, \n                                   n_samples: int = 8, device=DEVICE):\n    \"\"\"Visualize FF-SOEN model predictions.\"\"\"\n    \n    model.eval()\n    \n    indices = np.random.choice(len(dataset), min(n_samples, len(dataset)), replace=False)\n    \n    n_cols = min(4, n_samples)\n    n_rows = (n_samples + n_cols - 1) // n_cols\n    \n    fig, axes = plt.subplots(n_rows, n_cols, figsize=(3.5*n_cols, 3*n_rows))\n    if n_rows == 1:\n        axes = axes.reshape(1, -1)\n    \n    fig.suptitle('FF-SOEN Predictions (Multi-Pass Inference)', fontsize=14, fontweight='bold')\n    \n    class_names = [\"One Pulse\", \"Two Pulses\"]\n    correct_count = 0\n    \n    with torch.no_grad():\n        for i, idx in enumerate(indices):\n            sample = dataset[idx]\n            signal = sample[\"signal\"].unsqueeze(0).to(device)  # (1, T, 1)\n            label = sample[\"label\"].item()\n            \n            prediction = predict_multipass(model, signal, num_classes).item()\n            \n            is_correct = prediction == label\n            correct_count += is_correct\n            \n            row, col = i // n_cols, i % n_cols\n            ax = axes[row, col]\n            \n            ax.plot(sample[\"signal\"][:, 0].numpy(), 'b-', linewidth=1.5)\n            ax.set_ylim(-0.1, 1.1)\n            ax.grid(True, alpha=0.3)\n            \n            color = 'green' if is_correct else 'red'\n            symbol = '✓' if is_correct else '✗'\n            \n            ax.set_title(\n                f\"{symbol} Pred: {class_names[prediction]}\\nTrue: {class_names[label]}\",\n                fontsize=9, color=color,\n                fontweight='bold' if not is_correct else 'normal'\n            )\n            \n            if col == 0:\n                ax.set_ylabel(\"Signal\")\n            if row == n_rows - 1:\n                ax.set_xlabel(\"Time step\")\n    \n    for i in range(n_samples, n_rows * n_cols):\n        row, col = i // n_cols, i % n_cols\n        axes[row, col].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nSample accuracy: {correct_count}/{n_samples} ({correct_count/n_samples:.1%})\")\n\n\nvisualize_ff_soen_predictions(model, val_dataset, config.num_classes, n_samples=8)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## Step 10: Analyze Goodness Distributions\n",
    "\n",
    "Let's see how well the layers separate positive and negative goodness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# ==============================================================================\n# STEP 10: ANALYZE GOODNESS AND SOEN STATES\n# ==============================================================================\n# Analyze how the SOEN neuron states differ for positive vs negative data.\n# ==============================================================================\n\ndef analyze_soen_states(model, data_loader, config, device=DEVICE):\n    \"\"\"Analyze SOEN neuron states and goodness distributions.\"\"\"\n    \n    model.eval()\n    \n    all_pos_goodness = []\n    all_neg_goodness = []\n    all_pos_states = []\n    all_neg_states = []\n    \n    with torch.no_grad():\n        for batch in data_loader:\n            pos_data = batch[\"positive\"].to(device)\n            neg_data = batch[\"negative\"].to(device)\n            \n            # Positive pass\n            _, states_pos = forward_and_get_states(model, pos_data)\n            if len(states_pos) > 1:\n                hidden_pos = states_pos[1]\n            else:\n                hidden_pos = states_pos[0]\n            \n            g_pos = compute_goodness(hidden_pos, config.goodness_type)\n            all_pos_goodness.extend(g_pos.cpu().numpy())\n            all_pos_states.append(hidden_pos.cpu())\n            \n            # Negative pass\n            _, states_neg = forward_and_get_states(model, neg_data)\n            if len(states_neg) > 1:\n                hidden_neg = states_neg[1]\n            else:\n                hidden_neg = states_neg[0]\n            \n            g_neg = compute_goodness(hidden_neg, config.goodness_type)\n            all_neg_goodness.extend(g_neg.cpu().numpy())\n            all_neg_states.append(hidden_neg.cpu())\n    \n    all_pos_goodness = np.array(all_pos_goodness)\n    all_neg_goodness = np.array(all_neg_goodness)\n    all_pos_states = torch.cat(all_pos_states, dim=0)  # (N, T, D)\n    all_neg_states = torch.cat(all_neg_states, dim=0)\n    \n    # Plot\n    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n    fig.suptitle('SOEN State Analysis', fontsize=14, fontweight='bold')\n    \n    # 1. Goodness distributions\n    axes[0].hist(all_pos_goodness, bins=50, alpha=0.6, color='green', \n                 label=f'Positive (μ={all_pos_goodness.mean():.3f})', density=True)\n    axes[0].hist(all_neg_goodness, bins=50, alpha=0.6, color='red', \n                 label=f'Negative (μ={all_neg_goodness.mean():.3f})', density=True)\n    axes[0].axvline(x=config.threshold, color='black', linestyle='--', \n                    linewidth=2, label=f'Threshold ({config.threshold})')\n    axes[0].set_xlabel('Goodness')\n    axes[0].set_ylabel('Density')\n    axes[0].set_title('Goodness Distributions')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n    \n    # 2. Mean neuron activity over time\n    mean_pos = all_pos_states.mean(dim=(0, 2)).numpy()  # (T,)\n    mean_neg = all_neg_states.mean(dim=(0, 2)).numpy()\n    axes[1].plot(mean_pos, 'g-', linewidth=2, label='Positive')\n    axes[1].plot(mean_neg, 'r-', linewidth=2, label='Negative')\n    axes[1].set_xlabel('Timestep')\n    axes[1].set_ylabel('Mean Neuron Activity')\n    axes[1].set_title('SOEN State Dynamics')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    \n    # 3. Per-neuron activity comparison\n    pos_per_neuron = all_pos_states.mean(dim=(0, 1)).numpy()  # (D,)\n    neg_per_neuron = all_neg_states.mean(dim=(0, 1)).numpy()\n    x = np.arange(len(pos_per_neuron))\n    width = 0.35\n    axes[2].bar(x - width/2, pos_per_neuron, width, color='green', alpha=0.7, label='Positive')\n    axes[2].bar(x + width/2, neg_per_neuron, width, color='red', alpha=0.7, label='Negative')\n    axes[2].set_xlabel('Neuron Index')\n    axes[2].set_ylabel('Mean Activity')\n    axes[2].set_title('Per-Neuron Activity')\n    axes[2].legend()\n    axes[2].grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Print statistics\n    separation = all_pos_goodness.mean() - all_neg_goodness.mean()\n    print(f\"\\nGoodness Statistics:\")\n    print(f\"  Positive: mean={all_pos_goodness.mean():.4f}, std={all_pos_goodness.std():.4f}\")\n    print(f\"  Negative: mean={all_neg_goodness.mean():.4f}, std={all_neg_goodness.std():.4f}\")\n    print(f\"  Separation: {separation:+.4f}\")\n\n\nanalyze_soen_states(model, val_loader, config)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": "## Summary\n\n### What We Achieved\n\nThis tutorial demonstrates **true FF-SOEN integration**:\n\n1. **Real SOEN dynamics**: Uses actual SingleDendrite layers with superconductor physics\n2. **Temporal processing**: 64-timestep sequences (not flattened like standard FF)\n3. **Label embedding**: One-hot labels at each timestep `[signal, label_0, label_1]`\n4. **Goodness from SOEN states**: `mean(s²)` computed from neuron state trajectories\n5. **J matrix updates**: Connection weights trained with FF local learning\n\n### Architecture Comparison\n\n| Component | SOEN Backprop | FF-SOEN (This Tutorial) |\n|-----------|---------------|-------------------------|\n| **Layers** | SingleDendrite | SingleDendrite (same!) |\n| **Input dim** | 1 | 3 (signal + label) |\n| **Processing** | Temporal (64 steps) | Temporal (64 steps) |\n| **Recurrence** | Yes (J_1_to_1) | Yes (J_1_to_1) |\n| **Learning** | Backprop + CE loss | FF + goodness loss |\n| **J_0_to_1** | 5 params | 15 params |\n| **J_1_to_1** | 20 params | 20 params |\n| **J_1_to_2** | 2 params | 10 params |\n\n### Key Insight\n\nForward-Forward learning is a **learning algorithm**, not an architecture. By integrating FF with SOEN:\n- We preserve the **temporal dynamics** and **recurrence** of SOEN\n- We replace **backpropagation** with **local goodness-based learning**\n- The model remains **hardware-compatible** (no weight transport needed)\n\n### Parameter Comparison\n\n| Connection | Original (1D input) | FF-SOEN (3D input) |\n|------------|---------------------|---------------------|\n| J_0_to_1 | 1×5 = 5 | 3×5 = **15** |\n| J_1_to_1 | 5×5-5 = 20 | 5×5-5 = **20** |\n| J_1_to_2 | one-to-one ≈ 2 | all-to-all = **10** |\n| **Total** | ~27 | ~**45** |\n\nThe extra parameters come from:\n- Expanded input (signal + label)\n- Full output connection (needed for proper gradient flow)\n\n### Next Steps\n\n- Compare accuracy with backprop version (Tutorial 02)\n- Try different goodness functions (`mean_abs` for hardware)\n- Experiment with threshold values\n- Apply to MNIST (Tutorial 04)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}