{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Network Complexity Study: Can More Neurons Learn Linear Regression?\n",
    "\n",
    "The SingleDendrite has nonlinear dynamics, so a single neuron cannot perfectly learn y = αx + β.\n",
    "\n",
    "**Question:** Can we approximate linear functions by adding more neurons?\n",
    "\n",
    "This is the fundamental principle of neural networks - the **Universal Approximation Theorem** states that a network with enough hidden units can approximate any continuous function.\n",
    "\n",
    "## Architectures to Compare\n",
    "\n",
    "All architectures use **only trainable connection weights (J)** - hardware compatible.\n",
    "\n",
    "| Model | Architecture | Hidden Neurons | Trainable Params |\n",
    "|-------|-------------|----------------|------------------|\n",
    "| A1 | 1 → 1 → 1 | 1 | 2 |\n",
    "| A2 | 1 → 2 → 1 | 2 | 4 |\n",
    "| A3 | 1 → 3 → 1 | 3 | 6 |\n",
    "| A5 | 1 → 5 → 1 | 5 | 10 |\n",
    "| A10 | 1 → 10 → 1 | 10 | 20 |\n",
    "| Deep | 1 → 3 → 3 → 1 | 6 (2 layers) | 15 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Linear Regression Task\n",
    "\n",
    "Target: y = 2.0·x + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "TRUE_ALPHA = 2.0\n",
    "TRUE_BETA = 0.5\n",
    "\n",
    "# Data\n",
    "N_SAMPLES = 100\n",
    "SEQ_LEN = 50\n",
    "\n",
    "x_values = torch.linspace(0.05, 0.20, N_SAMPLES)\n",
    "X_data = x_values.unsqueeze(1).unsqueeze(2).expand(-1, SEQ_LEN, 1).clone()\n",
    "y_data = (TRUE_ALPHA * x_values + TRUE_BETA).unsqueeze(1)\n",
    "\n",
    "print(f\"Task: y = {TRUE_ALPHA}·x + {TRUE_BETA}\")\n",
    "print(f\"Input range: [{x_values.min():.3f}, {x_values.max():.3f}]\")\n",
    "print(f\"Target range: [{y_data.min():.3f}, {y_data.max():.3f}]\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(x_values.numpy(), y_data.squeeze().numpy(), alpha=0.6)\n",
    "plt.plot(x_values.numpy(), TRUE_ALPHA * x_values.numpy() + TRUE_BETA, 'r-', lw=2)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title(f'Target: y = {TRUE_ALPHA}x + {TRUE_BETA}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 2. Model Builder Function\n",
    "\n",
    "Generic function to build SOEN models with varying complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_soen_model(hidden_dims, dt=50.0):\n",
    "    \"\"\"\n",
    "    Build a SOEN model with specified hidden layer dimensions.\n",
    "    \n",
    "    Args:\n",
    "        hidden_dims: List of hidden layer sizes, e.g., [3] or [3, 3]\n",
    "        \n",
    "    Architecture:\n",
    "        1 (input) → hidden_dims[0] → hidden_dims[1] → ... → 1 (output)\n",
    "    \n",
    "    All connections are trainable. All SingleDendrite params are fixed.\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=dt,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = []\n",
    "    connections = []\n",
    "    \n",
    "    # Layer 0: Input (dim=1)\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    ))\n",
    "    \n",
    "    # Hidden layers (SingleDendrite)\n",
    "    prev_dim = 1\n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        \n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "                \"learnable_params\": {\n",
    "                    \"phi_offset\": False,\n",
    "                    \"bias_current\": False,\n",
    "                    \"gamma_plus\": False,\n",
    "                    \"gamma_minus\": False,\n",
    "                },\n",
    "            },\n",
    "        ))\n",
    "        \n",
    "        # Connection from previous layer\n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\n",
    "                \"init\": \"xavier_uniform\",\n",
    "            },\n",
    "        ))\n",
    "        \n",
    "        prev_dim = hidden_dim\n",
    "    \n",
    "    # Output layer (dim=1)\n",
    "    output_layer_id = len(hidden_dims) + 1\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=output_layer_id,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": 1},\n",
    "    ))\n",
    "    \n",
    "    # Connection to output\n",
    "    connections.append(ConnectionConfig(\n",
    "        from_layer=output_layer_id - 1,\n",
    "        to_layer=output_layer_id,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\n",
    "            \"init\": \"xavier_uniform\",\n",
    "        },\n",
    "    ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def count_params(model):\n",
    "    \"\"\"Count trainable parameters.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# Test the builder\n",
    "print(\"Testing model builder...\")\n",
    "test_configs = [\n",
    "    ([1], \"1→1→1\"),\n",
    "    ([2], \"1→2→1\"),\n",
    "    ([3], \"1→3→1\"),\n",
    "    ([5], \"1→5→1\"),\n",
    "    ([10], \"1→10→1\"),\n",
    "    ([3, 3], \"1→3→3→1\"),\n",
    "]\n",
    "\n",
    "for hidden_dims, name in test_configs:\n",
    "    model = build_soen_model(hidden_dims)\n",
    "    n_params = count_params(model)\n",
    "    layer_dims = [l.dim for l in model.layers]\n",
    "    print(f\"  {name}: layers={layer_dims}, trainable_params={n_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 3. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, n_epochs=500, lr=0.01, verbose=False):\n",
    "    \"\"\"\n",
    "    Train a SOEN model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        final_hist, _ = model(X_train)\n",
    "        y_pred = final_hist[:, -1, :]\n",
    "        \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 100 == 0:\n",
    "            print(f\"  Epoch {epoch+1}: Loss = {loss.item():.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate model and return predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_hist, _ = model(X_test)\n",
    "        y_pred = final_hist[:, -1, :].squeeze().numpy()\n",
    "    \n",
    "    y_true = y_test.squeeze().numpy()\n",
    "    mse = np.mean((y_pred - y_true) ** 2)\n",
    "    \n",
    "    return y_pred, mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 4. Train All Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architectures to compare\n",
    "ARCHITECTURES = {\n",
    "    \"1 neuron\": [1],\n",
    "    \"2 neurons\": [2],\n",
    "    \"3 neurons\": [3],\n",
    "    \"5 neurons\": [5],\n",
    "    \"10 neurons\": [10],\n",
    "    \"20 neurons\": [20],\n",
    "    \"3→3 (deep)\": [3, 3],\n",
    "    \"5→5 (deep)\": [5, 5],\n",
    "}\n",
    "\n",
    "N_EPOCHS = 500\n",
    "LR = 0.02\n",
    "\n",
    "results = {}\n",
    "\n",
    "print(\"Training all architectures...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, hidden_dims in ARCHITECTURES.items():\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    \n",
    "    # Build model\n",
    "    model = build_soen_model(hidden_dims)\n",
    "    n_params = count_params(model)\n",
    "    \n",
    "    # Train\n",
    "    losses = train_model(model, X_data, y_data, n_epochs=N_EPOCHS, lr=LR, verbose=False)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred, mse = evaluate_model(model, X_data, y_data)\n",
    "    \n",
    "    results[name] = {\n",
    "        'hidden_dims': hidden_dims,\n",
    "        'n_params': n_params,\n",
    "        'losses': losses,\n",
    "        'final_loss': losses[-1],\n",
    "        'y_pred': y_pred,\n",
    "        'mse': mse,\n",
    "        'model': model,\n",
    "    }\n",
    "    \n",
    "    print(f\"  Params: {n_params}, Final Loss: {losses[-1]:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 5. Compare Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(results)))\n",
    "\n",
    "# Linear scale\n",
    "ax1 = axes[0]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    ax1.plot(res['losses'], label=f\"{name} ({res['n_params']} params)\", color=color, lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('MSE Loss')\n",
    "ax1.set_title('Training Loss (Linear Scale)')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "ax2 = axes[1]\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    ax2.plot(res['losses'], label=f\"{name}\", color=color, lw=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('MSE Loss (log)')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title('Training Loss (Log Scale)')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 6. Compare Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = x_values.numpy()\n",
    "y_true = y_data.squeeze().numpy()\n",
    "\n",
    "# Plot predictions for each architecture\n",
    "n_models = len(results)\n",
    "cols = 4\n",
    "rows = (n_models + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(16, 4*rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    ax.scatter(x_plot, y_true, alpha=0.4, s=20, color='gray', label='Target')\n",
    "    ax.plot(x_plot, TRUE_ALPHA * x_plot + TRUE_BETA, 'k--', lw=2, label='y=2x+0.5')\n",
    "    ax.scatter(x_plot, res['y_pred'], alpha=0.6, s=20, color='blue', label='Prediction')\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f\"{name}\\nMSE={res['mse']:.6f}, Params={res['n_params']}\")\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_models, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 7. Performance vs Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for plotting\n",
    "names = list(results.keys())\n",
    "n_params = [results[n]['n_params'] for n in names]\n",
    "final_losses = [results[n]['final_loss'] for n in names]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart of final losses\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(range(len(names)), final_losses, color=colors)\n",
    "ax1.set_xticks(range(len(names)))\n",
    "ax1.set_xticklabels(names, rotation=45, ha='right')\n",
    "ax1.set_ylabel('Final MSE Loss')\n",
    "ax1.set_title('Final Loss by Architecture')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{loss:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Scatter: params vs loss\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(n_params, final_losses, s=100, c=colors, edgecolors='black', zorder=5)\n",
    "for i, name in enumerate(names):\n",
    "    ax2.annotate(name, (n_params[i], final_losses[i]), \n",
    "                 textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "ax2.set_xlabel('Number of Trainable Parameters')\n",
    "ax2.set_ylabel('Final MSE Loss')\n",
    "ax2.set_title('Loss vs Model Complexity')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 8. Transfer Function Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended input range\n",
    "x_extended = torch.linspace(0.0, 0.30, 200)\n",
    "X_extended = x_extended.unsqueeze(1).unsqueeze(2).expand(-1, SEQ_LEN, 1).clone()\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# True linear function\n",
    "plt.plot(x_extended.numpy(), TRUE_ALPHA * x_extended.numpy() + TRUE_BETA, \n",
    "         'k-', lw=3, label=f'Target: y = {TRUE_ALPHA}x + {TRUE_BETA}', alpha=0.8)\n",
    "\n",
    "# Each model's transfer function\n",
    "for (name, res), color in zip(results.items(), colors):\n",
    "    model = res['model']\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_hist, _ = model(X_extended)\n",
    "        y_ext = final_hist[:, -1, :].squeeze().numpy()\n",
    "    plt.plot(x_extended.numpy(), y_ext, '--', color=color, lw=2, \n",
    "             label=f\"{name} (MSE={res['mse']:.4f})\")\n",
    "\n",
    "# Mark training region\n",
    "plt.axvspan(0.05, 0.20, alpha=0.15, color='green', label='Training region')\n",
    "\n",
    "plt.xlabel('Input x', fontsize=12)\n",
    "plt.ylabel('Output y', fontsize=12)\n",
    "plt.title('Transfer Functions: Different SOEN Architectures', fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 9. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for name, res in results.items():\n",
    "    # Compute R² score\n",
    "    y_true = y_data.squeeze().numpy()\n",
    "    y_pred = res['y_pred']\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - y_true.mean()) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Architecture': name,\n",
    "        'Hidden Dims': str(res['hidden_dims']),\n",
    "        'Trainable Params': res['n_params'],\n",
    "        'Final MSE': f\"{res['mse']:.6f}\",\n",
    "        'R² Score': f\"{r2:.4f}\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df = df.sort_values('Trainable Params')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"NETWORK COMPLEXITY STUDY: SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTask: Learn y = {TRUE_ALPHA}·x + {TRUE_BETA}\")\n",
    "print(f\"Training epochs: {N_EPOCHS}\")\n",
    "print(f\"Learning rate: {LR}\")\n",
    "print()\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 10. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_name = min(results, key=lambda x: results[x]['mse'])\n",
    "best_res = results[best_name]\n",
    "worst_name = max(results, key=lambda x: results[x]['mse'])\n",
    "worst_res = results[worst_name]\n",
    "\n",
    "print(f\"Best architecture: {best_name}\")\n",
    "print(f\"  MSE: {best_res['mse']:.6f}\")\n",
    "print(f\"  Params: {best_res['n_params']}\")\n",
    "\n",
    "print(f\"\\nWorst architecture: {worst_name}\")\n",
    "print(f\"  MSE: {worst_res['mse']:.6f}\")\n",
    "print(f\"  Params: {worst_res['n_params']}\")\n",
    "\n",
    "improvement = (worst_res['mse'] - best_res['mse']) / worst_res['mse'] * 100\n",
    "print(f\"\\nImprovement: {improvement:.1f}% reduction in MSE\")\n",
    "\n",
    "# Plot best vs worst\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x_plot = x_values.numpy()\n",
    "y_true = y_data.squeeze().numpy()\n",
    "\n",
    "for ax, (name, res), title in zip(axes, \n",
    "                                   [(worst_name, worst_res), (best_name, best_res)],\n",
    "                                   ['Worst (Simplest)', 'Best']):\n",
    "    ax.scatter(x_plot, y_true, alpha=0.5, s=30, color='gray', label='Target')\n",
    "    ax.plot(x_plot, TRUE_ALPHA * x_plot + TRUE_BETA, 'k--', lw=2)\n",
    "    ax.scatter(x_plot, res['y_pred'], alpha=0.7, s=30, color='blue', label='Prediction')\n",
    "    \n",
    "    # Show residuals\n",
    "    for i in range(0, len(x_plot), 10):\n",
    "        ax.plot([x_plot[i], x_plot[i]], [y_true[i], res['y_pred'][i]], \n",
    "                'r-', alpha=0.3, lw=1)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(f\"{title}: {name}\\nMSE={res['mse']:.6f}, Params={res['n_params']}\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 11. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze trend\n",
    "wide_models = ['1 neuron', '2 neurons', '3 neurons', '5 neurons', '10 neurons', '20 neurons']\n",
    "wide_losses = [results[n]['mse'] for n in wide_models if n in results]\n",
    "\n",
    "print(\"\\n1. EFFECT OF WIDTH (more neurons in hidden layer):\")\n",
    "for name in wide_models:\n",
    "    if name in results:\n",
    "        print(f\"   {name}: MSE = {results[name]['mse']:.6f}\")\n",
    "\n",
    "if len(wide_losses) > 1:\n",
    "    if wide_losses[-1] < wide_losses[0]:\n",
    "        print(f\"   → More neurons HELPS (MSE reduced by {(wide_losses[0]-wide_losses[-1])/wide_losses[0]*100:.1f}%)\")\n",
    "    else:\n",
    "        print(f\"   → More neurons does NOT significantly help\")\n",
    "\n",
    "print(\"\\n2. EFFECT OF DEPTH (multiple hidden layers):\")\n",
    "if '3→3 (deep)' in results and '3 neurons' in results:\n",
    "    shallow = results['3 neurons']['mse']\n",
    "    deep = results['3→3 (deep)']['mse']\n",
    "    print(f\"   3 neurons (1 layer): MSE = {shallow:.6f}\")\n",
    "    print(f\"   3→3 (2 layers):      MSE = {deep:.6f}\")\n",
    "    if deep < shallow:\n",
    "        print(f\"   → Depth HELPS\")\n",
    "    else:\n",
    "        print(f\"   → Depth does NOT help significantly\")\n",
    "\n",
    "print(\"\\n3. HARDWARE IMPLICATIONS:\")\n",
    "print(f\"   Best architecture: {best_name}\")\n",
    "print(f\"   Required neurons: {sum(best_res['hidden_dims'])} SingleDendrite(s)\")\n",
    "print(f\"   Required connections: {best_res['n_params']} trainable weights\")\n",
    "\n",
    "print(\"\\n4. KEY INSIGHT:\")\n",
    "print(\"   The SingleDendrite's nonlinear dynamics mean that even with many\")\n",
    "print(\"   neurons, perfect linear regression may not be achievable.\")\n",
    "print(\"   However, more neurons provide better APPROXIMATION of the linear function.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
