{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Classification with SingleDendrite Readout Layer\n\nIn this notebook, we use **SingleDendrite neurons for the readout layer** instead of a passive linear readout.\n\n## The Problem with Single-Neuron Readout\n\nSingleDendrite output `s` is **always non-negative** (it's a current in a superconducting loop).\nThis means:\n- `sigmoid(s) >= 0.5` always\n- The network **cannot predict class 0**!\n\n## The Solution: Two Competing Neurons with CrossEntropyLoss\n\nWe use **two SingleDendrite readout neurons** as direct class scores:\n\n```\n           ┌─► SingleDendrite₀ ─► s₀ (class 0 score)\nHidden ────┤\n           └─► SingleDendrite₁ ─► s₁ (class 1 score)\n\n           P(class=1) = softmax([s₀, s₁])[1] = exp(s₁) / (exp(s₀) + exp(s₁))\n```\n\n**Training**: CrossEntropyLoss on `[s₀, s₁]` - each neuron gets its own gradient signal.\n\n**Hardware inference**: Simple comparison `s₁ > s₀ ?` - no softmax needed!\n\n## Why CrossEntropyLoss?\n\n| Approach | Loss | Gradient to s₀ | Gradient to s₁ |\n|----------|------|----------------|----------------|\n| BCE on (s₁-s₀) | BCEWithLogitsLoss | -∂L/∂logit | +∂L/∂logit |\n| CrossEntropy | CrossEntropyLoss | Direct from CE | Direct from CE |\n\nCrossEntropyLoss gives each neuron **independent gradient signals**, potentially more stable training."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Circle-in-Ring Dataset\n",
    "\n",
    "Same nonlinear binary classification task as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circle_ring_data(n_samples=500, inner_radius=0.3, outer_radius_min=0.5, \n",
    "                               outer_radius_max=0.8, noise=0.05):\n",
    "    \"\"\"\n",
    "    Generate 2D classification data: circle inside a ring.\n",
    "    \n",
    "    Class 0: Points inside inner circle (r < inner_radius)\n",
    "    Class 1: Points in outer ring (outer_radius_min < r < outer_radius_max)\n",
    "    \"\"\"\n",
    "    n_each = n_samples // 2\n",
    "    \n",
    "    # Class 0: Inner circle\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_inner = np.random.uniform(0, inner_radius, n_each)\n",
    "    x_inner = r_inner * np.cos(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    y_inner = r_inner * np.sin(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    # Class 1: Outer ring\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_outer = np.random.uniform(outer_radius_min, outer_radius_max, n_each)\n",
    "    x_outer = r_outer * np.cos(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    y_outer = r_outer * np.sin(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([\n",
    "        np.column_stack([x_inner, y_inner]),\n",
    "        np.column_stack([x_outer, y_outer])\n",
    "    ])\n",
    "    y = np.array([0] * n_each + [1] * n_each)\n",
    "    \n",
    "    # Shuffle\n",
    "    idx = np.random.permutation(len(y))\n",
    "    X, y = X[idx], y[idx]\n",
    "    \n",
    "    # Scale to SOEN operating range [0, 0.3]\n",
    "    X = (X + 1) / 2 * 0.25 + 0.025  # Map [-1, 1] to [0.025, 0.275]\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "# Generate data\n",
    "N_SAMPLES = 500\n",
    "X_data, y_data = generate_circle_ring_data(N_SAMPLES)\n",
    "\n",
    "print(f\"Dataset shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "print(f\"Class distribution: {(y_data == 0).sum().item()} inner, {(y_data == 1).sum().item()} outer\")\n",
    "print(f\"X range: [{X_data.min():.3f}, {X_data.max():.3f}]\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 8))\n",
    "colors = ['blue', 'red']\n",
    "for c in [0, 1]:\n",
    "    mask = y_data == c\n",
    "    label = 'Inner circle (class 0)' if c == 0 else 'Outer ring (class 1)'\n",
    "    plt.scatter(X_data[mask, 0], X_data[mask, 1], c=colors[c], \n",
    "                alpha=0.6, s=30, label=label)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Binary Classification: Circle vs Ring')\n",
    "plt.legend()\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for SOEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50  # Time steps for SOEN dynamics to settle\n",
    "\n",
    "# Expand to sequence: [N, T, 2]\n",
    "X_seq = X_data.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "y_labels = y_data.unsqueeze(1)  # [N, 1]\n",
    "\n",
    "print(f\"SOEN input shape: {X_seq.shape}\")\n",
    "print(f\"Labels shape: {y_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Model Builders: Linear vs SingleDendrite Readout\n\nWe create two builders:\n1. **Linear readout**: Output layer is `Input` type (linear projection) with dim=1\n2. **SingleDendrite readout**: Output layer is `SingleDendrite` with **dim=2** (two competing neurons)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_classifier_linear_readout(hidden_dims, input_dim=2, dt=50.0):\n    \"\"\"\n    Build a SOEN classifier with LINEAR readout (Input layer).\n    \n    Architecture: 2 (input) → [hidden SingleDendrites] → 1 (linear readout)\n    \"\"\"\n    sim_cfg = SimulationConfig(\n        dt=dt,\n        input_type=\"state\",\n        track_phi=False,\n        track_power=False,\n    )\n    \n    layers = []\n    connections = []\n    \n    # Input layer (dim=2 for x, y)\n    layers.append(LayerConfig(\n        layer_id=0,\n        layer_type=\"Input\",\n        params={\"dim\": input_dim},\n    ))\n    \n    # Hidden layers (SingleDendrite)\n    prev_dim = input_dim\n    for i, hidden_dim in enumerate(hidden_dims):\n        layer_id = i + 1\n        \n        layers.append(LayerConfig(\n            layer_id=layer_id,\n            layer_type=\"SingleDendrite\",\n            params={\n                \"dim\": hidden_dim,\n                \"solver\": \"FE\",\n                \"source_func\": \"Heaviside_fit_state_dep\",\n                \"phi_offset\": 0.02,\n                \"bias_current\": 1.98,\n                \"gamma_plus\": 0.0005,\n                \"gamma_minus\": 1e-6,\n                \"learnable_params\": {\n                    \"phi_offset\": False,\n                    \"bias_current\": False,\n                    \"gamma_plus\": False,\n                    \"gamma_minus\": False,\n                },\n            },\n        ))\n        \n        connections.append(ConnectionConfig(\n            from_layer=layer_id - 1,\n            to_layer=layer_id,\n            connection_type=\"all_to_all\",\n            learnable=True,\n            params={\"init\": \"xavier_uniform\"},\n        ))\n        \n        prev_dim = hidden_dim\n    \n    # Output layer - LINEAR (Input type)\n    output_layer_id = len(hidden_dims) + 1\n    layers.append(LayerConfig(\n        layer_id=output_layer_id,\n        layer_type=\"Input\",  # Linear readout\n        params={\"dim\": 1},\n    ))\n    \n    connections.append(ConnectionConfig(\n        from_layer=output_layer_id - 1,\n        to_layer=output_layer_id,\n        connection_type=\"all_to_all\",\n        learnable=True,\n        params={\"init\": \"xavier_uniform\"},\n    ))\n    \n    model = SOENModelCore(\n        sim_config=sim_cfg,\n        layers_config=layers,\n        connections_config=connections,\n    )\n    \n    return model\n\n\ndef build_classifier_singledendrite_readout(hidden_dims, input_dim=2, dt=50.0, \n                                             readout_phi_offset=0.23):\n    \"\"\"\n    Build a SOEN classifier with TWO-NEURON SINGLEDENDRITE readout.\n    \n    Architecture: 2 (input) → [hidden SingleDendrites] → 2 (SingleDendrite readout)\n    \n    The two readout neurons compete: logit = s₁ - s₀\n    This allows the output to be positive or negative, enabling both class predictions.\n    \"\"\"\n    sim_cfg = SimulationConfig(\n        dt=dt,\n        input_type=\"state\",\n        track_phi=False,\n        track_power=False,\n    )\n    \n    layers = []\n    connections = []\n    \n    # Input layer (dim=2 for x, y)\n    layers.append(LayerConfig(\n        layer_id=0,\n        layer_type=\"Input\",\n        params={\"dim\": input_dim},\n    ))\n    \n    # Hidden layers (SingleDendrite)\n    prev_dim = input_dim\n    for i, hidden_dim in enumerate(hidden_dims):\n        layer_id = i + 1\n        \n        layers.append(LayerConfig(\n            layer_id=layer_id,\n            layer_type=\"SingleDendrite\",\n            params={\n                \"dim\": hidden_dim,\n                \"solver\": \"FE\",\n                \"source_func\": \"Heaviside_fit_state_dep\",\n                \"phi_offset\": 0.02,\n                \"bias_current\": 1.98,\n                \"gamma_plus\": 0.0005,\n                \"gamma_minus\": 1e-6,\n                \"learnable_params\": {\n                    \"phi_offset\": False,\n                    \"bias_current\": False,\n                    \"gamma_plus\": False,\n                    \"gamma_minus\": False,\n                },\n            },\n        ))\n        \n        connections.append(ConnectionConfig(\n            from_layer=layer_id - 1,\n            to_layer=layer_id,\n            connection_type=\"all_to_all\",\n            learnable=True,\n            params={\"init\": \"xavier_uniform\"},\n        ))\n        \n        prev_dim = hidden_dim\n    \n    # Output layer - TWO SingleDendrite neurons (competing)\n    output_layer_id = len(hidden_dims) + 1\n    layers.append(LayerConfig(\n        layer_id=output_layer_id,\n        layer_type=\"SingleDendrite\",\n        params={\n            \"dim\": 2,  # TWO neurons: one for each class\n            \"solver\": \"FE\",\n            \"source_func\": \"Heaviside_fit_state_dep\",\n            \"phi_offset\": readout_phi_offset,  # At threshold!\n            \"bias_current\": 1.98,\n            \"gamma_plus\": 0.0005,\n            \"gamma_minus\": 1e-6,\n            \"learnable_params\": {\n                \"phi_offset\": False,\n                \"bias_current\": False,\n                \"gamma_plus\": False,\n                \"gamma_minus\": False,\n            },\n        },\n    ))\n    \n    connections.append(ConnectionConfig(\n        from_layer=output_layer_id - 1,\n        to_layer=output_layer_id,\n        connection_type=\"all_to_all\",\n        learnable=True,\n        params={\"init\": \"xavier_uniform\"},\n    ))\n    \n    model = SOENModelCore(\n        sim_config=sim_cfg,\n        layers_config=layers,\n        connections_config=connections,\n    )\n    \n    return model\n\n\ndef count_params(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\n# Test builders\nprint(\"Testing model builders...\")\nprint(\"\\nLinear readout models:\")\nfor hidden_dims, name in [([4], \"2→4→1\"), ([8], \"2→8→1\")]:\n    model = build_classifier_linear_readout(hidden_dims)\n    n_params = count_params(model)\n    layer_dims = [l.dim for l in model.layers]\n    print(f\"  {name}: dims={layer_dims}, params={n_params}\")\n\nprint(\"\\nSingleDendrite readout models (2 competing neurons):\")\nfor hidden_dims, name in [([4], \"2→4→2\"), ([8], \"2→8→2\")]:\n    model = build_classifier_singledendrite_readout(hidden_dims)\n    n_params = count_params(model)\n    layer_dims = [l.dim for l in model.layers]\n    print(f\"  {name}: dims={layer_dims}, params={n_params}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_classifier(model, X_train, y_train, n_epochs=300, lr=0.02, verbose=False, \n                     two_neuron_readout=False):\n    \"\"\"\n    Train SOEN classifier.\n    \n    Args:\n        two_neuron_readout: If True, use CrossEntropyLoss on [s₀, s₁] class scores.\n                           If False, use BCEWithLogitsLoss on single output.\n    \"\"\"\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    \n    # Choose loss based on readout type\n    if two_neuron_readout:\n        criterion = nn.CrossEntropyLoss()\n        # Convert labels to class indices for CrossEntropyLoss\n        y_target = y_train.squeeze().long()  # [N] with values 0 or 1\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n        y_target = y_train  # [N, 1] float\n    \n    losses = []\n    accuracies = []\n    \n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        \n        # Forward\n        final_hist, _ = model(X_train)\n        output = final_hist[:, -1, :]  # [N, output_dim]\n        \n        # Compute loss\n        if two_neuron_readout:\n            # Two neurons as class scores: [s₀, s₁]\n            # CrossEntropyLoss applies softmax internally\n            loss = criterion(output, y_target)  # output: [N, 2], y_target: [N]\n        else:\n            # Single output with BCE\n            loss = criterion(output, y_target)\n        \n        # Backward\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        # Metrics\n        with torch.no_grad():\n            if two_neuron_readout:\n                # Prediction: argmax of [s₀, s₁]\n                preds = output.argmax(dim=1)  # [N]\n                acc = (preds == y_target).float().mean().item()\n            else:\n                preds = (torch.sigmoid(output) > 0.5).float()\n                acc = (preds == y_target).float().mean().item()\n        \n        losses.append(loss.item())\n        accuracies.append(acc)\n        \n        if verbose and (epoch + 1) % 50 == 0:\n            print(f\"  Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc:.4f}\")\n    \n    return losses, accuracies\n\n\ndef evaluate_classifier(model, X_test, y_test, two_neuron_readout=False):\n    \"\"\"\n    Evaluate classifier and return predictions.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        final_hist, _ = model(X_test)\n        output = final_hist[:, -1, :]  # [N, output_dim]\n        \n        if two_neuron_readout:\n            # Softmax probabilities\n            probs_all = torch.softmax(output, dim=1)  # [N, 2]\n            probs = probs_all[:, 1].numpy()  # P(class=1)\n            preds = output.argmax(dim=1).numpy()  # [N]\n        else:\n            probs = torch.sigmoid(output).squeeze().numpy()\n            preds = (probs > 0.5).astype(float)\n    \n    y_true = y_test.squeeze().numpy()\n    accuracy = (preds == y_true).mean()\n    \n    return preds, probs, accuracy"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Linear vs SingleDendrite Readout\n",
    "\n",
    "We compare the same hidden architectures with two different readout types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Architectures to compare\nHIDDEN_CONFIGS = {\n    \"4 hidden\": [4],\n    \"8 hidden\": [8],\n    \"16 hidden\": [16],\n    \"4→4 deep\": [4, 4],\n    \"8→8 deep\": [8, 8],\n}\n\nN_EPOCHS = 400\nLR = 0.02\n\nresults_linear = {}\nresults_sd = {}  # SingleDendrite readout\n\nprint(\"Training all architectures...\")\nprint(\"=\" * 70)\n\nfor name, hidden_dims in HIDDEN_CONFIGS.items():\n    print(f\"\\n{name}:\")\n    \n    # Linear readout (single neuron, no two_neuron_readout)\n    model_linear = build_classifier_linear_readout(hidden_dims)\n    n_params_linear = count_params(model_linear)\n    losses_l, accs_l = train_classifier(model_linear, X_seq, y_labels, n_epochs=N_EPOCHS, lr=LR,\n                                        two_neuron_readout=False)\n    _, _, final_acc_l = evaluate_classifier(model_linear, X_seq, y_labels, two_neuron_readout=False)\n    results_linear[name] = {\n        'hidden_dims': hidden_dims,\n        'n_params': n_params_linear,\n        'losses': losses_l,\n        'accuracies': accs_l,\n        'final_acc': final_acc_l,\n        'model': model_linear,\n    }\n    print(f\"  Linear readout:       Acc={final_acc_l:.4f}, Params={n_params_linear}\")\n    \n    # SingleDendrite readout (TWO competing neurons, use two_neuron_readout=True)\n    model_sd = build_classifier_singledendrite_readout(hidden_dims, readout_phi_offset=0.23)\n    n_params_sd = count_params(model_sd)\n    losses_sd_vals, accs_sd_vals = train_classifier(model_sd, X_seq, y_labels, n_epochs=N_EPOCHS, lr=LR,\n                                                     two_neuron_readout=True)  # KEY FIX!\n    _, _, final_acc_sd = evaluate_classifier(model_sd, X_seq, y_labels, two_neuron_readout=True)\n    results_sd[name] = {\n        'hidden_dims': hidden_dims,\n        'n_params': n_params_sd,\n        'losses': losses_sd_vals,\n        'accuracies': accs_sd_vals,\n        'final_acc': final_acc_sd,\n        'model': model_sd,\n    }\n    print(f\"  SD readout (φ=0.23): Acc={final_acc_sd:.4f}, Params={n_params_sd}\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Top row: Loss curves\n",
    "ax1, ax2 = axes[0]\n",
    "\n",
    "ax1.set_title('Linear Readout - Loss')\n",
    "for name, res in results_linear.items():\n",
    "    ax1.plot(res['losses'], label=name, lw=1.5)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('BCE Loss')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.set_title('SingleDendrite Readout (φ=0.23) - Loss')\n",
    "for name, res in results_sd.items():\n",
    "    ax2.plot(res['losses'], label=name, lw=1.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('BCE Loss')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Bottom row: Accuracy curves\n",
    "ax3, ax4 = axes[1]\n",
    "\n",
    "ax3.set_title('Linear Readout - Accuracy')\n",
    "for name, res in results_linear.items():\n",
    "    ax3.plot(res['accuracies'], label=name, lw=1.5)\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.axhline(y=1.0, color='red', linestyle='--', alpha=0.3)\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.legend(fontsize=8)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0.4, 1.05)\n",
    "\n",
    "ax4.set_title('SingleDendrite Readout (φ=0.23) - Accuracy')\n",
    "for name, res in results_sd.items():\n",
    "    ax4.plot(res['accuracies'], label=name, lw=1.5)\n",
    "ax4.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax4.axhline(y=1.0, color='red', linestyle='--', alpha=0.3)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('Accuracy')\n",
    "ax4.legend(fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Direct Comparison: Linear vs SingleDendrite Readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "names = list(HIDDEN_CONFIGS.keys())\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "linear_accs = [results_linear[n]['final_acc'] for n in names]\n",
    "sd_accs = [results_sd[n]['final_acc'] for n in names]\n",
    "\n",
    "# Bar chart comparison\n",
    "ax1 = axes[0]\n",
    "bars1 = ax1.bar(x - width/2, linear_accs, width, label='Linear Readout', color='steelblue')\n",
    "bars2 = ax1.bar(x + width/2, sd_accs, width, label='SingleDendrite Readout (φ=0.23)', color='coral')\n",
    "\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(names, rotation=30, ha='right')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Final Accuracy: Linear vs SingleDendrite Readout')\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0.4, 1.05)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars1, linear_accs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "for bar, acc in zip(bars2, sd_accs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Difference plot\n",
    "ax2 = axes[1]\n",
    "diffs = [sd - lin for sd, lin in zip(sd_accs, linear_accs)]\n",
    "colors = ['green' if d > 0 else 'red' for d in diffs]\n",
    "bars = ax2.bar(x, diffs, color=colors, alpha=0.7)\n",
    "ax2.axhline(y=0, color='black', linewidth=1)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(names, rotation=30, ha='right')\n",
    "ax2.set_ylabel('Accuracy Difference (SD - Linear)')\n",
    "ax2.set_title('SingleDendrite Readout Advantage')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, diff in zip(bars, diffs):\n",
    "    sign = '+' if diff > 0 else ''\n",
    "    y_pos = diff + 0.005 if diff > 0 else diff - 0.015\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2, y_pos, \n",
    "             f'{sign}{diff:.3f}', ha='center', va='bottom' if diff > 0 else 'top', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decision Boundary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def plot_decision_boundary(model, X_data, y_data, ax, title, resolution=100, \n                           two_neuron_readout=False):\n    \"\"\"Plot decision boundary for a 2D classifier.\"\"\"\n    x_min, x_max = X_data[:, 0].min() - 0.02, X_data[:, 0].max() + 0.02\n    y_min, y_max = X_data[:, 1].min() - 0.02, X_data[:, 1].max() + 0.02\n    \n    xx, yy = np.meshgrid(\n        np.linspace(x_min, x_max, resolution),\n        np.linspace(y_min, y_max, resolution)\n    )\n    \n    grid_points = np.c_[xx.ravel(), yy.ravel()]\n    grid_tensor = torch.FloatTensor(grid_points)\n    grid_seq = grid_tensor.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n    \n    model.eval()\n    with torch.no_grad():\n        final_hist, _ = model(grid_seq)\n        output = final_hist[:, -1, :]  # [N, output_dim]\n        \n        if two_neuron_readout:\n            # Softmax probabilities: P(class=1)\n            probs = torch.softmax(output, dim=1)[:, 1].numpy()\n        else:\n            # Sigmoid probability\n            probs = torch.sigmoid(output).squeeze().numpy()\n    \n    Z = probs.reshape(xx.shape)\n    \n    ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.7)\n    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n    \n    for c, color in enumerate(['blue', 'red']):\n        mask = y_data.squeeze() == c\n        ax.scatter(X_data[mask, 0], X_data[mask, 1], c=color, \n                   s=15, alpha=0.5, edgecolors='white', linewidths=0.3)\n    \n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_title(title, fontsize=10)\n    ax.set_aspect('equal')\n\n\nX_np = X_data.numpy()\ny_np = y_data.numpy()\n\n# Compare decision boundaries for each architecture\nn_configs = len(HIDDEN_CONFIGS)\nfig, axes = plt.subplots(n_configs, 2, figsize=(12, 4*n_configs))\n\nfor idx, name in enumerate(HIDDEN_CONFIGS.keys()):\n    ax_lin = axes[idx, 0]\n    ax_sd = axes[idx, 1]\n    \n    res_lin = results_linear[name]\n    res_sd = results_sd[name]\n    \n    plot_decision_boundary(\n        res_lin['model'], X_np, y_np, ax_lin,\n        f\"{name} - Linear Readout\\nAcc={res_lin['final_acc']:.3f}\",\n        two_neuron_readout=False\n    )\n    \n    plot_decision_boundary(\n        res_sd['model'], X_np, y_np, ax_sd,\n        f\"{name} - SD Readout (φ=0.23)\\nAcc={res_sd['final_acc']:.3f}\",\n        two_neuron_readout=True\n    )\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Effect of Readout phi_offset\n",
    "\n",
    "Let's test different `phi_offset` values for the readout layer to see how threshold positioning affects training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test different phi_offset values for readout\nPHI_OFFSETS = [0.02, 0.10, 0.15, 0.20, 0.23, 0.25, 0.30]\nHIDDEN_DIM = [8]  # Use 8 hidden neurons\n\nphi_results = {}\n\nprint(\"Testing different phi_offset values for readout layer...\")\nprint(\"Hidden architecture: 2 → 8 → 2 (two competing SingleDendrite readout neurons)\")\nprint(\"=\" * 60)\n\nfor phi in PHI_OFFSETS:\n    model = build_classifier_singledendrite_readout(HIDDEN_DIM, readout_phi_offset=phi)\n    losses, accs = train_classifier(model, X_seq, y_labels, n_epochs=N_EPOCHS, lr=LR,\n                                    two_neuron_readout=True)  # KEY: use two-neuron readout\n    _, _, final_acc = evaluate_classifier(model, X_seq, y_labels, two_neuron_readout=True)\n    \n    phi_results[phi] = {\n        'losses': losses,\n        'accuracies': accs,\n        'final_acc': final_acc,\n    }\n    print(f\"  phi_offset={phi:.2f}: Final Accuracy = {final_acc:.4f}\")\n\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(PHI_OFFSETS)))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "for (phi, res), color in zip(phi_results.items(), colors):\n",
    "    ax1.plot(res['losses'], label=f'φ={phi:.2f}', color=color, lw=1.5)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('BCE Loss')\n",
    "ax1.set_title('Training Loss by Readout phi_offset')\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2 = axes[1]\n",
    "for (phi, res), color in zip(phi_results.items(), colors):\n",
    "    ax2.plot(res['accuracies'], label=f'φ={phi:.2f}', color=color, lw=1.5)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy by Readout phi_offset')\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "\n",
    "# Final accuracy vs phi_offset\n",
    "ax3 = axes[2]\n",
    "final_accs = [phi_results[phi]['final_acc'] for phi in PHI_OFFSETS]\n",
    "ax3.plot(PHI_OFFSETS, final_accs, 'o-', markersize=10, lw=2, color='steelblue')\n",
    "ax3.axvline(x=0.23, color='red', linestyle='--', alpha=0.7, label='Threshold (0.23)')\n",
    "ax3.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax3.set_xlabel('Readout phi_offset')\n",
    "ax3.set_ylabel('Final Accuracy')\n",
    "ax3.set_title('Final Accuracy vs Readout phi_offset')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Summary comparison\n",
    "summary_data = []\n",
    "for name in HIDDEN_CONFIGS.keys():\n",
    "    res_lin = results_linear[name]\n",
    "    res_sd = results_sd[name]\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Architecture': name,\n",
    "        'Hidden Neurons': sum(res_lin['hidden_dims']),\n",
    "        'Params (Linear)': res_lin['n_params'],\n",
    "        'Params (SD)': res_sd['n_params'],\n",
    "        'Acc (Linear)': f\"{res_lin['final_acc']:.4f}\",\n",
    "        'Acc (SD φ=0.23)': f\"{res_sd['final_acc']:.4f}\",\n",
    "        'Difference': f\"{res_sd['final_acc'] - res_lin['final_acc']:+.4f}\",\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"COMPARISON: LINEAR vs SINGLEDENDRITE READOUT\")\n",
    "print(\"=\" * 100)\n",
    "print(f\"\\nTask: Binary classification (Circle vs Ring)\")\n",
    "print(f\"SingleDendrite readout: phi_offset = 0.23 (at threshold)\")\n",
    "print(f\"Training epochs: {N_EPOCHS}\")\n",
    "print()\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze results\n",
    "linear_best = max(results_linear.values(), key=lambda x: x['final_acc'])\n",
    "sd_best = max(results_sd.values(), key=lambda x: x['final_acc'])\n",
    "\n",
    "print(\"\\n1. READOUT COMPARISON:\")\n",
    "print(f\"   Best Linear Readout:       {max(r['final_acc'] for r in results_linear.values()):.4f}\")\n",
    "print(f\"   Best SD Readout (φ=0.23):  {max(r['final_acc'] for r in results_sd.values()):.4f}\")\n",
    "\n",
    "# Count wins\n",
    "sd_wins = sum(1 for n in HIDDEN_CONFIGS.keys() \n",
    "              if results_sd[n]['final_acc'] > results_linear[n]['final_acc'])\n",
    "linear_wins = len(HIDDEN_CONFIGS) - sd_wins\n",
    "\n",
    "print(f\"\\n2. WIN COUNT:\")\n",
    "print(f\"   SingleDendrite readout wins: {sd_wins}/{len(HIDDEN_CONFIGS)}\")\n",
    "print(f\"   Linear readout wins: {linear_wins}/{len(HIDDEN_CONFIGS)}\")\n",
    "\n",
    "print(\"\\n3. EFFECT OF READOUT phi_offset:\")\n",
    "best_phi = max(phi_results.keys(), key=lambda x: phi_results[x]['final_acc'])\n",
    "worst_phi = min(phi_results.keys(), key=lambda x: phi_results[x]['final_acc'])\n",
    "print(f\"   Best phi_offset:  {best_phi:.2f} (Acc={phi_results[best_phi]['final_acc']:.4f})\")\n",
    "print(f\"   Worst phi_offset: {worst_phi:.2f} (Acc={phi_results[worst_phi]['final_acc']:.4f})\")\n",
    "print(f\"   phi=0.23 (threshold): Acc={phi_results[0.23]['final_acc']:.4f}\")\n",
    "\n",
    "print(\"\\n4. HARDWARE IMPLICATIONS:\")\n",
    "print(\"   • SingleDendrite readout is more hardware-faithful\")\n",
    "print(\"   • phi_offset=0.23 places neuron at threshold for easier training\")\n",
    "print(\"   • Linear readout requires idealized hardware (perfect linear response)\")\n",
    "\n",
    "print(\"\\n5. KEY INSIGHT:\")\n",
    "if sd_wins >= linear_wins:\n",
    "    print(\"   ✓ SingleDendrite readout performs comparably or better than linear\")\n",
    "    print(\"   ✓ Hardware-faithful architectures are viable!\")\n",
    "else:\n",
    "    print(\"   Linear readout has an advantage for this task\")\n",
    "    print(\"   But SingleDendrite is still viable with proper phi_offset tuning\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Output Dynamics\n",
    "\n",
    "Let's visualize how the SingleDendrite readout neuron's state evolves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Select best SingleDendrite readout model\nbest_sd_name = max(results_sd.keys(), key=lambda x: results_sd[x]['final_acc'])\nbest_model = results_sd[best_sd_name]['model']\n\n# Get a few samples from each class\nclass0_idx = torch.where(y_data == 0)[0][:3]\nclass1_idx = torch.where(y_data == 1)[0][:3]\n\nsample_idx = torch.cat([class0_idx, class1_idx])\nX_samples = X_seq[sample_idx]\ny_samples = y_data[sample_idx]\n\n# Forward pass to get full history\nbest_model.eval()\nwith torch.no_grad():\n    output_hist, layer_states = best_model(X_samples)\n\n# Plot output dynamics - now we have TWO neurons\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\ntime_steps = np.arange(SEQ_LEN)\n\n# Left: Neuron 0 (s₀) dynamics\nax1 = axes[0]\nfor i, (idx, y_val) in enumerate(zip(sample_idx, y_samples)):\n    s0 = output_hist[i, :, 0].numpy()  # First neuron\n    color = 'blue' if y_val == 0 else 'red'\n    label = f'Class {int(y_val)}' if i < 2 else None\n    ax1.plot(time_steps, s0, color=color, alpha=0.7, lw=1.5, label=label)\nax1.set_xlabel('Time Step')\nax1.set_ylabel('State s₀ (class 0 score)')\nax1.set_title('Neuron 0 (class 0 detector)')\nax1.legend()\nax1.grid(True, alpha=0.3)\n\n# Middle: Neuron 1 (s₁) dynamics\nax2 = axes[1]\nfor i, (idx, y_val) in enumerate(zip(sample_idx, y_samples)):\n    s1 = output_hist[i, :, 1].numpy()  # Second neuron\n    color = 'blue' if y_val == 0 else 'red'\n    ax2.plot(time_steps, s1, color=color, alpha=0.7, lw=1.5)\nax2.set_xlabel('Time Step')\nax2.set_ylabel('State s₁ (class 1 score)')\nax2.set_title('Neuron 1 (class 1 detector)')\nax2.grid(True, alpha=0.3)\n\n# Right: Softmax probability P(class=1) over time\nax3 = axes[2]\nfor i, (idx, y_val) in enumerate(zip(sample_idx, y_samples)):\n    s0 = output_hist[i, :, 0]\n    s1 = output_hist[i, :, 1]\n    # Compute softmax P(class=1) = exp(s1) / (exp(s0) + exp(s1))\n    probs = torch.softmax(torch.stack([s0, s1], dim=1), dim=1)[:, 1].numpy()\n    color = 'blue' if y_val == 0 else 'red'\n    ax3.plot(time_steps, probs, color=color, alpha=0.7, lw=1.5)\nax3.axhline(y=0.5, color='black', linestyle='--', alpha=0.5, label='Decision boundary')\nax3.set_xlabel('Time Step')\nax3.set_ylabel('P(class=1)')\nax3.set_title('Softmax Probability Over Time')\nax3.legend()\nax3.grid(True, alpha=0.3)\nax3.set_ylim(-0.05, 1.05)\n\nplt.suptitle(f'Two-Neuron Readout Dynamics ({best_sd_name}) - CrossEntropyLoss\\nBlue=Class 0 (inner), Red=Class 1 (outer)', \n             fontsize=12, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nFinal output states (two competing neurons with softmax):\")\nprint(f\"{'Sample':<8} {'True':<6} {'s₀':<10} {'s₁':<10} {'P(cls0)':<10} {'P(cls1)':<10} {'Pred':<6} {'Status'}\")\nprint(\"-\" * 76)\nfor i, (idx, y_val) in enumerate(zip(sample_idx, y_samples)):\n    s0 = output_hist[i, -1, 0].item()\n    s1 = output_hist[i, -1, 1].item()\n    probs = torch.softmax(torch.tensor([s0, s1]), dim=0)\n    p0, p1 = probs[0].item(), probs[1].item()\n    pred = 1 if p1 > p0 else 0\n    status = '✓' if pred == y_val else '✗'\n    print(f\"{i:<8} {int(y_val):<6} {s0:<10.4f} {s1:<10.4f} {p0:<10.4f} {p1:<10.4f} {pred:<6} {status}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}