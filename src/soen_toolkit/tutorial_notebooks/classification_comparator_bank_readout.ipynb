{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Classification with Comparator Bank Readout\n\n## The Problem\n\nA single thresholded SingleDendrite neuron is brittle:\n- If threshold is too high: nothing fires\n- If threshold is too low: everything fires\n- Small calibration errors → large performance changes\n\n## The Solution: Comparator Bank (Population Coding)\n\nUse **K neurons per class** with **staggered phi_offsets** (effective thresholds):\n\n```\n           ┌─► SD(φ=0.15) ─► y₀ ─┐\n           ├─► SD(φ=0.18) ─► y₁ ─┤\nHidden ────┼─► SD(φ=0.21) ─► y₂ ─┼──► S = Σyₖ (count)\n           ├─► SD(φ=0.24) ─► y₃ ─┤\n           └─► SD(φ=0.27) ─► y₄ ─┘\n```\n\n**Key Design: SHARED WEIGHTS**\n\nAll comparators receive the **same evidence** (shared weights from hidden layer) but have **different thresholds** (phi_offsets). This creates a true staircase approximation:\n\n```\n                 Independent Weights (BAD)          Shared Weights (GOOD)\n                 \nHidden ──┬─► W₁ ──► Comparator(φ=0.15)    Hidden ──► W ──┬─► Comparator(φ=0.15)\n         ├─► W₂ ──► Comparator(φ=0.20)              (shared) ├─► Comparator(φ=0.20)  \n         └─► W₃ ──► Comparator(φ=0.25)                      └─► Comparator(φ=0.25)\n         \nEach learns different features!              All see same evidence, different thresholds!\n```\n\n## Hardware Mapping\n\n| Component | Hardware Implementation |\n|-----------|------------------------|\n| Each comparator | SingleDendrite neuron |\n| Shared weights | Single fan-out from hidden layer |\n| Staggered thresholds | Different phi_offset per neuron |\n| Spike counting | SFQ counter or optical pulse counter |\n| Class decision | Compare counts: argmax(S_class) |"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Circle-in-Ring Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_circle_ring_data(n_samples=500, inner_radius=0.3, outer_radius_min=0.5, \n",
    "                               outer_radius_max=0.8, noise=0.05):\n",
    "    \"\"\"\n",
    "    Generate 2D classification data: circle inside a ring.\n",
    "    \"\"\"\n",
    "    n_each = n_samples // 2\n",
    "    \n",
    "    # Class 0: Inner circle\n",
    "    theta_inner = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_inner = np.random.uniform(0, inner_radius, n_each)\n",
    "    x_inner = r_inner * np.cos(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    y_inner = r_inner * np.sin(theta_inner) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    # Class 1: Outer ring\n",
    "    theta_outer = np.random.uniform(0, 2*np.pi, n_each)\n",
    "    r_outer = np.random.uniform(outer_radius_min, outer_radius_max, n_each)\n",
    "    x_outer = r_outer * np.cos(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    y_outer = r_outer * np.sin(theta_outer) + np.random.normal(0, noise, n_each)\n",
    "    \n",
    "    X = np.vstack([\n",
    "        np.column_stack([x_inner, y_inner]),\n",
    "        np.column_stack([x_outer, y_outer])\n",
    "    ])\n",
    "    y = np.array([0] * n_each + [1] * n_each)\n",
    "    \n",
    "    idx = np.random.permutation(len(y))\n",
    "    X, y = X[idx], y[idx]\n",
    "    \n",
    "    # Scale to SOEN operating range\n",
    "    X = (X + 1) / 2 * 0.25 + 0.025\n",
    "    \n",
    "    return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "\n",
    "\n",
    "N_SAMPLES = 500\n",
    "X_data, y_data = generate_circle_ring_data(N_SAMPLES)\n",
    "\n",
    "print(f\"Dataset shape: X={X_data.shape}, y={y_data.shape}\")\n",
    "print(f\"Class distribution: {(y_data == 0).sum().item()} inner, {(y_data == 1).sum().item()} outer\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "for c, color in enumerate(['blue', 'red']):\n",
    "    mask = y_data == c\n",
    "    plt.scatter(X_data[mask, 0], X_data[mask, 1], c=color, alpha=0.6, s=20)\n",
    "plt.title('Circle vs Ring Dataset')\n",
    "plt.axis('equal')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for SOEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "\n",
    "X_seq = X_data.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "y_labels = y_data.long()  # Class indices for CrossEntropyLoss\n",
    "\n",
    "print(f\"SOEN input shape: {X_seq.shape}\")\n",
    "print(f\"Labels shape: {y_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Comparator Bank Model Builder\n\nArchitecture:\n- Input: 2D (x, y)\n- Hidden: SingleDendrite layer(s)\n- Readout: **K neurons per class** with staggered phi_offsets\n- **CRITICAL**: All comparators share the same weights (weight tying)\n- Output: Sum of neuron outputs per class → class scores"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def build_comparator_bank_classifier(hidden_dims, n_comparators_per_class=5,\n                                      phi_offset_range=(0.15, 0.30),\n                                      input_dim=2, dt=50.0, n_classes=2):\n    \"\"\"\n    Build a SOEN classifier with COMPARATOR BANK readout.\n    \n    Architecture: input → [hidden] → K*n_classes comparator neurons\n    \n    Each class has K neurons with staggered phi_offsets.\n    All K comparators SHARE WEIGHTS from the hidden layer (weight tying).\n    Class score = sum of neuron outputs for that class.\n    \n    Args:\n        hidden_dims: List of hidden layer dimensions\n        n_comparators_per_class: K neurons per class\n        phi_offset_range: (min, max) for staggered phi_offsets\n        n_classes: Number of output classes\n    \"\"\"\n    K = n_comparators_per_class\n    \n    # Generate staggered phi_offsets for each comparator\n    phi_min, phi_max = phi_offset_range\n    phi_offsets = np.linspace(phi_min, phi_max, K)\n    \n    print(f\"Comparator bank: {K} neurons per class (SHARED WEIGHTS)\")\n    print(f\"Staggered phi_offsets: {phi_offsets}\")\n    \n    sim_cfg = SimulationConfig(\n        dt=dt,\n        input_type=\"state\",\n        track_phi=False,\n        track_power=False,\n    )\n    \n    layers = []\n    connections = []\n    \n    # Input layer\n    layers.append(LayerConfig(\n        layer_id=0,\n        layer_type=\"Input\",\n        params={\"dim\": input_dim},\n    ))\n    \n    # Hidden layers\n    for i, hidden_dim in enumerate(hidden_dims):\n        layer_id = i + 1\n        \n        layers.append(LayerConfig(\n            layer_id=layer_id,\n            layer_type=\"SingleDendrite\",\n            params={\n                \"dim\": hidden_dim,\n                \"solver\": \"FE\",\n                \"source_func\": \"Heaviside_fit_state_dep\",\n                \"phi_offset\": 0.02,\n                \"bias_current\": 1.98,\n                \"gamma_plus\": 0.0005,\n                \"gamma_minus\": 1e-6,\n                \"learnable_params\": {\n                    \"phi_offset\": False,\n                    \"bias_current\": False,\n                    \"gamma_plus\": False,\n                    \"gamma_minus\": False,\n                },\n            },\n        ))\n        \n        connections.append(ConnectionConfig(\n            from_layer=layer_id - 1,\n            to_layer=layer_id,\n            connection_type=\"all_to_all\",\n            learnable=True,\n            params={\"init\": \"xavier_uniform\"},\n        ))\n    \n    # Output layers: K Comparator layers with different phi_offsets\n    # We'll track the connection indices for weight tying\n    output_layer_ids = []\n    comparator_connection_indices = []  # Track which connections need weight tying\n    last_hidden_id = len(hidden_dims)\n    \n    for k, phi in enumerate(phi_offsets):\n        layer_id = len(hidden_dims) + 1 + k\n        output_layer_ids.append(layer_id)\n        \n        layers.append(LayerConfig(\n            layer_id=layer_id,\n            layer_type=\"SingleDendrite\",\n            params={\n                \"dim\": n_classes,  # One neuron per class at this threshold\n                \"solver\": \"FE\",\n                \"source_func\": \"Heaviside_fit_state_dep\",\n                \"phi_offset\": float(phi),  # Staggered threshold!\n                \"bias_current\": 1.98,\n                \"gamma_plus\": 0.0005,\n                \"gamma_minus\": 1e-6,\n                \"learnable_params\": {\n                    \"phi_offset\": False,\n                    \"bias_current\": False,\n                    \"gamma_plus\": False,\n                    \"gamma_minus\": False,\n                },\n            },\n        ))\n        \n        # Connect from last hidden layer to this comparator layer\n        conn_idx = len(connections)\n        comparator_connection_indices.append(conn_idx)\n        \n        connections.append(ConnectionConfig(\n            from_layer=last_hidden_id,\n            to_layer=layer_id,\n            connection_type=\"all_to_all\",\n            learnable=True,\n            params={\"init\": \"xavier_uniform\"},\n        ))\n    \n    model = SOENModelCore(\n        sim_config=sim_cfg,\n        layers_config=layers,\n        connections_config=connections,\n    )\n    \n    # Store metadata for later use\n    model.comparator_config = {\n        'n_comparators_per_class': K,\n        'n_classes': n_classes,\n        'phi_offsets': phi_offsets,\n        'output_layer_ids': output_layer_ids,\n        'comparator_connection_indices': comparator_connection_indices,\n    }\n    \n    return model\n\n\ndef tie_comparator_weights(model):\n    \"\"\"\n    Enforce weight sharing across all comparator layers.\n    \n    Copies weights from the first comparator's connection to all others.\n    Call this after each optimizer step during training.\n    \n    Note: model.connections is a ParameterDict with string keys.\n    \"\"\"\n    config = model.comparator_config\n    conn_indices = config['comparator_connection_indices']\n    \n    if len(conn_indices) <= 1:\n        return  # No tying needed for K=1\n    \n    # Get the first comparator's weights as the \"master\"\n    # ParameterDict requires string keys\n    first_conn_key = str(conn_indices[0])\n    master_weights = model.connections[first_conn_key].weight.data\n    \n    # Copy to all other comparators\n    for conn_idx in conn_indices[1:]:\n        conn_key = str(conn_idx)\n        model.connections[conn_key].weight.data.copy_(master_weights)\n\n\ndef count_params(model):\n    \"\"\"Count learnable parameters.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef count_effective_params(model):\n    \"\"\"\n    Count effective parameters (accounting for weight sharing).\n    \n    With weight tying, all K comparator connections share the same weights,\n    so we only count them once.\n    \"\"\"\n    total = 0\n    config = model.comparator_config\n    conn_indices = config['comparator_connection_indices']\n    \n    for i, p in enumerate(model.parameters()):\n        if not p.requires_grad:\n            continue\n        # Check if this is a comparator connection (beyond the first)\n        # If so, don't count it (shared weights)\n        # This is a simplification - proper implementation would check parameter names\n        total += p.numel()\n    \n    # Subtract the duplicated comparator weights\n    if len(conn_indices) > 1:\n        # ParameterDict requires string keys\n        first_conn_key = str(conn_indices[0])\n        first_conn = model.connections[first_conn_key]\n        weight_size = first_conn.weight.numel()\n        # We have K copies but only need 1, so subtract (K-1) * weight_size\n        duplicates = (len(conn_indices) - 1) * weight_size\n        total -= duplicates\n    \n    return total\n\n\n# Test builder\nprint(\"Testing comparator bank model builder...\")\ntest_model = build_comparator_bank_classifier([8], n_comparators_per_class=5)\nprint(f\"Total parameters (before tying): {count_params(test_model)}\")\nprint(f\"Effective parameters (with tying): {count_effective_params(test_model)}\")\nprint(f\"Number of layers: {len(test_model.layers)}\")\nprint(f\"Layer dimensions: {[l.dim for l in test_model.layers]}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparator Bank Forward Pass\n",
    "\n",
    "We need a custom forward function that:\n",
    "1. Runs the model to get all layer outputs\n",
    "2. Sums outputs from all comparator layers per class\n",
    "3. Returns class scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparator_bank_forward(model, X):\n",
    "    \"\"\"\n",
    "    Forward pass that sums comparator outputs per class.\n",
    "    \n",
    "    Returns:\n",
    "        class_scores: [N, n_classes] - sum of comparator outputs per class\n",
    "        all_outputs: List of [N, T, n_classes] for each comparator layer\n",
    "    \"\"\"\n",
    "    config = model.comparator_config\n",
    "    K = config['n_comparators_per_class']\n",
    "    n_classes = config['n_classes']\n",
    "    output_layer_ids = config['output_layer_ids']\n",
    "    \n",
    "    # Forward pass - get all layer states\n",
    "    _, layer_states = model(X)\n",
    "    \n",
    "    # Collect outputs from all comparator layers\n",
    "    # layer_states is a dict: {layer_id: [N, T, dim]}\n",
    "    comparator_outputs = []\n",
    "    for layer_id in output_layer_ids:\n",
    "        # Get final timestep output: [N, n_classes]\n",
    "        output = layer_states[layer_id][:, -1, :]\n",
    "        comparator_outputs.append(output)\n",
    "    \n",
    "    # Stack and sum across comparators: [K, N, n_classes] -> [N, n_classes]\n",
    "    stacked = torch.stack(comparator_outputs, dim=0)  # [K, N, n_classes]\n",
    "    class_scores = stacked.sum(dim=0)  # [N, n_classes]\n",
    "    \n",
    "    return class_scores, comparator_outputs\n",
    "\n",
    "\n",
    "# Test forward pass\n",
    "test_model.eval()\n",
    "with torch.no_grad():\n",
    "    scores, outputs = comparator_bank_forward(test_model, X_seq[:5])\n",
    "    print(f\"Class scores shape: {scores.shape}\")\n",
    "    print(f\"Number of comparator layers: {len(outputs)}\")\n",
    "    print(f\"Sample class scores:\\n{scores}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training with Comparator Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def train_comparator_bank(model, X_train, y_train, n_epochs=300, lr=0.02, verbose=False,\n                          use_weight_tying=True):\n    \"\"\"\n    Train comparator bank classifier.\n    \n    Uses CrossEntropyLoss on summed class scores.\n    With weight tying, all comparators share the same weights.\n    \n    Args:\n        use_weight_tying: If True, enforce shared weights across comparators after each step.\n    \"\"\"\n    model.train()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    # Initialize with tied weights\n    if use_weight_tying:\n        tie_comparator_weights(model)\n    \n    losses = []\n    accuracies = []\n    \n    for epoch in range(n_epochs):\n        optimizer.zero_grad()\n        \n        # Forward with comparator bank summing\n        class_scores, _ = comparator_bank_forward(model, X_train)\n        \n        # Loss on summed scores\n        loss = criterion(class_scores, y_train)\n        \n        # Backward\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        # CRITICAL: Tie weights after optimizer step\n        if use_weight_tying:\n            tie_comparator_weights(model)\n        \n        # Metrics\n        with torch.no_grad():\n            preds = class_scores.argmax(dim=1)\n            acc = (preds == y_train).float().mean().item()\n        \n        losses.append(loss.item())\n        accuracies.append(acc)\n        \n        if verbose and (epoch + 1) % 50 == 0:\n            print(f\"  Epoch {epoch+1}: Loss={loss.item():.4f}, Acc={acc:.4f}\")\n    \n    return losses, accuracies\n\n\ndef evaluate_comparator_bank(model, X_test, y_test):\n    \"\"\"\n    Evaluate comparator bank classifier.\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        class_scores, comparator_outputs = comparator_bank_forward(model, X_test)\n        preds = class_scores.argmax(dim=1).numpy()\n        probs = torch.softmax(class_scores, dim=1)[:, 1].numpy()\n    \n    y_true = y_test.numpy()\n    accuracy = (preds == y_true).mean()\n    \n    return preds, probs, accuracy, class_scores.numpy(), comparator_outputs"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Experiment: Vary Number of Comparators (WITH WEIGHT TYING)\n\nTest K = 1, 3, 5, 7, 10 comparators per class.\n\n**Key change**: All comparators now share the same weights!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "K_VALUES = [1, 3, 5, 7, 10]\nHIDDEN_DIM = [8]\nN_EPOCHS = 400\nLR = 0.02\nPHI_RANGE = (0.10, 0.35)  # Range of phi_offsets\n\nresults_by_k = {}\n\nprint(\"Training comparator bank models with SHARED WEIGHTS...\")\nprint(f\"phi_offset range: {PHI_RANGE}\")\nprint(\"=\" * 60)\n\nfor K in K_VALUES:\n    print(f\"\\nK = {K} comparators per class:\")\n    \n    # Reset seed for fair comparison\n    torch.manual_seed(42)\n    \n    model = build_comparator_bank_classifier(\n        HIDDEN_DIM, \n        n_comparators_per_class=K,\n        phi_offset_range=PHI_RANGE\n    )\n    n_params = count_params(model)\n    n_effective = count_effective_params(model)\n    \n    # Train WITH weight tying\n    losses, accs = train_comparator_bank(model, X_seq, y_labels, n_epochs=N_EPOCHS, lr=LR,\n                                         use_weight_tying=True)\n    _, _, final_acc, _, _ = evaluate_comparator_bank(model, X_seq, y_labels)\n    \n    results_by_k[K] = {\n        'model': model,\n        'losses': losses,\n        'accuracies': accs,\n        'final_acc': final_acc,\n        'n_params': n_params,\n        'n_effective_params': n_effective,\n        'phi_offsets': model.comparator_config['phi_offsets'],\n    }\n    \n    print(f\"  Final Accuracy: {final_acc:.4f}\")\n    print(f\"  Params (total/effective): {n_params}/{n_effective}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"Training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(K_VALUES)))\n",
    "\n",
    "# Loss curves\n",
    "ax1 = axes[0]\n",
    "for (K, res), color in zip(results_by_k.items(), colors):\n",
    "    ax1.plot(res['losses'], label=f'K={K}', color=color, lw=1.5)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('CrossEntropy Loss')\n",
    "ax1.set_title('Training Loss by Number of Comparators')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "ax2 = axes[1]\n",
    "for (K, res), color in zip(results_by_k.items(), colors):\n",
    "    ax2.plot(res['accuracies'], label=f'K={K}', color=color, lw=1.5)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy by Number of Comparators')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Accuracy vs Number of Comparators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy vs K\n",
    "ax1 = axes[0]\n",
    "K_list = list(results_by_k.keys())\n",
    "accs = [results_by_k[k]['final_acc'] for k in K_list]\n",
    "params = [results_by_k[k]['n_params'] for k in K_list]\n",
    "\n",
    "ax1.plot(K_list, accs, 'o-', markersize=10, lw=2, color='steelblue')\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')\n",
    "ax1.set_xlabel('K (Comparators per Class)')\n",
    "ax1.set_ylabel('Final Accuracy')\n",
    "ax1.set_title('Accuracy vs Number of Comparators')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_ylim(0.4, 1.05)\n",
    "\n",
    "for k, acc in zip(K_list, accs):\n",
    "    ax1.annotate(f'{acc:.3f}', (k, acc), textcoords=\"offset points\", \n",
    "                 xytext=(0, 10), ha='center')\n",
    "\n",
    "# Parameters vs K\n",
    "ax2 = axes[1]\n",
    "ax2.bar(K_list, params, color='coral', alpha=0.7)\n",
    "ax2.set_xlabel('K (Comparators per Class)')\n",
    "ax2.set_ylabel('Number of Parameters')\n",
    "ax2.set_title('Model Size vs Number of Comparators')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for k, p in zip(K_list, params):\n",
    "    ax2.annotate(f'{p}', (k, p), textcoords=\"offset points\", \n",
    "                 xytext=(0, 5), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Comparator Responses\n",
    "\n",
    "For the best model, visualize how different comparators (different phi_offsets) respond."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best K\n",
    "best_K = max(results_by_k.keys(), key=lambda k: results_by_k[k]['final_acc'])\n",
    "best_model = results_by_k[best_K]['model']\n",
    "phi_offsets = results_by_k[best_K]['phi_offsets']\n",
    "\n",
    "print(f\"Best K = {best_K}, Accuracy = {results_by_k[best_K]['final_acc']:.4f}\")\n",
    "print(f\"Phi offsets: {phi_offsets}\")\n",
    "\n",
    "# Get comparator outputs for all samples\n",
    "best_model.eval()\n",
    "with torch.no_grad():\n",
    "    class_scores, comparator_outputs = comparator_bank_forward(best_model, X_seq)\n",
    "\n",
    "# Visualize: For each comparator, show which samples activate each class neuron\n",
    "fig, axes = plt.subplots(2, best_K, figsize=(3*best_K, 6))\n",
    "\n",
    "for k in range(best_K):\n",
    "    output = comparator_outputs[k].numpy()  # [N, 2]\n",
    "    \n",
    "    for c in range(2):\n",
    "        ax = axes[c, k]\n",
    "        \n",
    "        # Color by true class\n",
    "        class0_mask = y_labels.numpy() == 0\n",
    "        class1_mask = y_labels.numpy() == 1\n",
    "        \n",
    "        ax.scatter(X_data[class0_mask, 0], X_data[class0_mask, 1], \n",
    "                   c=output[class0_mask, c], cmap='Blues', s=10, alpha=0.7,\n",
    "                   vmin=0, vmax=output[:, c].max())\n",
    "        ax.scatter(X_data[class1_mask, 0], X_data[class1_mask, 1], \n",
    "                   c=output[class1_mask, c], cmap='Reds', s=10, alpha=0.7,\n",
    "                   vmin=0, vmax=output[:, c].max())\n",
    "        \n",
    "        class_name = \"Class 0\" if c == 0 else \"Class 1\"\n",
    "        ax.set_title(f'φ={phi_offsets[k]:.2f}\\n{class_name} neuron', fontsize=9)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "axes[0, 0].set_ylabel('Class 0 Neuron Response')\n",
    "axes[1, 0].set_ylabel('Class 1 Neuron Response')\n",
    "\n",
    "plt.suptitle(f'Comparator Bank Responses (K={best_K})\\nBrighter = Higher Activation', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparator_decision_boundary(model, X_data, y_data, ax, title, resolution=100):\n",
    "    \"\"\"Plot decision boundary for comparator bank classifier.\"\"\"\n",
    "    x_min, x_max = X_data[:, 0].min() - 0.02, X_data[:, 0].max() + 0.02\n",
    "    y_min, y_max = X_data[:, 1].min() - 0.02, X_data[:, 1].max() + 0.02\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, resolution),\n",
    "        np.linspace(y_min, y_max, resolution)\n",
    "    )\n",
    "    \n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    grid_tensor = torch.FloatTensor(grid_points)\n",
    "    grid_seq = grid_tensor.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        class_scores, _ = comparator_bank_forward(model, grid_seq)\n",
    "        probs = torch.softmax(class_scores, dim=1)[:, 1].numpy()\n",
    "    \n",
    "    Z = probs.reshape(xx.shape)\n",
    "    \n",
    "    ax.contourf(xx, yy, Z, levels=50, cmap='RdBu', alpha=0.7)\n",
    "    ax.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
    "    \n",
    "    for c, color in enumerate(['blue', 'red']):\n",
    "        mask = y_data == c\n",
    "        ax.scatter(X_data[mask, 0], X_data[mask, 1], c=color, \n",
    "                   s=15, alpha=0.6, edgecolors='white', linewidths=0.3)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "# Plot decision boundaries for different K values\n",
    "fig, axes = plt.subplots(1, len(K_VALUES), figsize=(4*len(K_VALUES), 4))\n",
    "\n",
    "X_np = X_data.numpy()\n",
    "y_np = y_labels.numpy()\n",
    "\n",
    "for idx, K in enumerate(K_VALUES):\n",
    "    model = results_by_k[K]['model']\n",
    "    acc = results_by_k[K]['final_acc']\n",
    "    \n",
    "    plot_comparator_decision_boundary(\n",
    "        model, X_np, y_np, axes[idx],\n",
    "        f'K={K}\\nAcc={acc:.3f}'\n",
    "    )\n",
    "\n",
    "plt.suptitle('Decision Boundaries: Comparator Bank Readout', fontsize=12, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare with Two-Neuron Approach\n",
    "\n",
    "Build a simple two-neuron model (K=1 with single phi_offset) for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_two_neuron_classifier(hidden_dims, phi_offset=0.23, input_dim=2, dt=50.0):\n",
    "    \"\"\"Build simple two-neuron SingleDendrite readout for comparison.\"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=dt,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = []\n",
    "    connections = []\n",
    "    \n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": input_dim},\n",
    "    ))\n",
    "    \n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "            },\n",
    "        ))\n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "    \n",
    "    output_layer_id = len(hidden_dims) + 1\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=output_layer_id,\n",
    "        layer_type=\"SingleDendrite\",\n",
    "        params={\n",
    "            \"dim\": 2,\n",
    "            \"solver\": \"FE\",\n",
    "            \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "            \"phi_offset\": phi_offset,\n",
    "            \"bias_current\": 1.98,\n",
    "            \"gamma_plus\": 0.0005,\n",
    "            \"gamma_minus\": 1e-6,\n",
    "        },\n",
    "    ))\n",
    "    connections.append(ConnectionConfig(\n",
    "        from_layer=output_layer_id - 1,\n",
    "        to_layer=output_layer_id,\n",
    "        connection_type=\"all_to_all\",\n",
    "        learnable=True,\n",
    "        params={\"init\": \"xavier_uniform\"},\n",
    "    ))\n",
    "    \n",
    "    return SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_two_neuron(model, X_train, y_train, n_epochs=300, lr=0.02):\n",
    "    \"\"\"Train two-neuron classifier with BCE on difference.\"\"\"\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    y_target = y_train.float().unsqueeze(1)\n",
    "    losses, accuracies = [], []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        final_hist, _ = model(X_train)\n",
    "        output = final_hist[:, -1, :]\n",
    "        logits = (output[:, 1] - output[:, 0]).unsqueeze(1)\n",
    "        \n",
    "        loss = criterion(logits, y_target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            acc = (preds == y_target).float().mean().item()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    return losses, accuracies\n",
    "\n",
    "\n",
    "# Train two-neuron baseline\n",
    "print(\"Training two-neuron baseline (BCE on s1-s0)...\")\n",
    "torch.manual_seed(42)\n",
    "two_neuron_model = build_two_neuron_classifier(HIDDEN_DIM, phi_offset=0.23)\n",
    "two_neuron_losses, two_neuron_accs = train_two_neuron(two_neuron_model, X_seq, y_labels, n_epochs=N_EPOCHS, lr=LR)\n",
    "two_neuron_final_acc = two_neuron_accs[-1]\n",
    "print(f\"Two-neuron final accuracy: {two_neuron_final_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# Build comparison table\ncomparison_data = []\n\n# Two-neuron baseline\ncomparison_data.append({\n    'Method': 'Two-Neuron (BCE)',\n    'K': 1,\n    'Params (Total)': count_params(two_neuron_model),\n    'Params (Effective)': count_params(two_neuron_model),\n    'Final Acc': f'{two_neuron_final_acc:.4f}',\n})\n\n# Comparator bank results\nfor K, res in results_by_k.items():\n    comparison_data.append({\n        'Method': f'Comparator Bank (Shared W)',\n        'K': K,\n        'Params (Total)': res['n_params'],\n        'Params (Effective)': res['n_effective_params'],\n        'Final Acc': f\"{res['final_acc']:.4f}\",\n    })\n\ndf = pd.DataFrame(comparison_data)\n\nprint(\"=\" * 80)\nprint(\"COMPARISON: TWO-NEURON vs COMPARATOR BANK (SHARED WEIGHTS)\")\nprint(\"=\" * 80)\nprint(f\"\\nHidden architecture: {HIDDEN_DIM}\")\nprint(f\"Comparator phi_offset range: {PHI_RANGE}\")\nprint(f\"Training epochs: {N_EPOCHS}\")\nprint(f\"Weight tying: ENABLED (all comparators share same weights)\")\nprint()\nprint(df.to_string(index=False))\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Bar chart\n",
    "ax1 = axes[0]\n",
    "methods = ['2-Neuron'] + [f'CB K={k}' for k in K_VALUES]\n",
    "accs = [two_neuron_final_acc] + [results_by_k[k]['final_acc'] for k in K_VALUES]\n",
    "colors = ['steelblue'] + ['coral'] * len(K_VALUES)\n",
    "\n",
    "bars = ax1.bar(methods, accs, color=colors, alpha=0.7)\n",
    "ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Final Accuracy: Two-Neuron vs Comparator Bank')\n",
    "ax1.set_ylim(0.4, 1.05)\n",
    "ax1.tick_params(axis='x', rotation=30)\n",
    "\n",
    "for bar, acc in zip(bars, accs):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Training curves comparison\n",
    "ax2 = axes[1]\n",
    "ax2.plot(two_neuron_accs, label='Two-Neuron (BCE)', color='steelblue', lw=2)\n",
    "best_K = max(results_by_k.keys(), key=lambda k: results_by_k[k]['final_acc'])\n",
    "ax2.plot(results_by_k[best_K]['accuracies'], label=f'Comparator Bank K={best_K}', color='coral', lw=2)\n",
    "ax2.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Curves: Best Methods')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"CONCLUSIONS\")\nprint(\"=\" * 70)\n\nbest_K = max(results_by_k.keys(), key=lambda k: results_by_k[k]['final_acc'])\nbest_acc = results_by_k[best_K]['final_acc']\n\nprint(f\"\\n1. BEST COMPARATOR BANK: K={best_K} with accuracy {best_acc:.4f}\")\nprint(f\"   Two-neuron baseline: {two_neuron_final_acc:.4f}\")\nprint(f\"   Improvement: {best_acc - two_neuron_final_acc:+.4f}\")\n\nprint(\"\\n2. EFFECT OF K (Comparators per Class) - WITH SHARED WEIGHTS:\")\nfor K in K_VALUES:\n    acc = results_by_k[K]['final_acc']\n    n_eff = results_by_k[K]['n_effective_params']\n    print(f\"   K={K}: Acc={acc:.4f}, Effective Params={n_eff}\")\n\nprint(\"\\n3. WEIGHT SHARING BENEFITS:\")\nprint(\"   - All comparators see the SAME evidence\")\nprint(\"   - Different thresholds create true staircase approximation\")\nprint(\"   - Effective parameters stay constant regardless of K\")\nprint(\"   - More robust than independent weights\")\n\nprint(\"\\n4. HARDWARE IMPLICATIONS:\")\nprint(\"   - Shared weights = single fan-out from hidden layer\")\nprint(\"   - Different phi_offsets = different physical thresholds\")\nprint(\"   - Spike counting gives graded output\")\nprint(\"   - No need for precise analog readout\")\n\nprint(\"\\n5. RECOMMENDATION:\")\nif best_acc > two_neuron_final_acc:\n    print(f\"   ✓ Comparator bank (K={best_K}) outperforms two-neuron approach!\")\n    print(f\"   ✓ Shared weights provide true staircase approximation\")\nelse:\n    print(\"   Two-neuron approach with BCE loss remains competitive\")\n    print(\"   Consider comparator bank for robustness to threshold calibration\")\n\nprint(\"\\n\" + \"=\" * 70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}