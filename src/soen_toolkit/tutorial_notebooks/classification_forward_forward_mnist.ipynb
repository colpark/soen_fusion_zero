{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Forward-Forward MNIST Classification\n",
    "\n",
    "Extending Forward-Forward to 10-class MNIST with minimal hidden neurons (12).\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Input: 784 (28×28 flattened) + 10 (one-hot label) = 794\n",
    "Hidden: 12 SingleDendrite neurons\n",
    "Output: Goodness (sum of squared activations)\n",
    "```\n",
    "\n",
    "## Inference (10 forward passes)\n",
    "\n",
    "```\n",
    "For each digit d ∈ {0,1,...,9}:\n",
    "    X_embedded = [image_pixels, one_hot(d)]\n",
    "    goodness_d = forward(X_embedded)\n",
    "Predict: argmax(goodness_0, ..., goodness_9)\n",
    "```\n",
    "\n",
    "## Hardware Compatibility\n",
    "\n",
    "- Goodness = mean(I²) = power measurement\n",
    "- Label embedding = optical input modulation\n",
    "- No backward pass needed for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport gzip\nimport urllib.request\n\nfrom soen_toolkit.core import (\n    ConnectionConfig,\n    LayerConfig,\n    SimulationConfig,\n    SOENModelCore,\n)\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(f\"PyTorch version: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Direct MNIST download without torchvision\ndef download_mnist(data_dir='./data/mnist'):\n    \"\"\"Download MNIST dataset without torchvision.\"\"\"\n    os.makedirs(data_dir, exist_ok=True)\n    \n    base_url = 'https://ossci-datasets.s3.amazonaws.com/mnist/'\n    files = {\n        'train_images': 'train-images-idx3-ubyte.gz',\n        'train_labels': 'train-labels-idx1-ubyte.gz',\n        'test_images': 't10k-images-idx3-ubyte.gz',\n        'test_labels': 't10k-labels-idx1-ubyte.gz',\n    }\n    \n    paths = {}\n    for key, filename in files.items():\n        filepath = os.path.join(data_dir, filename)\n        if not os.path.exists(filepath):\n            print(f\"Downloading {filename}...\")\n            urllib.request.urlretrieve(base_url + filename, filepath)\n        paths[key] = filepath\n    \n    return paths\n\n\ndef load_mnist_images(filepath):\n    \"\"\"Load MNIST images from gzipped IDX file.\"\"\"\n    with gzip.open(filepath, 'rb') as f:\n        # Read header\n        magic = int.from_bytes(f.read(4), 'big')\n        n_images = int.from_bytes(f.read(4), 'big')\n        n_rows = int.from_bytes(f.read(4), 'big')\n        n_cols = int.from_bytes(f.read(4), 'big')\n        # Read data\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        return data.reshape(n_images, n_rows * n_cols).astype(np.float32) / 255.0\n\n\ndef load_mnist_labels(filepath):\n    \"\"\"Load MNIST labels from gzipped IDX file.\"\"\"\n    with gzip.open(filepath, 'rb') as f:\n        # Read header\n        magic = int.from_bytes(f.read(4), 'big')\n        n_labels = int.from_bytes(f.read(4), 'big')\n        # Read data\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\n\n# Download and load MNIST\npaths = download_mnist()\nX_train_full = torch.from_numpy(load_mnist_images(paths['train_images']))\ny_train_full = torch.from_numpy(load_mnist_labels(paths['train_labels'])).long()\nX_test_full = torch.from_numpy(load_mnist_images(paths['test_images']))\ny_test_full = torch.from_numpy(load_mnist_labels(paths['test_labels'])).long()\n\nprint(f\"Full dataset: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n\n# Use subset for faster experimentation\nN_TRAIN = 5000\nN_TEST = 1000\n\ntorch.manual_seed(42)\ntrain_idx = torch.randperm(len(X_train_full))[:N_TRAIN]\ntest_idx = torch.randperm(len(X_test_full))[:N_TEST]\n\nX_train = X_train_full[train_idx]\ny_train = y_train_full[train_idx]\nX_test = X_test_full[test_idx]\ny_test = y_test_full[test_idx]\n\n# Scale to SOEN operating range [0.025, 0.275]\nX_train = X_train * 0.25 + 0.025\nX_test = X_test * 0.25 + 0.025\n\nprint(f\"Training set: {X_train.shape}, {y_train.shape}\")\nprint(f\"Test set: {X_test.shape}, {y_test.shape}\")\nprint(f\"X range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\nprint(f\"Class distribution (train): {torch.bincount(y_train)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_train[i].view(28, 28).numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i].item()}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Samples (scaled to SOEN range)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Forward-Forward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "SEQ_LEN = 50\n",
    "LABEL_SCALE = 0.25  # Strong label signal\n",
    "\n",
    "def embed_label(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Embed one-hot label into input.\n",
    "    \n",
    "    Args:\n",
    "        X: [N, 784] flattened MNIST images\n",
    "        y: [N] class labels (0-9)\n",
    "    \n",
    "    Returns:\n",
    "        X_embedded: [N, 794] image + one-hot label\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    one_hot = torch.zeros(N, n_classes)\n",
    "    one_hot.scatter_(1, y.unsqueeze(1), label_scale)\n",
    "    return torch.cat([X, one_hot], dim=1)\n",
    "\n",
    "\n",
    "def create_positive_negative_pairs(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Create positive and negative samples for Forward-Forward.\n",
    "    \n",
    "    Positive: image with correct label\n",
    "    Negative: image with random wrong label\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Positive: correct labels\n",
    "    X_pos = embed_label(X, y, n_classes, label_scale)\n",
    "    \n",
    "    # Negative: random wrong labels\n",
    "    y_wrong = (y + torch.randint(1, n_classes, (N,))) % n_classes\n",
    "    X_neg = embed_label(X, y_wrong, n_classes, label_scale)\n",
    "    \n",
    "    return X_pos, X_neg\n",
    "\n",
    "\n",
    "# Test embedding\n",
    "X_pos, X_neg = create_positive_negative_pairs(X_train[:5], y_train[:5])\n",
    "print(f\"Embedded shape: {X_pos.shape}\")\n",
    "print(f\"Input dim: 784 pixels + 10 label = 794\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_goodness(activations):\n",
    "    \"\"\"\n",
    "    Compute goodness as mean of squared activations.\n",
    "    Hardware-compatible: measures mean power in the layer.\n",
    "    \"\"\"\n",
    "    return (activations ** 2).mean(dim=1)\n",
    "\n",
    "\n",
    "def forward_forward_loss(goodness_pos, goodness_neg, threshold=0.1, margin=0.05):\n",
    "    \"\"\"\n",
    "    Forward-Forward loss with contrastive term.\n",
    "    \"\"\"\n",
    "    loss_pos = F.softplus(threshold - goodness_pos).mean()\n",
    "    loss_neg = F.softplus(goodness_neg - threshold).mean()\n",
    "    contrastive = F.softplus(margin - (goodness_pos - goodness_neg)).mean()\n",
    "    return loss_pos + loss_neg + contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Build SOEN Model for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ff_mnist_model(hidden_dims, input_dim=794, dt=50.0):\n",
    "    \"\"\"\n",
    "    Build a SOEN model for Forward-Forward MNIST.\n",
    "    \n",
    "    Args:\n",
    "        hidden_dims: List of hidden layer dimensions (e.g., [12] or [12, 12])\n",
    "        input_dim: 784 pixels + 10 label = 794\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=dt,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = []\n",
    "    connections = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": input_dim},\n",
    "    ))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        \n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "                \"learnable_params\": {\n",
    "                    \"phi_offset\": False,\n",
    "                    \"bias_current\": False,\n",
    "                    \"gamma_plus\": False,\n",
    "                    \"gamma_minus\": False,\n",
    "                },\n",
    "            },\n",
    "        ))\n",
    "        \n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Test model\n",
    "HIDDEN_DIMS = [12]  # Only 12 hidden neurons!\n",
    "test_model = build_ff_mnist_model(HIDDEN_DIMS)\n",
    "print(f\"Model architecture: 794 → {HIDDEN_DIMS} → goodness\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in test_model.parameters() if p.requires_grad)}\")\n",
    "print(f\"  (794 × 12 = 9528 weights for first layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_ff_mnist_fast(model, X, y, batch_size=200, seq_len=10):\n    \"\"\"\n    Fast evaluation by batching all 10 class hypotheses together.\n    \n    Instead of 10 separate forward passes per batch, we concatenate all\n    hypotheses into a single large batch for one forward pass.\n    \"\"\"\n    model.eval()\n    N = X.shape[0]\n    all_predictions = []\n    \n    with torch.no_grad():\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            X_batch = X[start:end]  # [B, 784]\n            B = X_batch.shape[0]\n            \n            # Repeat each sample 10 times (one for each class hypothesis)\n            X_repeated = X_batch.unsqueeze(1).expand(-1, N_CLASSES, -1)  # [B, 10, 784]\n            X_repeated = X_repeated.reshape(B * N_CLASSES, 784)  # [B*10, 784]\n            \n            # Create labels 0-9 for each sample\n            y_hypotheses = torch.arange(N_CLASSES).unsqueeze(0).expand(B, -1)  # [B, 10]\n            y_hypotheses = y_hypotheses.reshape(B * N_CLASSES)  # [B*10]\n            \n            # Embed all hypotheses\n            X_embedded = embed_label(X_repeated, y_hypotheses)  # [B*10, 794]\n            X_seq = X_embedded.unsqueeze(1).expand(-1, seq_len, -1)  # [B*10, seq_len, 794]\n            \n            # Single forward pass for all hypotheses\n            _, layer_states = model(X_seq)\n            \n            # Compute goodness for all\n            total_goodness = torch.zeros(B * N_CLASSES)\n            for layer_idx in range(1, len(model.layers)):\n                act = layer_states[layer_idx][:, -1, :]  # [B*10, hidden]\n                total_goodness += compute_goodness(act)\n            \n            # Reshape to [B, 10] and get predictions\n            goodness_matrix = total_goodness.reshape(B, N_CLASSES)\n            predictions = goodness_matrix.argmax(dim=1)\n            all_predictions.append(predictions)\n    \n    all_predictions = torch.cat(all_predictions)\n    accuracy = (all_predictions == y).float().mean().item()\n    model.train()\n    return accuracy\n\n\ndef train_forward_forward_mnist(model, X_train, y_train, X_test, y_test,\n                                 n_epochs=100, lr=0.01, threshold=0.1, \n                                 batch_size=100, eval_subset=500, verbose=True):\n    \"\"\"\n    Train SOEN model with Forward-Forward on MNIST.\n    \n    Args:\n        eval_subset: Number of training samples to use for evaluation (for speed)\n    \"\"\"\n    model.train()\n    \n    # Layer-wise optimizers\n    hidden_layer_indices = [i for i, l in enumerate(model.layers) if l.layer_type != 'Input']\n    layer_optimizers = []\n    for conn_key in model.connections.keys():\n        conn_params = [model.connections[conn_key]]\n        layer_optimizers.append(torch.optim.Adam(conn_params, lr=lr))\n    \n    history = {\n        'loss': [],\n        'train_acc': [],\n        'test_acc': [],\n        'goodness_pos': [],\n        'goodness_neg': [],\n    }\n    \n    N = X_train.shape[0]\n    n_batches = (N + batch_size - 1) // batch_size\n    \n    # Subset for fast evaluation during training\n    eval_idx = torch.randperm(N)[:eval_subset]\n    X_train_eval = X_train[eval_idx]\n    y_train_eval = y_train[eval_idx]\n    \n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        epoch_g_pos = []\n        epoch_g_neg = []\n        \n        # Shuffle data\n        perm = torch.randperm(N)\n        X_shuffled = X_train[perm]\n        y_shuffled = y_train[perm]\n        \n        for batch_idx in range(n_batches):\n            start = batch_idx * batch_size\n            end = min(start + batch_size, N)\n            \n            X_batch = X_shuffled[start:end]\n            y_batch = y_shuffled[start:end]\n            \n            # Create pos/neg pairs\n            X_pos, X_neg = create_positive_negative_pairs(X_batch, y_batch)\n            \n            # Expand to sequence\n            X_pos_seq = X_pos.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n            X_neg_seq = X_neg.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n            \n            # Forward pass\n            _, layer_states_pos = model(X_pos_seq)\n            _, layer_states_neg = model(X_neg_seq)\n            \n            # Train each layer\n            batch_loss = 0\n            for layer_idx, opt in zip(hidden_layer_indices, layer_optimizers):\n                opt.zero_grad()\n                \n                act_pos = layer_states_pos[layer_idx][:, -1, :]\n                act_neg = layer_states_neg[layer_idx][:, -1, :]\n                \n                g_pos = compute_goodness(act_pos)\n                g_neg = compute_goodness(act_neg)\n                \n                epoch_g_pos.append(g_pos.mean().item())\n                epoch_g_neg.append(g_neg.mean().item())\n                \n                layer_loss = forward_forward_loss(g_pos, g_neg, threshold)\n                batch_loss += layer_loss.item()\n                \n                layer_loss.backward(retain_graph=True)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                opt.step()\n            \n            epoch_loss += batch_loss\n        \n        # Fast evaluation on subset\n        train_acc = evaluate_ff_mnist_fast(model, X_train_eval, y_train_eval)\n        test_acc = evaluate_ff_mnist_fast(model, X_test, y_test)\n        \n        history['loss'].append(epoch_loss / n_batches)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n        history['goodness_pos'].append(np.mean(epoch_g_pos))\n        history['goodness_neg'].append(np.mean(epoch_g_neg))\n        \n        if verbose and (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}: Loss={epoch_loss/n_batches:.4f}, \"\n                  f\"Train={train_acc:.4f}, Test={test_acc:.4f}, \"\n                  f\"G_pos={np.mean(epoch_g_pos):.4f}, G_neg={np.mean(epoch_g_neg):.4f}\")\n    \n    return history\n\n\ndef evaluate_ff_mnist(model, X, y, batch_size=200):\n    \"\"\"\n    Full evaluation (for final results, not during training).\n    \"\"\"\n    return evaluate_ff_mnist_fast(model, X, y, batch_size, seq_len=SEQ_LEN)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with only 12 hidden neurons\n",
    "HIDDEN_DIMS = [12]\n",
    "THRESHOLD = 0.1\n",
    "N_EPOCHS = 100\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print(f\"Training Forward-Forward MNIST classifier...\")\n",
    "print(f\"Architecture: 794 → {HIDDEN_DIMS} → goodness\")\n",
    "print(f\"Only {HIDDEN_DIMS[0]} hidden neurons for 10-class MNIST!\")\n",
    "print(f\"Threshold: {THRESHOLD}\")\n",
    "print(f\"Training samples: {N_TRAIN}, Test samples: {N_TEST}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = build_ff_mnist_model(HIDDEN_DIMS)\n",
    "\n",
    "history = train_forward_forward_mnist(\n",
    "    model, X_train, y_train, X_test, y_test,\n",
    "    n_epochs=N_EPOCHS, lr=LR, threshold=THRESHOLD,\n",
    "    batch_size=BATCH_SIZE, verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final test accuracy: {history['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['loss'], color='steelblue', lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Forward-Forward Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Goodness\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['goodness_pos'], label='Positive', color='green', lw=2)\n",
    "ax2.plot(history['goodness_neg'], label='Negative', color='red', lw=2)\n",
    "ax2.axhline(y=THRESHOLD, color='black', linestyle='--', label=f'Threshold={THRESHOLD}')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Goodness')\n",
    "ax2.set_title('Goodness Separation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['train_acc'], label='Train', color='coral', lw=2)\n",
    "ax3.plot(history['test_acc'], label='Test', color='steelblue', lw=2)\n",
    "ax3.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random (10%)')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Classification Accuracy')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0, 1.0)\n",
    "\n",
    "# Separation\n",
    "ax4 = axes[1, 1]\n",
    "separation = [p - n for p, n in zip(history['goodness_pos'], history['goodness_neg'])]\n",
    "ax4.plot(separation, color='purple', lw=2)\n",
    "ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('G_pos - G_neg')\n",
    "ax4.set_title('Goodness Separation')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Forward-Forward MNIST (12 hidden neurons)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "def get_predictions_ff(model, X, batch_size=200):\n    \"\"\"Get predictions and goodness for all samples (optimized batched version).\"\"\"\n    model.eval()\n    N = X.shape[0]\n    all_predictions = []\n    all_goodness = []\n    \n    with torch.no_grad():\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            X_batch = X[start:end]\n            B = X_batch.shape[0]\n            \n            # Batch all 10 class hypotheses together\n            X_repeated = X_batch.unsqueeze(1).expand(-1, N_CLASSES, -1).reshape(B * N_CLASSES, 784)\n            y_hypotheses = torch.arange(N_CLASSES).unsqueeze(0).expand(B, -1).reshape(B * N_CLASSES)\n            \n            X_embedded = embed_label(X_repeated, y_hypotheses)\n            X_seq = X_embedded.unsqueeze(1).expand(-1, SEQ_LEN, -1)\n            \n            _, layer_states = model(X_seq)\n            \n            total_goodness = torch.zeros(B * N_CLASSES)\n            for layer_idx in range(1, len(model.layers)):\n                act = layer_states[layer_idx][:, -1, :]\n                total_goodness += compute_goodness(act)\n            \n            goodness_matrix = total_goodness.reshape(B, N_CLASSES)\n            all_goodness.append(goodness_matrix)\n            all_predictions.append(goodness_matrix.argmax(dim=1))\n    \n    return torch.cat(all_predictions), torch.cat(all_goodness)\n\n\ndef compute_confusion_matrix(y_true, y_pred, n_classes=10):\n    \"\"\"Compute confusion matrix without sklearn.\"\"\"\n    cm = np.zeros((n_classes, n_classes), dtype=np.int32)\n    for true, pred in zip(y_true, y_pred):\n        cm[true, pred] += 1\n    return cm\n\n\n# Get test predictions\ntest_preds, test_goodness = get_predictions_ff(model, X_test)\n\n# Confusion matrix (no sklearn needed)\ncm = compute_confusion_matrix(y_test.numpy(), test_preds.numpy())\n\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(cm, cmap='Blues')\nax.set_xticks(range(10))\nax.set_yticks(range(10))\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title(f'Confusion Matrix (Test Acc: {history[\"test_acc\"][-1]:.2%})')\n\n# Add text annotations\nfor i in range(10):\n    for j in range(10):\n        text = ax.text(j, i, cm[i, j], ha='center', va='center',\n                       color='white' if cm[i, j] > cm.max()/2 else 'black')\n\nplt.colorbar(im)\nplt.tight_layout()\nplt.show()\n\n# Per-class accuracy\nprint(\"\\nPer-class accuracy:\")\nfor digit in range(10):\n    mask = y_test == digit\n    digit_acc = (test_preds[mask] == digit).float().mean().item()\n    print(f\"  Digit {digit}: {digit_acc:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some predictions\n",
    "n_show = 20\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= n_show:\n",
    "        break\n",
    "    \n",
    "    img = X_test[i].view(28, 28).numpy()\n",
    "    true_label = y_test[i].item()\n",
    "    pred_label = test_preds[i].item()\n",
    "    goodness_vals = test_goodness[i].numpy()\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Forward-Forward MNIST Predictions (green=correct, red=wrong)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show goodness distribution for a few samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    goodness_vals = test_goodness[i].numpy()\n",
    "    true_label = y_test[i].item()\n",
    "    pred_label = test_preds[i].item()\n",
    "    \n",
    "    colors = ['green' if d == true_label else 'lightgray' for d in range(10)]\n",
    "    colors[pred_label] = 'red' if pred_label != true_label else 'green'\n",
    "    \n",
    "    ax.bar(range(10), goodness_vals, color=colors)\n",
    "    ax.set_xticks(range(10))\n",
    "    ax.set_xlabel('Digit')\n",
    "    ax.set_ylabel('Goodness')\n",
    "    status = '✓' if pred_label == true_label else '✗'\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label} {status}')\n",
    "\n",
    "plt.suptitle('Goodness Distribution per Digit Hypothesis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9. Compare with Different Hidden Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hidden layer sizes\n",
    "hidden_configs = [\n",
    "    [8],\n",
    "    [12],\n",
    "    [16],\n",
    "    [24],\n",
    "    [12, 12],\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "print(\"Comparing different architectures...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for hidden_dims in hidden_configs:\n",
    "    torch.manual_seed(42)\n",
    "    model = build_ff_mnist_model(hidden_dims)\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    history = train_forward_forward_mnist(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, threshold=0.1,\n",
    "        batch_size=100, verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'hidden_dims': str(hidden_dims),\n",
    "        'n_params': n_params,\n",
    "        'train_acc': history['train_acc'][-1],\n",
    "        'test_acc': history['test_acc'][-1],\n",
    "    })\n",
    "    \n",
    "    print(f\"Hidden={str(hidden_dims):12s} | Params={n_params:6d} | \"\n",
    "          f\"Train={history['train_acc'][-1]:.4f} | Test={history['test_acc'][-1]:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS: FORWARD-FORWARD MNIST WITH 12 HIDDEN NEURONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. ARCHITECTURE:\")\n",
    "print(f\"   Input: 784 pixels + 10 label = 794\")\n",
    "print(f\"   Hidden: {HIDDEN_DIMS[0]} SingleDendrite neurons\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "print(f\"\\n2. PERFORMANCE:\")\n",
    "print(f\"   Train accuracy: {history['train_acc'][-1]:.2%}\")\n",
    "print(f\"   Test accuracy:  {history['test_acc'][-1]:.2%}\")\n",
    "print(f\"   Random baseline: 10%\")\n",
    "\n",
    "print(f\"\\n3. HARDWARE COMPATIBILITY:\")\n",
    "print(f\"   ✓ Goodness = mean(I²) = power measurement\")\n",
    "print(f\"   ✓ Label embedding = optical input modulation\")\n",
    "print(f\"   ✓ No backward pass for inference\")\n",
    "print(f\"   ✓ Only {HIDDEN_DIMS[0]} physical neurons needed!\")\n",
    "\n",
    "print(f\"\\n4. INFERENCE COST:\")\n",
    "print(f\"   10 forward passes per sample (one per digit hypothesis)\")\n",
    "print(f\"   Compare goodness across hypotheses to classify\")\n",
    "\n",
    "print(f\"\\n5. SCALING:\")\n",
    "print(f\"   More hidden neurons → better accuracy\")\n",
    "print(f\"   But even 12 neurons achieves meaningful classification!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}