{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Forward-Forward MNIST Classification\n",
    "\n",
    "Extending Forward-Forward to 10-class MNIST with minimal hidden neurons (12).\n",
    "\n",
    "## Architecture\n",
    "\n",
    "```\n",
    "Input: 784 (28×28 flattened) + 10 (one-hot label) = 794\n",
    "Hidden: 12 SingleDendrite neurons\n",
    "Output: Goodness (sum of squared activations)\n",
    "```\n",
    "\n",
    "## Inference (10 forward passes)\n",
    "\n",
    "```\n",
    "For each digit d ∈ {0,1,...,9}:\n",
    "    X_embedded = [image_pixels, one_hot(d)]\n",
    "    goodness_d = forward(X_embedded)\n",
    "Predict: argmax(goodness_0, ..., goodness_9)\n",
    "```\n",
    "\n",
    "## Hardware Compatibility\n",
    "\n",
    "- Goodness = mean(I²) = power measurement\n",
    "- Label embedding = optical input modulation\n",
    "- No backward pass needed for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from soen_toolkit.core import (\n",
    "    ConnectionConfig,\n",
    "    LayerConfig,\n",
    "    SimulationConfig,\n",
    "    SOENModelCore,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Use subset for faster experimentation\n",
    "N_TRAIN = 5000\n",
    "N_TEST = 1000\n",
    "\n",
    "train_idx = torch.randperm(len(train_dataset))[:N_TRAIN]\n",
    "test_idx = torch.randperm(len(test_dataset))[:N_TEST]\n",
    "\n",
    "# Extract data\n",
    "X_train = train_dataset.data[train_idx].float().view(-1, 784) / 255.0\n",
    "y_train = train_dataset.targets[train_idx]\n",
    "\n",
    "X_test = test_dataset.data[test_idx].float().view(-1, 784) / 255.0\n",
    "y_test = test_dataset.targets[test_idx]\n",
    "\n",
    "# Scale to SOEN operating range [0.025, 0.275]\n",
    "X_train = X_train * 0.25 + 0.025\n",
    "X_test = X_test * 0.25 + 0.025\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"X range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\n",
    "print(f\"Class distribution (train): {torch.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_train[i].view(28, 28).numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i].item()}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Samples (scaled to SOEN range)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Forward-Forward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "SEQ_LEN = 50\n",
    "LABEL_SCALE = 0.25  # Strong label signal\n",
    "\n",
    "def embed_label(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Embed one-hot label into input.\n",
    "    \n",
    "    Args:\n",
    "        X: [N, 784] flattened MNIST images\n",
    "        y: [N] class labels (0-9)\n",
    "    \n",
    "    Returns:\n",
    "        X_embedded: [N, 794] image + one-hot label\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    one_hot = torch.zeros(N, n_classes)\n",
    "    one_hot.scatter_(1, y.unsqueeze(1), label_scale)\n",
    "    return torch.cat([X, one_hot], dim=1)\n",
    "\n",
    "\n",
    "def create_positive_negative_pairs(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n",
    "    \"\"\"\n",
    "    Create positive and negative samples for Forward-Forward.\n",
    "    \n",
    "    Positive: image with correct label\n",
    "    Negative: image with random wrong label\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    # Positive: correct labels\n",
    "    X_pos = embed_label(X, y, n_classes, label_scale)\n",
    "    \n",
    "    # Negative: random wrong labels\n",
    "    y_wrong = (y + torch.randint(1, n_classes, (N,))) % n_classes\n",
    "    X_neg = embed_label(X, y_wrong, n_classes, label_scale)\n",
    "    \n",
    "    return X_pos, X_neg\n",
    "\n",
    "\n",
    "# Test embedding\n",
    "X_pos, X_neg = create_positive_negative_pairs(X_train[:5], y_train[:5])\n",
    "print(f\"Embedded shape: {X_pos.shape}\")\n",
    "print(f\"Input dim: 784 pixels + 10 label = 794\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_goodness(activations):\n",
    "    \"\"\"\n",
    "    Compute goodness as mean of squared activations.\n",
    "    Hardware-compatible: measures mean power in the layer.\n",
    "    \"\"\"\n",
    "    return (activations ** 2).mean(dim=1)\n",
    "\n",
    "\n",
    "def forward_forward_loss(goodness_pos, goodness_neg, threshold=0.1, margin=0.05):\n",
    "    \"\"\"\n",
    "    Forward-Forward loss with contrastive term.\n",
    "    \"\"\"\n",
    "    loss_pos = F.softplus(threshold - goodness_pos).mean()\n",
    "    loss_neg = F.softplus(goodness_neg - threshold).mean()\n",
    "    contrastive = F.softplus(margin - (goodness_pos - goodness_neg)).mean()\n",
    "    return loss_pos + loss_neg + contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Build SOEN Model for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ff_mnist_model(hidden_dims, input_dim=794, dt=50.0):\n",
    "    \"\"\"\n",
    "    Build a SOEN model for Forward-Forward MNIST.\n",
    "    \n",
    "    Args:\n",
    "        hidden_dims: List of hidden layer dimensions (e.g., [12] or [12, 12])\n",
    "        input_dim: 784 pixels + 10 label = 794\n",
    "    \"\"\"\n",
    "    sim_cfg = SimulationConfig(\n",
    "        dt=dt,\n",
    "        input_type=\"state\",\n",
    "        track_phi=False,\n",
    "        track_power=False,\n",
    "    )\n",
    "    \n",
    "    layers = []\n",
    "    connections = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(LayerConfig(\n",
    "        layer_id=0,\n",
    "        layer_type=\"Input\",\n",
    "        params={\"dim\": input_dim},\n",
    "    ))\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i, hidden_dim in enumerate(hidden_dims):\n",
    "        layer_id = i + 1\n",
    "        \n",
    "        layers.append(LayerConfig(\n",
    "            layer_id=layer_id,\n",
    "            layer_type=\"SingleDendrite\",\n",
    "            params={\n",
    "                \"dim\": hidden_dim,\n",
    "                \"solver\": \"FE\",\n",
    "                \"source_func\": \"Heaviside_fit_state_dep\",\n",
    "                \"phi_offset\": 0.02,\n",
    "                \"bias_current\": 1.98,\n",
    "                \"gamma_plus\": 0.0005,\n",
    "                \"gamma_minus\": 1e-6,\n",
    "                \"learnable_params\": {\n",
    "                    \"phi_offset\": False,\n",
    "                    \"bias_current\": False,\n",
    "                    \"gamma_plus\": False,\n",
    "                    \"gamma_minus\": False,\n",
    "                },\n",
    "            },\n",
    "        ))\n",
    "        \n",
    "        connections.append(ConnectionConfig(\n",
    "            from_layer=layer_id - 1,\n",
    "            to_layer=layer_id,\n",
    "            connection_type=\"all_to_all\",\n",
    "            learnable=True,\n",
    "            params={\"init\": \"xavier_uniform\"},\n",
    "        ))\n",
    "    \n",
    "    model = SOENModelCore(\n",
    "        sim_config=sim_cfg,\n",
    "        layers_config=layers,\n",
    "        connections_config=connections,\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Test model\n",
    "HIDDEN_DIMS = [12]  # Only 12 hidden neurons!\n",
    "test_model = build_ff_mnist_model(HIDDEN_DIMS)\n",
    "print(f\"Model architecture: 794 → {HIDDEN_DIMS} → goodness\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in test_model.parameters() if p.requires_grad)}\")\n",
    "print(f\"  (794 × 12 = 9528 weights for first layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forward_forward_mnist(model, X_train, y_train, X_test, y_test,\n",
    "                                 n_epochs=100, lr=0.01, threshold=0.1, \n",
    "                                 batch_size=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Train SOEN model with Forward-Forward on MNIST.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Layer-wise optimizers\n",
    "    hidden_layer_indices = [i for i, l in enumerate(model.layers) if l.layer_type != 'Input']\n",
    "    layer_optimizers = []\n",
    "    for conn_key in model.connections.keys():\n",
    "        conn_params = [model.connections[conn_key]]\n",
    "        layer_optimizers.append(torch.optim.Adam(conn_params, lr=lr))\n",
    "    \n",
    "    history = {\n",
    "        'loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': [],\n",
    "        'goodness_pos': [],\n",
    "        'goodness_neg': [],\n",
    "    }\n",
    "    \n",
    "    N = X_train.shape[0]\n",
    "    n_batches = (N + batch_size - 1) // batch_size\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_g_pos = []\n",
    "        epoch_g_neg = []\n",
    "        \n",
    "        # Shuffle data\n",
    "        perm = torch.randperm(N)\n",
    "        X_shuffled = X_train[perm]\n",
    "        y_shuffled = y_train[perm]\n",
    "        \n",
    "        for batch_idx in range(n_batches):\n",
    "            start = batch_idx * batch_size\n",
    "            end = min(start + batch_size, N)\n",
    "            \n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Create pos/neg pairs\n",
    "            X_pos, X_neg = create_positive_negative_pairs(X_batch, y_batch)\n",
    "            \n",
    "            # Expand to sequence\n",
    "            X_pos_seq = X_pos.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "            X_neg_seq = X_neg.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "            \n",
    "            # Forward pass\n",
    "            _, layer_states_pos = model(X_pos_seq)\n",
    "            _, layer_states_neg = model(X_neg_seq)\n",
    "            \n",
    "            # Train each layer\n",
    "            batch_loss = 0\n",
    "            for layer_idx, opt in zip(hidden_layer_indices, layer_optimizers):\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                act_pos = layer_states_pos[layer_idx][:, -1, :]\n",
    "                act_neg = layer_states_neg[layer_idx][:, -1, :]\n",
    "                \n",
    "                g_pos = compute_goodness(act_pos)\n",
    "                g_neg = compute_goodness(act_neg)\n",
    "                \n",
    "                epoch_g_pos.append(g_pos.mean().item())\n",
    "                epoch_g_neg.append(g_neg.mean().item())\n",
    "                \n",
    "                layer_loss = forward_forward_loss(g_pos, g_neg, threshold)\n",
    "                batch_loss += layer_loss.item()\n",
    "                \n",
    "                layer_loss.backward(retain_graph=True)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                opt.step()\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "        # Evaluate\n",
    "        train_acc = evaluate_ff_mnist(model, X_train, y_train)\n",
    "        test_acc = evaluate_ff_mnist(model, X_test, y_test)\n",
    "        \n",
    "        history['loss'].append(epoch_loss / n_batches)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['goodness_pos'].append(np.mean(epoch_g_pos))\n",
    "        history['goodness_neg'].append(np.mean(epoch_g_neg))\n",
    "        \n",
    "        if verbose and (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}: Loss={epoch_loss/n_batches:.4f}, \"\n",
    "                  f\"Train={train_acc:.4f}, Test={test_acc:.4f}, \"\n",
    "                  f\"G_pos={np.mean(epoch_g_pos):.4f}, G_neg={np.mean(epoch_g_neg):.4f}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_ff_mnist(model, X, y, batch_size=200):\n",
    "    \"\"\"\n",
    "    Evaluate Forward-Forward model on MNIST.\n",
    "    \n",
    "    For each sample, try all 10 digit labels and pick the one with highest goodness.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    N = X.shape[0]\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    for start in range(0, N, batch_size):\n",
    "        end = min(start + batch_size, N)\n",
    "        X_batch = X[start:end]\n",
    "        batch_size_actual = X_batch.shape[0]\n",
    "        \n",
    "        # Compute goodness for each class hypothesis\n",
    "        batch_goodness = []\n",
    "        \n",
    "        for c in range(N_CLASSES):\n",
    "            y_test = torch.full((batch_size_actual,), c, dtype=torch.long)\n",
    "            X_embedded = embed_label(X_batch, y_test)\n",
    "            X_seq = X_embedded.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, layer_states = model(X_seq)\n",
    "                total_goodness = torch.zeros(batch_size_actual)\n",
    "                for layer_idx in range(1, len(model.layers)):\n",
    "                    act = layer_states[layer_idx][:, -1, :]\n",
    "                    total_goodness += compute_goodness(act)\n",
    "            \n",
    "            batch_goodness.append(total_goodness)\n",
    "        \n",
    "        goodness_matrix = torch.stack(batch_goodness, dim=1)  # [batch, 10]\n",
    "        predictions = goodness_matrix.argmax(dim=1)\n",
    "        all_predictions.append(predictions)\n",
    "    \n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    accuracy = (all_predictions == y).float().mean().item()\n",
    "    \n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with only 12 hidden neurons\n",
    "HIDDEN_DIMS = [12]\n",
    "THRESHOLD = 0.1\n",
    "N_EPOCHS = 100\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "print(f\"Training Forward-Forward MNIST classifier...\")\n",
    "print(f\"Architecture: 794 → {HIDDEN_DIMS} → goodness\")\n",
    "print(f\"Only {HIDDEN_DIMS[0]} hidden neurons for 10-class MNIST!\")\n",
    "print(f\"Threshold: {THRESHOLD}\")\n",
    "print(f\"Training samples: {N_TRAIN}, Test samples: {N_TEST}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = build_ff_mnist_model(HIDDEN_DIMS)\n",
    "\n",
    "history = train_forward_forward_mnist(\n",
    "    model, X_train, y_train, X_test, y_test,\n",
    "    n_epochs=N_EPOCHS, lr=LR, threshold=THRESHOLD,\n",
    "    batch_size=BATCH_SIZE, verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Final train accuracy: {history['train_acc'][-1]:.4f}\")\n",
    "print(f\"Final test accuracy: {history['test_acc'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Loss\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(history['loss'], color='steelblue', lw=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Forward-Forward Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Goodness\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history['goodness_pos'], label='Positive', color='green', lw=2)\n",
    "ax2.plot(history['goodness_neg'], label='Negative', color='red', lw=2)\n",
    "ax2.axhline(y=THRESHOLD, color='black', linestyle='--', label=f'Threshold={THRESHOLD}')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Mean Goodness')\n",
    "ax2.set_title('Goodness Separation')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(history['train_acc'], label='Train', color='coral', lw=2)\n",
    "ax3.plot(history['test_acc'], label='Test', color='steelblue', lw=2)\n",
    "ax3.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random (10%)')\n",
    "ax3.set_xlabel('Epoch')\n",
    "ax3.set_ylabel('Accuracy')\n",
    "ax3.set_title('Classification Accuracy')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_ylim(0, 1.0)\n",
    "\n",
    "# Separation\n",
    "ax4 = axes[1, 1]\n",
    "separation = [p - n for p, n in zip(history['goodness_pos'], history['goodness_neg'])]\n",
    "ax4.plot(separation, color='purple', lw=2)\n",
    "ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "ax4.set_xlabel('Epoch')\n",
    "ax4.set_ylabel('G_pos - G_neg')\n",
    "ax4.set_title('Goodness Separation')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Forward-Forward MNIST (12 hidden neurons)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_ff(model, X, batch_size=200):\n",
    "    \"\"\"Get predictions for all samples.\"\"\"\n",
    "    model.eval()\n",
    "    N = X.shape[0]\n",
    "    all_predictions = []\n",
    "    all_goodness = []\n",
    "    \n",
    "    for start in range(0, N, batch_size):\n",
    "        end = min(start + batch_size, N)\n",
    "        X_batch = X[start:end]\n",
    "        batch_size_actual = X_batch.shape[0]\n",
    "        \n",
    "        batch_goodness = []\n",
    "        for c in range(N_CLASSES):\n",
    "            y_test = torch.full((batch_size_actual,), c, dtype=torch.long)\n",
    "            X_embedded = embed_label(X_batch, y_test)\n",
    "            X_seq = X_embedded.unsqueeze(1).expand(-1, SEQ_LEN, -1).clone()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, layer_states = model(X_seq)\n",
    "                total_goodness = torch.zeros(batch_size_actual)\n",
    "                for layer_idx in range(1, len(model.layers)):\n",
    "                    act = layer_states[layer_idx][:, -1, :]\n",
    "                    total_goodness += compute_goodness(act)\n",
    "            batch_goodness.append(total_goodness)\n",
    "        \n",
    "        goodness_matrix = torch.stack(batch_goodness, dim=1)\n",
    "        all_goodness.append(goodness_matrix)\n",
    "        all_predictions.append(goodness_matrix.argmax(dim=1))\n",
    "    \n",
    "    return torch.cat(all_predictions), torch.cat(all_goodness)\n",
    "\n",
    "\n",
    "# Get test predictions\n",
    "test_preds, test_goodness = get_predictions_ff(model, X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test.numpy(), test_preds.numpy())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "ax.set_xticks(range(10))\n",
    "ax.set_yticks(range(10))\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title(f'Confusion Matrix (Test Acc: {history[\"test_acc\"][-1]:.2%})')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        text = ax.text(j, i, cm[i, j], ha='center', va='center',\n",
    "                       color='white' if cm[i, j] > cm.max()/2 else 'black')\n",
    "\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for digit in range(10):\n",
    "    mask = y_test == digit\n",
    "    digit_acc = (test_preds[mask] == digit).float().mean().item()\n",
    "    print(f\"  Digit {digit}: {digit_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some predictions\n",
    "n_show = 20\n",
    "fig, axes = plt.subplots(4, 5, figsize=(15, 12))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= n_show:\n",
    "        break\n",
    "    \n",
    "    img = X_test[i].view(28, 28).numpy()\n",
    "    true_label = y_test[i].item()\n",
    "    pred_label = test_preds[i].item()\n",
    "    goodness_vals = test_goodness[i].numpy()\n",
    "    \n",
    "    ax.imshow(img, cmap='gray')\n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label}', color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Forward-Forward MNIST Predictions (green=correct, red=wrong)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show goodness distribution for a few samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    goodness_vals = test_goodness[i].numpy()\n",
    "    true_label = y_test[i].item()\n",
    "    pred_label = test_preds[i].item()\n",
    "    \n",
    "    colors = ['green' if d == true_label else 'lightgray' for d in range(10)]\n",
    "    colors[pred_label] = 'red' if pred_label != true_label else 'green'\n",
    "    \n",
    "    ax.bar(range(10), goodness_vals, color=colors)\n",
    "    ax.set_xticks(range(10))\n",
    "    ax.set_xlabel('Digit')\n",
    "    ax.set_ylabel('Goodness')\n",
    "    status = '✓' if pred_label == true_label else '✗'\n",
    "    ax.set_title(f'True: {true_label}, Pred: {pred_label} {status}')\n",
    "\n",
    "plt.suptitle('Goodness Distribution per Digit Hypothesis', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9. Compare with Different Hidden Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different hidden layer sizes\n",
    "hidden_configs = [\n",
    "    [8],\n",
    "    [12],\n",
    "    [16],\n",
    "    [24],\n",
    "    [12, 12],\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "\n",
    "print(\"Comparing different architectures...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for hidden_dims in hidden_configs:\n",
    "    torch.manual_seed(42)\n",
    "    model = build_ff_mnist_model(hidden_dims)\n",
    "    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    history = train_forward_forward_mnist(\n",
    "        model, X_train, y_train, X_test, y_test,\n",
    "        n_epochs=50, lr=0.01, threshold=0.1,\n",
    "        batch_size=100, verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_results.append({\n",
    "        'hidden_dims': str(hidden_dims),\n",
    "        'n_params': n_params,\n",
    "        'train_acc': history['train_acc'][-1],\n",
    "        'test_acc': history['test_acc'][-1],\n",
    "    })\n",
    "    \n",
    "    print(f\"Hidden={str(hidden_dims):12s} | Params={n_params:6d} | \"\n",
    "          f\"Train={history['train_acc'][-1]:.4f} | Test={history['test_acc'][-1]:.4f}\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONCLUSIONS: FORWARD-FORWARD MNIST WITH 12 HIDDEN NEURONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. ARCHITECTURE:\")\n",
    "print(f\"   Input: 784 pixels + 10 label = 794\")\n",
    "print(f\"   Hidden: {HIDDEN_DIMS[0]} SingleDendrite neurons\")\n",
    "print(f\"   Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "print(f\"\\n2. PERFORMANCE:\")\n",
    "print(f\"   Train accuracy: {history['train_acc'][-1]:.2%}\")\n",
    "print(f\"   Test accuracy:  {history['test_acc'][-1]:.2%}\")\n",
    "print(f\"   Random baseline: 10%\")\n",
    "\n",
    "print(f\"\\n3. HARDWARE COMPATIBILITY:\")\n",
    "print(f\"   ✓ Goodness = mean(I²) = power measurement\")\n",
    "print(f\"   ✓ Label embedding = optical input modulation\")\n",
    "print(f\"   ✓ No backward pass for inference\")\n",
    "print(f\"   ✓ Only {HIDDEN_DIMS[0]} physical neurons needed!\")\n",
    "\n",
    "print(f\"\\n4. INFERENCE COST:\")\n",
    "print(f\"   10 forward passes per sample (one per digit hypothesis)\")\n",
    "print(f\"   Compare goodness across hypotheses to classify\")\n",
    "\n",
    "print(f\"\\n5. SCALING:\")\n",
    "print(f\"   More hidden neurons → better accuracy\")\n",
    "print(f\"   But even 12 neurons achieves meaningful classification!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
