{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# Forward-Forward MNIST Classification (10 Classes)\n\n10-class MNIST classification using Forward-Forward algorithm with minimal hidden neurons (<26).\n\n## Architecture\n\n```\nInput: 784 (28×28 flattened) + 10 (one-hot label) = 794\nHidden: ≤24 SingleDendrite neurons\nOutput: Goodness (sum of squared activations)\n```\n\n## Inference (10 forward passes)\n\n```\nFor each class c ∈ {0, 1, ..., 9}:\n    X_embedded = [image_pixels, one_hot(c)]\n    goodness_c = forward(X_embedded)\nPredict: argmax(goodness_0, ..., goodness_9)\n```\n\n## Hardware Compatibility\n\n- Goodness = mean(I²) = power measurement\n- Label embedding = optical input modulation\n- No backward pass needed for inference\n\n## Training Improvements\n\n- Learning rate decay (0.98/epoch)\n- Weight decay (1e-4) for regularization\n- More training data (20K samples)\n- Architecture search for optimal neuron count"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport gzip\nimport urllib.request\n\nfrom soen_toolkit.core import (\n    ConnectionConfig,\n    LayerConfig,\n    SimulationConfig,\n    SOENModelCore,\n)\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\nprint(f\"PyTorch version: {torch.__version__}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## 1. Load MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Direct MNIST download without torchvision\ndef download_mnist(data_dir='./data/mnist'):\n    \"\"\"Download MNIST dataset without torchvision.\"\"\"\n    os.makedirs(data_dir, exist_ok=True)\n    \n    base_url = 'https://ossci-datasets.s3.amazonaws.com/mnist/'\n    files = {\n        'train_images': 'train-images-idx3-ubyte.gz',\n        'train_labels': 'train-labels-idx1-ubyte.gz',\n        'test_images': 't10k-images-idx3-ubyte.gz',\n        'test_labels': 't10k-labels-idx1-ubyte.gz',\n    }\n    \n    paths = {}\n    for key, filename in files.items():\n        filepath = os.path.join(data_dir, filename)\n        if not os.path.exists(filepath):\n            print(f\"Downloading {filename}...\")\n            urllib.request.urlretrieve(base_url + filename, filepath)\n        paths[key] = filepath\n    \n    return paths\n\n\ndef load_mnist_images(filepath):\n    \"\"\"Load MNIST images from gzipped IDX file.\"\"\"\n    with gzip.open(filepath, 'rb') as f:\n        magic = int.from_bytes(f.read(4), 'big')\n        n_images = int.from_bytes(f.read(4), 'big')\n        n_rows = int.from_bytes(f.read(4), 'big')\n        n_cols = int.from_bytes(f.read(4), 'big')\n        data = np.frombuffer(f.read(), dtype=np.uint8)\n        return data.reshape(n_images, n_rows * n_cols).astype(np.float32) / 255.0\n\n\ndef load_mnist_labels(filepath):\n    \"\"\"Load MNIST labels from gzipped IDX file.\"\"\"\n    with gzip.open(filepath, 'rb') as f:\n        magic = int.from_bytes(f.read(4), 'big')\n        n_labels = int.from_bytes(f.read(4), 'big')\n        return np.frombuffer(f.read(), dtype=np.uint8)\n\n\n# Download and load MNIST\npaths = download_mnist()\nX_train_full = torch.from_numpy(load_mnist_images(paths['train_images']))\ny_train_full = torch.from_numpy(load_mnist_labels(paths['train_labels'])).long()\nX_test_full = torch.from_numpy(load_mnist_images(paths['test_images']))\ny_test_full = torch.from_numpy(load_mnist_labels(paths['test_labels'])).long()\n\nprint(f\"Full dataset: Train={X_train_full.shape}, Test={X_test_full.shape}\")\n\n# Use MORE training data for better performance\nN_TRAIN = 20000  # Increased from 5000 for better generalization\nN_TEST = 2000    # More test samples for stable evaluation\n\ntorch.manual_seed(42)\ntrain_idx = torch.randperm(len(X_train_full))[:N_TRAIN]\ntest_idx = torch.randperm(len(X_test_full))[:N_TEST]\n\nX_train = X_train_full[train_idx]\ny_train = y_train_full[train_idx]\nX_test = X_test_full[test_idx]\ny_test = y_test_full[test_idx]\n\n# Scale to SOEN operating range [0.025, 0.275]\nX_train = X_train * 0.25 + 0.025\nX_test = X_test * 0.25 + 0.025\n\nprint(f\"\\nUsing subset:\")\nprint(f\"  Training set: {X_train.shape}, labels: {y_train.shape}\")\nprint(f\"  Test set: {X_test.shape}, labels: {y_test.shape}\")\nprint(f\"  X range: [{X_train.min():.3f}, {X_train.max():.3f}]\")\nprint(f\"  Class distribution (train): {torch.bincount(y_train)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = X_train[i].view(28, 28).numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[i].item()}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('MNIST Samples (scaled to SOEN range)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Forward-Forward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "N_CLASSES = 10  # Full 10-class MNIST\nSEQ_LEN = 1     # Single timestep (no temporal simulation needed for training)\nLABEL_SCALE = 0.25  # Strong label signal\n\ndef embed_label(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n    \"\"\"\n    Embed one-hot label into input.\n    \n    Args:\n        X: [N, 784] flattened MNIST images\n        y: [N] class labels (0-9)\n    \n    Returns:\n        X_embedded: [N, 794] image + one-hot label (784 + 10)\n    \"\"\"\n    N = X.shape[0]\n    one_hot = torch.zeros(N, n_classes)\n    one_hot.scatter_(1, y.unsqueeze(1), label_scale)\n    return torch.cat([X, one_hot], dim=1)\n\n\ndef create_positive_negative_pairs(X, y, n_classes=N_CLASSES, label_scale=LABEL_SCALE):\n    \"\"\"\n    Create positive and negative samples for Forward-Forward.\n    \n    Positive: image with correct label\n    Negative: image with random wrong label\n    \"\"\"\n    N = X.shape[0]\n    \n    # Positive: correct labels\n    X_pos = embed_label(X, y, n_classes, label_scale)\n    \n    # Negative: random wrong labels\n    y_wrong = (y + torch.randint(1, n_classes, (N,))) % n_classes\n    X_neg = embed_label(X, y_wrong, n_classes, label_scale)\n    \n    return X_pos, X_neg\n\n\n# Test embedding\nX_pos, X_neg = create_positive_negative_pairs(X_train[:5], y_train[:5])\nprint(f\"Embedded shape: {X_pos.shape}\")\nprint(f\"Input dim: 784 pixels + {N_CLASSES} label = {784 + N_CLASSES}\")\nprint(f\"SEQ_LEN = {SEQ_LEN} (single-step forward pass)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "def compute_goodness(activations):\n    \"\"\"\n    Compute goodness as mean of squared activations.\n    Hardware-compatible: measures mean power in the layer.\n    \"\"\"\n    return (activations ** 2).mean(dim=1)\n\n\ndef forward_forward_loss(goodness_pos, goodness_neg, margin=0.01):\n    \"\"\"\n    Contrastive Forward-Forward loss.\n    \n    Push G_pos to be greater than G_neg by at least margin.\n    Small margin (0.01) works well when separation is ~0.1.\n    \"\"\"\n    return F.softplus(margin - (goodness_pos - goodness_neg)).mean()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 3. Build SOEN Model for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "INPUT_DIM = 784 + N_CLASSES  # 794 for 10-class classification\n\ndef build_ff_mnist_model(hidden_dims, input_dim=INPUT_DIM, dt=1.0):\n    \"\"\"\n    Build a SOEN model for Forward-Forward MNIST.\n    \n    Args:\n        hidden_dims: List of hidden layer dimensions (e.g., [12] or [12, 12])\n        input_dim: 784 pixels + N_CLASSES label\n    \n    Note: gamma_plus=1.0 gives activations in reasonable range for training.\n    \"\"\"\n    sim_cfg = SimulationConfig(\n        dt=dt,\n        input_type=\"state\",\n        track_phi=False,\n        track_power=False,\n    )\n    \n    layers = []\n    connections = []\n    \n    # Input layer\n    layers.append(LayerConfig(\n        layer_id=0,\n        layer_type=\"Input\",\n        params={\"dim\": input_dim},\n    ))\n    \n    # Hidden layers\n    for i, hidden_dim in enumerate(hidden_dims):\n        layer_id = i + 1\n        \n        layers.append(LayerConfig(\n            layer_id=layer_id,\n            layer_type=\"SingleDendrite\",\n            params={\n                \"dim\": hidden_dim,\n                \"solver\": \"FE\",\n                \"source_func\": \"Heaviside_fit_state_dep\",\n                \"phi_offset\": 0.02,\n                \"bias_current\": 1.98,\n                \"gamma_plus\": 1.0,  # Much larger for meaningful activations\n                \"gamma_minus\": 1e-6,\n                \"learnable_params\": {\n                    \"phi_offset\": False,\n                    \"bias_current\": False,\n                    \"gamma_plus\": False,\n                    \"gamma_minus\": False,\n                },\n            },\n        ))\n        \n        connections.append(ConnectionConfig(\n            from_layer=layer_id - 1,\n            to_layer=layer_id,\n            connection_type=\"all_to_all\",\n            learnable=True,\n            params={\"init\": \"xavier_uniform\"},\n        ))\n    \n    model = SOENModelCore(\n        sim_config=sim_cfg,\n        layers_config=layers,\n        connections_config=connections,\n    )\n    \n    return model\n\n\n# Test model\nHIDDEN_DIMS = [12]  # Only 12 hidden neurons!\ntest_model = build_ff_mnist_model(HIDDEN_DIMS)\nprint(f\"Model architecture: {INPUT_DIM} → {HIDDEN_DIMS} → goodness\")\nprint(f\"Parameters: {sum(p.numel() for p in test_model.parameters() if p.requires_grad)}\")\nprint(f\"gamma_plus=1.0, dt=1.0 → activations ~ g(φ) (order 1)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## 4. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_ff_mnist_fast(model, X, y, batch_size=200, seq_len=1):\n    \"\"\"\n    Fast evaluation by batching all class hypotheses together.\n    \"\"\"\n    model.eval()\n    N = X.shape[0]\n    all_predictions = []\n    \n    with torch.no_grad():\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            X_batch = X[start:end]\n            B = X_batch.shape[0]\n            \n            # Repeat each sample N_CLASSES times\n            X_repeated = X_batch.unsqueeze(1).expand(-1, N_CLASSES, -1).reshape(B * N_CLASSES, 784)\n            y_hypotheses = torch.arange(N_CLASSES).unsqueeze(0).expand(B, -1).reshape(B * N_CLASSES)\n            \n            X_embedded = embed_label(X_repeated, y_hypotheses)\n            X_seq = X_embedded.unsqueeze(1).expand(-1, seq_len, -1)\n            \n            _, layer_states = model(X_seq)\n            \n            total_goodness = torch.zeros(B * N_CLASSES)\n            for layer_idx in range(1, len(model.layers)):\n                act = layer_states[layer_idx][:, -1, :]\n                total_goodness += compute_goodness(act)\n            \n            goodness_matrix = total_goodness.reshape(B, N_CLASSES)\n            predictions = goodness_matrix.argmax(dim=1)\n            all_predictions.append(predictions)\n    \n    all_predictions = torch.cat(all_predictions)\n    accuracy = (all_predictions == y).float().mean().item()\n    model.train()\n    return accuracy\n\n\ndef train_forward_forward_mnist(model, X_train, y_train, X_test, y_test,\n                                 n_epochs=100, lr=0.01, margin=0.01,\n                                 batch_size=100, eval_subset=1000, verbose=True,\n                                 weight_decay=1e-4, lr_decay=0.95):\n    \"\"\"\n    Train SOEN model with Forward-Forward on MNIST.\n    \n    For multi-layer networks, we accumulate losses from all layers and do\n    a single backward pass to avoid gradient computation issues.\n    \"\"\"\n    model.train()\n    \n    # Single optimizer for all parameters (simpler for multi-layer)\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=lr_decay)\n    \n    # Track which layers are hidden (for goodness computation)\n    hidden_layer_indices = [i for i, l in enumerate(model.layers) if l.layer_type != 'Input']\n    \n    history = {\n        'loss': [],\n        'train_acc': [],\n        'test_acc': [],\n        'goodness_pos': [],\n        'goodness_neg': [],\n        'lr': [],\n    }\n    \n    N = X_train.shape[0]\n    n_batches = (N + batch_size - 1) // batch_size\n    \n    # Subset for fast evaluation during training\n    eval_idx = torch.randperm(N)[:min(eval_subset, N)]\n    X_train_eval = X_train[eval_idx]\n    y_train_eval = y_train[eval_idx]\n    \n    best_test_acc = 0\n    \n    for epoch in range(n_epochs):\n        epoch_loss = 0\n        epoch_g_pos = []\n        epoch_g_neg = []\n        \n        # Shuffle data\n        perm = torch.randperm(N)\n        X_shuffled = X_train[perm]\n        y_shuffled = y_train[perm]\n        \n        for batch_idx in range(n_batches):\n            start = batch_idx * batch_size\n            end = min(start + batch_size, N)\n            \n            X_batch = X_shuffled[start:end]\n            y_batch = y_shuffled[start:end]\n            \n            # Create pos/neg pairs\n            X_pos, X_neg = create_positive_negative_pairs(X_batch, y_batch)\n            \n            # Single timestep forward pass\n            X_pos_seq = X_pos.unsqueeze(1)  # [B, 1, 794]\n            X_neg_seq = X_neg.unsqueeze(1)  # [B, 1, 794]\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            _, layer_states_pos = model(X_pos_seq)\n            _, layer_states_neg = model(X_neg_seq)\n            \n            # Accumulate loss from all hidden layers\n            total_loss = 0\n            batch_g_pos_list = []\n            batch_g_neg_list = []\n            \n            for layer_idx in hidden_layer_indices:\n                act_pos = layer_states_pos[layer_idx][:, -1, :]\n                act_neg = layer_states_neg[layer_idx][:, -1, :]\n                \n                g_pos = compute_goodness(act_pos)\n                g_neg = compute_goodness(act_neg)\n                \n                batch_g_pos_list.append(g_pos.mean().item())\n                batch_g_neg_list.append(g_neg.mean().item())\n                \n                layer_loss = forward_forward_loss(g_pos, g_neg, margin)\n                total_loss = total_loss + layer_loss\n            \n            # Single backward pass for accumulated loss\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            batch_loss = total_loss.item()\n            batch_g_pos = np.mean(batch_g_pos_list)\n            batch_g_neg = np.mean(batch_g_neg_list)\n            \n            epoch_loss += batch_loss\n            epoch_g_pos.append(batch_g_pos)\n            epoch_g_neg.append(batch_g_neg)\n            \n            # Print batch progress\n            if verbose and batch_idx % 20 == 0:\n                print(f\"\\rEpoch {epoch+1}/{n_epochs} | Batch {batch_idx+1}/{n_batches} | \"\n                      f\"Loss: {batch_loss:.4f} | G+: {batch_g_pos:.6f} | G-: {batch_g_neg:.6f}\", end=\"\")\n        \n        # Step LR scheduler\n        scheduler.step()\n        current_lr = optimizer.param_groups[0]['lr']\n        \n        # Fast evaluation\n        train_acc = evaluate_ff_mnist_fast(model, X_train_eval, y_train_eval)\n        test_acc = evaluate_ff_mnist_fast(model, X_test, y_test)\n        \n        if test_acc > best_test_acc:\n            best_test_acc = test_acc\n        \n        history['loss'].append(epoch_loss / n_batches)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n        history['goodness_pos'].append(np.mean(epoch_g_pos))\n        history['goodness_neg'].append(np.mean(epoch_g_neg))\n        history['lr'].append(current_lr)\n        \n        # Print epoch summary\n        if verbose:\n            sep = np.mean(epoch_g_pos) - np.mean(epoch_g_neg)\n            print(f\"\\rEpoch {epoch+1}/{n_epochs} | Loss: {epoch_loss/n_batches:.4f} | \"\n                  f\"Train: {train_acc:.4f} | Test: {test_acc:.4f} | \"\n                  f\"Best: {best_test_acc:.4f} | LR: {current_lr:.6f}    \")\n    \n    return history\n\n\ndef evaluate_ff_mnist(model, X, y, batch_size=200):\n    \"\"\"Full evaluation (for final results).\"\"\"\n    return evaluate_ff_mnist_fast(model, X, y, batch_size, seq_len=SEQ_LEN)"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 5. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "# Build model with 24 hidden neurons (best under 26 constraint)\nHIDDEN_DIMS = [24]  # Best single-layer result from comparison\nMARGIN = 0.01\nN_EPOCHS = 100  # More epochs with LR decay\nLR = 0.01  # Higher initial LR, will decay\nBATCH_SIZE = 128\nWEIGHT_DECAY = 1e-4  # Regularization\nLR_DECAY = 0.98  # Slow decay per epoch\n\nprint(f\"Training Forward-Forward MNIST classifier (10 classes)...\")\nprint(f\"Architecture: {INPUT_DIM} → {HIDDEN_DIMS} → goodness\")\nprint(f\"Total neurons: {sum(HIDDEN_DIMS)} (constraint: <26)\")\nprint(f\"Margin: {MARGIN}, Initial LR: {LR}, LR decay: {LR_DECAY}\")\nprint(f\"Weight decay: {WEIGHT_DECAY}\")\nprint(f\"Training samples: {N_TRAIN}, Test samples: {N_TEST}\")\nprint(\"=\" * 80)\n\ntorch.manual_seed(42)\nmodel = build_ff_mnist_model(HIDDEN_DIMS)\nn_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"Parameters: {n_params}\")\n\nhistory = train_forward_forward_mnist(\n    model, X_train, y_train, X_test, y_test,\n    n_epochs=N_EPOCHS, lr=LR, margin=MARGIN,\n    batch_size=BATCH_SIZE, verbose=True,\n    weight_decay=WEIGHT_DECAY, lr_decay=LR_DECAY\n)\n\nprint(\"=\" * 80)\nprint(f\"Final train accuracy: {history['train_acc'][-1]:.4f}\")\nprint(f\"Final test accuracy: {history['test_acc'][-1]:.4f}\")\nprint(f\"Best test accuracy: {max(history['test_acc']):.4f}\")\nprint(f\"Random baseline: 10%\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n\n# Loss\nax1 = axes[0, 0]\nax1.plot(history['loss'], color='steelblue', lw=2)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Contrastive Loss')\nax1.set_title('Training Loss')\nax1.grid(True, alpha=0.3)\n\n# Goodness\nax2 = axes[0, 1]\nax2.plot(history['goodness_pos'], label='Positive (G+)', color='green', lw=2)\nax2.plot(history['goodness_neg'], label='Negative (G-)', color='red', lw=2)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Mean Goodness')\nax2.set_title('Goodness Values')\nax2.legend()\nax2.grid(True, alpha=0.3)\n\n# Learning rate\nax3 = axes[0, 2]\nax3.plot(history['lr'], color='orange', lw=2)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Learning Rate')\nax3.set_title('Learning Rate Decay')\nax3.grid(True, alpha=0.3)\n\n# Accuracy\nax4 = axes[1, 0]\nax4.plot(history['train_acc'], label='Train', color='coral', lw=2)\nax4.plot(history['test_acc'], label='Test', color='steelblue', lw=2)\nax4.axhline(y=0.1, color='gray', linestyle='--', alpha=0.5, label='Random (10%)')\nbest_epoch = np.argmax(history['test_acc'])\nax4.axvline(x=best_epoch, color='green', linestyle=':', alpha=0.7, label=f'Best ({max(history[\"test_acc\"]):.2%})')\nax4.set_xlabel('Epoch')\nax4.set_ylabel('Accuracy')\nax4.set_title('Classification Accuracy')\nax4.legend()\nax4.grid(True, alpha=0.3)\nax4.set_ylim(0, 1.0)\n\n# Separation\nax5 = axes[1, 1]\nseparation = [p - n for p, n in zip(history['goodness_pos'], history['goodness_neg'])]\nax5.plot(separation, color='purple', lw=2)\nax5.axhline(y=0, color='black', linestyle='--', alpha=0.5)\nax5.set_xlabel('Epoch')\nax5.set_ylabel('G+ - G-')\nax5.set_title('Goodness Separation')\nax5.grid(True, alpha=0.3)\n\n# Train vs Test gap\nax6 = axes[1, 2]\ngap = [t - v for t, v in zip(history['train_acc'], history['test_acc'])]\nax6.plot(gap, color='brown', lw=2)\nax6.axhline(y=0, color='black', linestyle='--', alpha=0.5)\nax6.set_xlabel('Epoch')\nax6.set_ylabel('Train - Test')\nax6.set_title('Generalization Gap')\nax6.grid(True, alpha=0.3)\n\nplt.suptitle(f'Forward-Forward MNIST ({sum(HIDDEN_DIMS)} neurons, 10 classes)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": "def get_predictions_ff(model, X, batch_size=200):\n    \"\"\"Get predictions and goodness for all samples.\"\"\"\n    model.eval()\n    N = X.shape[0]\n    all_predictions = []\n    all_goodness = []\n    \n    with torch.no_grad():\n        for start in range(0, N, batch_size):\n            end = min(start + batch_size, N)\n            X_batch = X[start:end]\n            B = X_batch.shape[0]\n            \n            X_repeated = X_batch.unsqueeze(1).expand(-1, N_CLASSES, -1).reshape(B * N_CLASSES, 784)\n            y_hypotheses = torch.arange(N_CLASSES).unsqueeze(0).expand(B, -1).reshape(B * N_CLASSES)\n            \n            X_embedded = embed_label(X_repeated, y_hypotheses)\n            X_seq = X_embedded.unsqueeze(1)\n            \n            _, layer_states = model(X_seq)\n            \n            total_goodness = torch.zeros(B * N_CLASSES)\n            for layer_idx in range(1, len(model.layers)):\n                act = layer_states[layer_idx][:, -1, :]\n                total_goodness += compute_goodness(act)\n            \n            goodness_matrix = total_goodness.reshape(B, N_CLASSES)\n            all_goodness.append(goodness_matrix)\n            all_predictions.append(goodness_matrix.argmax(dim=1))\n    \n    return torch.cat(all_predictions), torch.cat(all_goodness)\n\n\ndef compute_confusion_matrix(y_true, y_pred, n_classes=N_CLASSES):\n    \"\"\"Compute confusion matrix.\"\"\"\n    cm = np.zeros((n_classes, n_classes), dtype=np.int32)\n    for true, pred in zip(y_true, y_pred):\n        cm[true, pred] += 1\n    return cm\n\n\n# Get test predictions\ntest_preds, test_goodness = get_predictions_ff(model, X_test)\n\n# Confusion matrix (10x10)\ncm = compute_confusion_matrix(y_test.numpy(), test_preds.numpy())\n\nfig, ax = plt.subplots(figsize=(10, 8))\nim = ax.imshow(cm, cmap='Blues')\nax.set_xticks(range(N_CLASSES))\nax.set_yticks(range(N_CLASSES))\nax.set_xlabel('Predicted')\nax.set_ylabel('True')\nax.set_title(f'Confusion Matrix (Test Acc: {history[\"test_acc\"][-1]:.2%})')\n\n# Add text annotations\nfor i in range(N_CLASSES):\n    for j in range(N_CLASSES):\n        text = ax.text(j, i, cm[i, j], ha='center', va='center', fontsize=10,\n                       color='white' if cm[i, j] > cm.max()/2 else 'black')\n\nplt.colorbar(im)\nplt.tight_layout()\nplt.show()\n\n# Per-class accuracy\nprint(\"\\nPer-class accuracy:\")\nfor digit in range(N_CLASSES):\n    mask = y_test == digit\n    if mask.sum() > 0:\n        digit_acc = (test_preds[mask] == digit).float().mean().item()\n        print(f\"  Digit {digit}: {digit_acc:.2%}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": "# Show some predictions\nn_show = 20\nfig, axes = plt.subplots(4, 5, figsize=(15, 12))\n\nfor i, ax in enumerate(axes.flat):\n    if i >= n_show:\n        break\n    \n    img = X_test[i].view(28, 28).numpy()\n    true_label = y_test[i].item()\n    pred_label = test_preds[i].item()\n    \n    ax.imshow(img, cmap='gray')\n    color = 'green' if pred_label == true_label else 'red'\n    ax.set_title(f'True: {true_label}, Pred: {pred_label}', color=color)\n    ax.axis('off')\n\nplt.suptitle('Forward-Forward MNIST Predictions (green=correct, red=wrong)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "# Show goodness distribution for a few samples\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\n\nfor i, ax in enumerate(axes.flat):\n    goodness_vals = test_goodness[i].numpy()\n    true_label = y_test[i].item()\n    pred_label = test_preds[i].item()\n    \n    colors = ['green' if d == true_label else 'lightgray' for d in range(N_CLASSES)]\n    colors[pred_label] = 'red' if pred_label != true_label else 'green'\n    \n    ax.bar(range(N_CLASSES), goodness_vals, color=colors)\n    ax.set_xticks(range(N_CLASSES))\n    ax.set_xlabel('Digit')\n    ax.set_ylabel('Goodness')\n    status = '✓' if pred_label == true_label else '✗'\n    ax.set_title(f'True: {true_label}, Pred: {pred_label} {status}')\n\nplt.suptitle('Goodness Distribution per Digit Hypothesis', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 9. Compare with Different Hidden Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Compare different architectures (all under 26 neurons)\nhidden_configs = [\n    [8],       # 8 neurons\n    [12],      # 12 neurons  \n    [16],      # 16 neurons\n    [20],      # 20 neurons\n    [24],      # 24 neurons (best single-layer so far)\n    [12, 12],  # Two-layer: 24 total neurons\n    [8, 16],   # Two-layer: 24 total neurons (asymmetric)\n]\n\ncomparison_results = []\n\nprint(\"Comparing architectures (all <26 neurons)...\")\nprint(\"=\" * 80)\n\nfor hidden_dims in hidden_configs:\n    torch.manual_seed(42)\n    model = build_ff_mnist_model(hidden_dims)\n    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_neurons = sum(hidden_dims)\n    \n    history = train_forward_forward_mnist(\n        model, X_train, y_train, X_test, y_test,\n        n_epochs=50, lr=0.01, margin=0.01,\n        batch_size=128, verbose=False,\n        weight_decay=1e-4, lr_decay=0.98\n    )\n    \n    best_test = max(history['test_acc'])\n    comparison_results.append({\n        'hidden_dims': str(hidden_dims),\n        'total_neurons': total_neurons,\n        'n_params': n_params,\n        'train_acc': history['train_acc'][-1],\n        'test_acc': history['test_acc'][-1],\n        'best_test': best_test,\n    })\n    \n    print(f\"Hidden={str(hidden_dims):12s} | Neurons={total_neurons:2d} | Params={n_params:6d} | \"\n          f\"Final: {history['test_acc'][-1]:.4f} | Best: {best_test:.4f}\")\n\nprint(\"=\" * 80)\n\n# Find best architecture\nbest_result = max(comparison_results, key=lambda x: x['best_test'])\nprint(f\"\\nBest architecture: {best_result['hidden_dims']} with {best_result['best_test']:.2%} test accuracy\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## 10. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\" * 70)\nprint(\"CONCLUSIONS: FORWARD-FORWARD MNIST (10 Classes)\")\nprint(\"=\" * 70)\n\nprint(f\"\\n1. ARCHITECTURE:\")\nprint(f\"   Input: 784 pixels + {N_CLASSES} label = {INPUT_DIM}\")\nprint(f\"   Hidden: {sum(HIDDEN_DIMS)} SingleDendrite neurons (constraint: <26)\")\nprint(f\"   Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n\nprint(f\"\\n2. PERFORMANCE:\")\nprint(f\"   Train accuracy: {history['train_acc'][-1]:.2%}\")\nprint(f\"   Test accuracy:  {history['test_acc'][-1]:.2%}\")\nprint(f\"   Best test:      {max(history['test_acc']):.2%}\")\nprint(f\"   Random baseline: 10%\")\n\nprint(f\"\\n3. TRAINING IMPROVEMENTS:\")\nprint(f\"   ✓ Learning rate decay: {LR_DECAY}/epoch\")\nprint(f\"   ✓ Weight decay: {WEIGHT_DECAY}\")\nprint(f\"   ✓ Training samples: {N_TRAIN} (4x increase)\")\nprint(f\"   ✓ Architecture search for optimal neuron count\")\n\nprint(f\"\\n4. HARDWARE COMPATIBILITY:\")\nprint(f\"   ✓ Goodness = mean(I²) = power measurement\")\nprint(f\"   ✓ Label embedding = optical input modulation\")\nprint(f\"   ✓ No backward pass for inference\")\nprint(f\"   ✓ Only {sum(HIDDEN_DIMS)} physical neurons needed!\")\n\nprint(f\"\\n5. INFERENCE COST:\")\nprint(f\"   {N_CLASSES} forward passes per sample (one per digit)\")\nprint(f\"   Compare goodness across hypotheses to classify\")\n\nprint(\"\\n\" + \"=\" * 70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}