{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# Training Pipeline Diagnostic\n",
    "\n",
    "Step-by-step inspection of every component before running training.\n",
    "Includes an audit against the reference `disruptcnn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "\n",
    "from dataset_ecei_tcn import ECEiTCNDataset, create_loaders\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-config",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration — disruptcnn run.sh vs ours\n",
    "\n",
    "| Parameter | disruptcnn `run.sh` | Our pipeline | Match? |\n",
    "|-----------|-------------------|--------------|--------|\n",
    "| batch_size | 12 | 12 | Yes |\n",
    "| lr | **0.5** | **2e-3** | **NO — 250x smaller** |\n",
    "| epochs | 1500 | 50 | Intentional |\n",
    "| dropout | 0.1 | 0.1 | Yes |\n",
    "| clip | 0.3 | 0.3 | Yes |\n",
    "| data_step | 10 | 10 | Yes |\n",
    "| nsub | 78125 (decimated) | 781250 (raw) / 10 = 78125 | Yes |\n",
    "| stride | nsub - nrecept + 1 = 48109 | **481260 / 10 = 48126** | **NO — off by 17** |\n",
    "| levels | 4 | 4 | Yes |\n",
    "| nhid | 80 | 80 | Yes |\n",
    "| kernel_size | 15 | 15 | Yes |\n",
    "| dilation_size | 10 | 10 | Yes |\n",
    "| nrecept (target) | 30000 | 30000 | Yes |\n",
    "| warmup | 5 epochs | **3 epochs** | **NO** |\n",
    "| warmup multiplier | 8 | 8 | Yes |\n",
    "| plateau patience | 10 (default) | **5** | **NO** |\n",
    "| label +1 offset | Yes | **No** | **NO (off-by-one)** |\n",
    "| sampling | undersample negatives | oversample positives | Different approach |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Data paths ─────────────────────────────────────────────────────\n",
    "ROOT           = '/global/cfs/cdirs/m5187/proj-share/ECEi_excerpt/dsrpt'\n",
    "DECIMATED_ROOT = '/global/cfs/cdirs/m5187/proj-share/ECEi_excerpt/dsrpt_decimated'\n",
    "\n",
    "# ── disruptcnn run.sh values ──────────────────────────────────────\n",
    "DATA_STEP       = 10\n",
    "TWARN           = 300_000       # 300 ms at 1 MHz\n",
    "BASELINE_LEN    = 40_000        # 40 ms at 1 MHz\n",
    "NSUB_RAW        = 781_250       # 78125 * 10\n",
    "BATCH_SIZE      = 12\n",
    "NUM_WORKERS     = 4\n",
    "\n",
    "# ── Model (from run.sh) ───────────────────────────────────────────\n",
    "INPUT_CHANNELS  = 160\n",
    "N_CLASSES       = 1\n",
    "LEVELS          = 4\n",
    "NHID            = 80\n",
    "KERNEL_SIZE     = 15\n",
    "DILATION_BASE   = 10\n",
    "DROPOUT         = 0.1\n",
    "NRECEPT_TARGET  = 30_000\n",
    "\n",
    "# ── Training (from run.sh) ────────────────────────────────────────\n",
    "LR_DISRUPTCNN   = 0.5           # run.sh value\n",
    "LR_OURS         = 2e-3          # what we had\n",
    "CLIP            = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-rf",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Receptive field & dilation schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rf(kernel_size, dilation_sizes):\n",
    "    return 1 + 2 * (kernel_size - 1) * int(np.sum(dilation_sizes))\n",
    "\n",
    "# Build dilation schedule exactly as disruptcnn\n",
    "base_dilations = [DILATION_BASE ** i for i in range(LEVELS - 1)]  # [1, 10, 100]\n",
    "rf_without_last = calc_rf(KERNEL_SIZE, base_dilations)\n",
    "last_dilation = int(np.ceil(\n",
    "    (NRECEPT_TARGET - rf_without_last) / (2.0 * (KERNEL_SIZE - 1))))\n",
    "last_dilation = max(last_dilation, 1)\n",
    "DILATION_SIZES = base_dilations + [last_dilation]\n",
    "NRECEPT = calc_rf(KERNEL_SIZE, DILATION_SIZES)\n",
    "\n",
    "print(f'Base dilations (levels 0..{LEVELS-2}): {base_dilations}')\n",
    "print(f'RF without last level   : {rf_without_last:,}')\n",
    "print(f'Last-level dilation     : {last_dilation}')\n",
    "print(f'Final dilation schedule : {DILATION_SIZES}')\n",
    "print(f'Actual receptive field  : {NRECEPT:,} decimated samples')\n",
    "print(f'  = {NRECEPT / 1e5 * 1e3:.2f} ms at 100 kHz')\n",
    "print()\n",
    "\n",
    "# ── Stride: should be nsub_dec - nrecept + 1 ──\n",
    "NSUB_DEC = NSUB_RAW // DATA_STEP  # 78125\n",
    "STRIDE_CORRECT_DEC = NSUB_DEC - NRECEPT + 1\n",
    "STRIDE_CORRECT_RAW = STRIDE_CORRECT_DEC * DATA_STEP\n",
    "\n",
    "STRIDE_OLD_RAW = 481_260\n",
    "STRIDE_OLD_DEC = STRIDE_OLD_RAW // DATA_STEP\n",
    "\n",
    "print(f'nsub (decimated)        : {NSUB_DEC:,}')\n",
    "print(f'Correct stride (dec)    : {STRIDE_CORRECT_DEC:,}  (nsub - nrecept + 1 = {NSUB_DEC} - {NRECEPT} + 1)')\n",
    "print(f'Correct stride (raw)    : {STRIDE_CORRECT_RAW:,}')\n",
    "print(f'Old stride (dec)        : {STRIDE_OLD_DEC:,}  <- ERROR: off by {STRIDE_OLD_DEC - STRIDE_CORRECT_DEC}')\n",
    "print(f'Overlap per subseq      : {NSUB_DEC - STRIDE_CORRECT_DEC:,} decimated samples = {(NSUB_DEC - STRIDE_CORRECT_DEC) / 1e5 * 1e3:.1f} ms')\n",
    "print(f'  (overlap ≈ receptive field - 1 = {NRECEPT - 1:,})')\n",
    "\n",
    "# Use the correct stride from here on\n",
    "STRIDE_RAW = STRIDE_CORRECT_RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-model",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model architecture inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-def",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, dilation_size=2, kernel_size=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        if np.isscalar(dilation_size):\n",
    "            dilation_size = [dilation_size ** i for i in range(num_levels)]\n",
    "        for i in range(num_levels):\n",
    "            in_ch = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_ch = num_channels[i]\n",
    "            layers.append(TemporalBlock(in_ch, out_ch, kernel_size, stride=1,\n",
    "                                        padding=(kernel_size-1)*dilation_size[i],\n",
    "                                        dilation=dilation_size[i], dropout=dropout))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout, dilation_size):\n",
    "        super().__init__()\n",
    "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size,\n",
    "                                   dropout=dropout, dilation_size=dilation_size)\n",
    "        self.linear = nn.Linear(num_channels[-1], output_size)\n",
    "    def forward(self, x):\n",
    "        y = self.tcn(x)\n",
    "        o = self.linear(y.permute(0, 2, 1))\n",
    "        return torch.sigmoid(o.squeeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-build",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_sizes = [NHID] * LEVELS\n",
    "model = TCN(INPUT_CHANNELS, N_CLASSES, channel_sizes,\n",
    "            kernel_size=KERNEL_SIZE, dropout=DROPOUT,\n",
    "            dilation_size=DILATION_SIZES).to(DEVICE)\n",
    "\n",
    "# ── Layer-by-layer summary ──\n",
    "print(f'{\"Layer\":<45s} {\"Type\":<25s} {\"Output shape\":<25s} {\"Params\":>10s}')\n",
    "print('─' * 110)\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if len(list(module.children())) > 0:\n",
    "        continue  # skip container modules\n",
    "    n_params = sum(p.numel() for p in module.parameters())\n",
    "    n_train  = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    total_params += n_params\n",
    "    trainable_params += n_train\n",
    "    typ = module.__class__.__name__\n",
    "\n",
    "    # extra info for Conv1d\n",
    "    extra = ''\n",
    "    if isinstance(module, nn.Conv1d):\n",
    "        extra = f'  k={module.kernel_size[0]} d={module.dilation[0]} p={module.padding[0]}'\n",
    "    print(f'{name:<45s} {typ:<25s} {extra:<25s} {n_params:>10,}')\n",
    "\n",
    "print('─' * 110)\n",
    "print(f'Total parameters     : {total_params:>12,}')\n",
    "print(f'Trainable parameters : {trainable_params:>12,}')\n",
    "print(f'Model size (MB)      : {total_params * 4 / 1e6:>12.2f}  (float32)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model-per-level",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Per-level breakdown ──\n",
    "print(f'{\"Level\":<8s} {\"In ch\":<8s} {\"Out ch\":<8s} {\"Dilation\":<10s} '\n",
    "      f'{\"Padding\":<10s} {\"RF contrib\":<12s} {\"Cumul RF\":<12s} {\"Params\":>10s}')\n",
    "print('─' * 85)\n",
    "\n",
    "cumul_rf = 1\n",
    "for i in range(LEVELS):\n",
    "    in_ch  = INPUT_CHANNELS if i == 0 else NHID\n",
    "    out_ch = NHID\n",
    "    d      = DILATION_SIZES[i]\n",
    "    pad    = (KERNEL_SIZE - 1) * d\n",
    "    rf_add = 2 * (KERNEL_SIZE - 1) * d  # each block adds this\n",
    "    cumul_rf += rf_add\n",
    "\n",
    "    # param count for this block\n",
    "    block = list(model.tcn.network.children())[i]\n",
    "    bp = sum(p.numel() for p in block.parameters())\n",
    "\n",
    "    print(f'{i:<8d} {in_ch:<8d} {out_ch:<8d} {d:<10,d} '\n",
    "          f'{pad:<10,d} {rf_add:<12,d} {cumul_rf:<12,d} {bp:>10,}')\n",
    "\n",
    "print('─' * 85)\n",
    "print(f'Final RF = {cumul_rf:,} decimated samples ({cumul_rf/1e5*1e3:.2f} ms at 100 kHz)')\n",
    "assert cumul_rf == NRECEPT, f'Mismatch: {cumul_rf} != {NRECEPT}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-data",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Dataset & subsequence structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ECEiTCNDataset(\n",
    "    root            = ROOT,\n",
    "    decimated_root  = DECIMATED_ROOT,\n",
    "    Twarn           = TWARN,\n",
    "    baseline_length = BASELINE_LEN,\n",
    "    data_step       = DATA_STEP,\n",
    "    nsub            = NSUB_RAW,\n",
    "    stride          = STRIDE_RAW,   # corrected stride\n",
    "    normalize       = True,\n",
    ")\n",
    "ds.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subseq-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total = len(ds)\n",
    "n_pos   = int(ds.seq_has_disrupt.sum())\n",
    "n_neg   = n_total - n_pos\n",
    "\n",
    "print(f'Total subsequences  : {n_total}')\n",
    "print(f'  disruptive (has label-1 region) : {n_pos} ({n_pos/n_total*100:.1f}%)')\n",
    "print(f'  clear      (all label-0)        : {n_neg} ({n_neg/n_total*100:.1f}%)')\n",
    "print(f'Subsequence length  : {ds._T_sub:,} decimated samples')\n",
    "print(f'  = {ds._T_sub / 1e5 * 1e3:.1f} ms at 100 kHz')\n",
    "print(f'Overlap             : {NSUB_DEC - STRIDE_CORRECT_DEC:,} decimated samples')\n",
    "print(f'  = receptive field - 1 = {NRECEPT - 1:,}')\n",
    "print()\n",
    "print(f'pos_weight = {ds.pos_weight:.4f}')\n",
    "print(f'neg_weight = {ds.neg_weight:.4f}')\n",
    "print(f'  ratio pos/neg weight = {ds.pos_weight / ds.neg_weight:.2f}x')\n",
    "\n",
    "# ── Per-shot breakdown ──\n",
    "print(f'\\nPer-shot subsequence counts:')\n",
    "from collections import Counter\n",
    "shot_counts = Counter(ds.seq_shot_idx.tolist())\n",
    "for s_idx, count in sorted(shot_counts.items()):\n",
    "    has_d = ds.seq_has_disrupt[ds.seq_shot_idx == s_idx]\n",
    "    n_d = int(has_d.sum())\n",
    "    shot = ds.shots[s_idx]\n",
    "    split = ds.splits[s_idx]\n",
    "    print(f'  shot {shot} ({split:>5s}): {count:3d} subseqs '\n",
    "          f'({n_d} disruptive, {count - n_d} clear)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-input",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Input data: shape, scale, statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "input-stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a few subsequences and inspect\n",
    "N_INSPECT = min(8, len(ds))\n",
    "print(f'Inspecting {N_INSPECT} subsequences (indices 0..{N_INSPECT-1}):\\n')\n",
    "\n",
    "print(f'{\"idx\":<5s} {\"X shape\":<18s} {\"X min\":<12s} {\"X max\":<12s} '\n",
    "      f'{\"X mean\":<12s} {\"X std\":<12s} {\"tgt mean\":<10s} {\"pos%\":<8s} '\n",
    "      f'{\"wgt_pos\":<10s} {\"wgt_neg\":<10s}')\n",
    "print('─' * 120)\n",
    "\n",
    "X_samples = []\n",
    "for i in range(N_INSPECT):\n",
    "    X, target, weight = ds[i]\n",
    "    X_np = X.numpy()\n",
    "    tgt_np = target.numpy()\n",
    "    wgt_np = weight.numpy()\n",
    "    X_samples.append(X_np)\n",
    "\n",
    "    pos_frac = tgt_np.mean()\n",
    "    wgt_pos = wgt_np[tgt_np > 0.5].mean() if pos_frac > 0 else 0\n",
    "    wgt_neg = wgt_np[tgt_np < 0.5].mean() if pos_frac < 1 else 0\n",
    "\n",
    "    print(f'{i:<5d} {str(X_np.shape):<18s} {X_np.min():<12.4f} {X_np.max():<12.4f} '\n",
    "          f'{X_np.mean():<12.6f} {X_np.std():<12.4f} {tgt_np.mean():<10.4f} '\n",
    "          f'{pos_frac:<8.3f} {wgt_pos:<10.4f} {wgt_neg:<10.4f}')\n",
    "\n",
    "X_all = np.stack(X_samples)  # (N, 20, 8, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "input-histograms",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# 1. Value distribution\n",
    "vals = X_all.flatten()\n",
    "axes[0].hist(vals[::100], bins=200, color='steelblue', alpha=0.7, edgecolor='none')\n",
    "axes[0].axvline(vals.mean(), color='red', linestyle='--', label=f'mean={vals.mean():.3f}')\n",
    "axes[0].axvline(0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[0].set_title(f'Input value distribution (sampled from {N_INSPECT} subseqs)')\n",
    "axes[0].set_xlabel('Normalized value')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(-6, 6)\n",
    "\n",
    "# 2. Per-channel mean\n",
    "ch_means = X_all.mean(axis=(0, -1))  # (20, 8)\n",
    "im = axes[1].imshow(ch_means, aspect='auto', cmap='RdBu_r')\n",
    "axes[1].set_title('Per-channel mean (should be ≈ 0)')\n",
    "axes[1].set_xlabel('Column (0-7)')\n",
    "axes[1].set_ylabel('Row (0-19)')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "# 3. Per-channel std\n",
    "ch_stds = X_all.std(axis=(0, -1))  # (20, 8)\n",
    "im2 = axes[2].imshow(ch_stds, aspect='auto', cmap='viridis')\n",
    "axes[2].set_title('Per-channel std (should be ≈ 1)')\n",
    "axes[2].set_xlabel('Column (0-7)')\n",
    "axes[2].set_ylabel('Row (0-19)')\n",
    "plt.colorbar(im2, ax=axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Overall: mean={vals.mean():.4f}, std={vals.std():.4f}, '\n",
    "      f'min={vals.min():.2f}, max={vals.max():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "input-signal-viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Visualise a single subsequence: signal + target + weight ──\n",
    "idx_show = 0\n",
    "X_s, tgt_s, wgt_s = ds[idx_show]\n",
    "X_np = X_s.numpy()\n",
    "tgt_np = tgt_s.numpy()\n",
    "wgt_np = wgt_s.numpy()\n",
    "T = X_np.shape[-1]\n",
    "t_ms = np.arange(T) / 1e5 * 1e3  # ms\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 7), sharex=True,\n",
    "                         gridspec_kw={'height_ratios': [3, 1, 1]})\n",
    "\n",
    "# Signal (a few channels)\n",
    "for ch in [(0,0), (5,3), (10,4), (19,7)]:\n",
    "    axes[0].plot(t_ms, X_np[ch[0], ch[1], :], linewidth=0.3,\n",
    "                label=f'ch({ch[0]},{ch[1]})', alpha=0.7)\n",
    "axes[0].axvspan(0, t_ms[NRECEPT-1], alpha=0.1, color='gray', label='Receptive field')\n",
    "axes[0].set_ylabel('Normalized signal')\n",
    "axes[0].set_title(f'Subsequence {idx_show}: (20, 8, {T}) → flattened to (160, {T})')\n",
    "axes[0].legend(fontsize=8, ncol=5)\n",
    "axes[0].grid(True, alpha=0.2)\n",
    "\n",
    "# Target\n",
    "axes[1].fill_between(t_ms, tgt_np, alpha=0.5, color='firebrick')\n",
    "axes[1].set_ylabel('Target')\n",
    "axes[1].set_ylim(-0.05, 1.05)\n",
    "axes[1].grid(True, alpha=0.2)\n",
    "\n",
    "# Weight\n",
    "axes[2].plot(t_ms, wgt_np, color='darkorange', linewidth=1)\n",
    "axes[2].set_ylabel('Weight')\n",
    "axes[2].set_xlabel('Time (ms)')\n",
    "axes[2].grid(True, alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-batch",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Batch structure & stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-inspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = create_loaders(ds, batch_size=BATCH_SIZE, num_workers=0,\n",
    "                         stratified_train=True)\n",
    "\n",
    "for split_name, loader in loaders.items():\n",
    "    n_batches = len(loader.batch_sampler) if hasattr(loader.batch_sampler, '__len__') else len(loader)\n",
    "    print(f'{split_name:>5s}: {n_batches} batches')\n",
    "\n",
    "# ── Inspect first few training batches ──\n",
    "train_loader = loaders.get('train', list(loaders.values())[0])\n",
    "\n",
    "print(f'\\nFirst 5 training batches:')\n",
    "print(f'{\"batch\":<7s} {\"X shape\":<25s} {\"tgt shape\":<18s} '\n",
    "      f'{\"n_pos\":<7s} {\"n_neg\":<7s} {\"pos%\":<8s} '\n",
    "      f'{\"X range\":<20s} {\"tgt mean\":<10s}')\n",
    "print('─' * 105)\n",
    "\n",
    "batch_pos_counts = []\n",
    "for b_idx, (X, target, weight) in enumerate(train_loader):\n",
    "    if b_idx >= 5:\n",
    "        break\n",
    "    # count subsequences with any positive labels\n",
    "    has_pos = (target.sum(dim=-1) > 0)  # per-subseq\n",
    "    n_pos = has_pos.sum().item()\n",
    "    n_neg = X.shape[0] - n_pos\n",
    "    batch_pos_counts.append(n_pos)\n",
    "\n",
    "    print(f'{b_idx:<7d} {str(tuple(X.shape)):<25s} {str(tuple(target.shape)):<18s} '\n",
    "          f'{n_pos:<7d} {n_neg:<7d} {n_pos/X.shape[0]:<8.2f} '\n",
    "          f'[{X.min():.2f}, {X.max():.2f}]{\"\":>5s} {target.mean():.4f}')\n",
    "\n",
    "# Scan all batches for balance\n",
    "print(f'\\nScanning all training batches for class balance...')\n",
    "all_pos = []\n",
    "for X, target, weight in train_loader:\n",
    "    has_pos = (target.sum(dim=-1) > 0)\n",
    "    all_pos.append(has_pos.sum().item())\n",
    "all_pos = batch_pos_counts + all_pos  # combine with first 5\n",
    "\n",
    "all_pos = np.array(all_pos)\n",
    "print(f'  Batches: {len(all_pos)}')\n",
    "print(f'  Pos subseqs per batch: mean={all_pos.mean():.1f}, '\n",
    "      f'min={all_pos.min()}, max={all_pos.max()} '\n",
    "      f'(target={BATCH_SIZE//2})')\n",
    "print(f'  Perfectly balanced batches: '\n",
    "      f'{(all_pos == BATCH_SIZE // 2).sum()}/{len(all_pos)} '\n",
    "      f'({(all_pos == BATCH_SIZE // 2).mean()*100:.0f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-forward",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Forward pass sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "X_sample, tgt_sample, wgt_sample = ds[0]\n",
    "X_in = X_sample.unsqueeze(0).to(DEVICE)             # (1, 20, 8, T)\n",
    "B = X_in.shape[0]\n",
    "X_flat = X_in.view(B, -1, X_in.shape[-1])           # (1, 160, T)\n",
    "\n",
    "print(f'Input shape  : {tuple(X_in.shape)}  →  flattened: {tuple(X_flat.shape)}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(X_flat)                            # (1, T)\n",
    "\n",
    "print(f'Output shape : {tuple(output.shape)}')\n",
    "print(f'Output range : [{output.min():.4f}, {output.max():.4f}]')\n",
    "print(f'Output mean  : {output.mean():.4f}  (untrained → should be ≈ 0.5)')\n",
    "print()\n",
    "\n",
    "# ── Loss region ──\n",
    "T_out = output.shape[-1]\n",
    "T_valid = T_out - (NRECEPT - 1)\n",
    "print(f'Total timesteps       : {T_out:,}')\n",
    "print(f'Excluded (receptive f.): {NRECEPT - 1:,} ({(NRECEPT-1)/T_out*100:.1f}%)')\n",
    "print(f'Valid for loss         : {T_valid:,} ({T_valid/T_out*100:.1f}%)')\n",
    "print()\n",
    "\n",
    "# ── Check untrained loss ──\n",
    "out_v = output[:, NRECEPT-1:].cpu()\n",
    "tgt_v = tgt_sample[NRECEPT-1:].unsqueeze(0)\n",
    "wgt_v = wgt_sample[NRECEPT-1:].unsqueeze(0)\n",
    "loss_val = F.binary_cross_entropy(out_v, tgt_v, weight=wgt_v).item()\n",
    "loss_unweighted = F.binary_cross_entropy(out_v, tgt_v).item()\n",
    "print(f'Untrained weighted BCE   : {loss_val:.4f}')\n",
    "print(f'Untrained unweighted BCE : {loss_unweighted:.4f}')\n",
    "print(f'Random baseline (ln2)    : {np.log(2):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-label",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Label off-by-one audit\n",
    "\n",
    "**disruptcnn** uses `(disrupt_idxi - start_idxi + 1) / data_step` to compute the\n",
    "disruption boundary — the `+1` shifts the label by one raw sample.\n",
    "\n",
    "**Our code** uses `dl // data_step` (no `+1`).\n",
    "\n",
    "The difference is **1 decimated timestep = 10 μs** — negligible for the model,\n",
    "but documented here for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "label-audit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a subsequence with disruption transition\n",
    "trans_idx = np.where((ds.seq_disrupt_local > 0) &\n",
    "                     (ds.seq_disrupt_local < ds._data_nsub))[0]\n",
    "if len(trans_idx) > 0:\n",
    "    i = trans_idx[0]\n",
    "    dl = int(ds.seq_disrupt_local[i])\n",
    "    step = ds._step_in_getitem\n",
    "\n",
    "    our_boundary    = dl // step\n",
    "    disruptcnn_boundary = (dl + 1) // step  # with the +1\n",
    "\n",
    "    print(f'Subsequence {i}:')\n",
    "    print(f'  disrupt_local (data-file space) : {dl}')\n",
    "    print(f'  step_in_getitem                 : {step}')\n",
    "    print(f'  Our label boundary (decimated)  : {our_boundary}')\n",
    "    print(f'  disruptcnn boundary (decimated)  : {disruptcnn_boundary}')\n",
    "    print(f'  Difference                      : {disruptcnn_boundary - our_boundary} '\n",
    "          f'decimated sample(s) = {(disruptcnn_boundary - our_boundary) * 10} μs')\n",
    "else:\n",
    "    print('No transition subsequences found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-lr",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Learning rate schedule comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lr-compare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "n_train_batches = len(train_loader)\n",
    "WARMUP_MULT = 8\n",
    "\n",
    "configs = [\n",
    "    {'name': 'disruptcnn (lr=0.5, warmup=5ep, patience=10)',\n",
    "     'lr': 0.5, 'warmup_epochs': 5, 'patience': 10},\n",
    "    {'name': 'Ours old (lr=2e-3, warmup=3ep, patience=5)',\n",
    "     'lr': 2e-3, 'warmup_epochs': 3, 'patience': 5},\n",
    "    {'name': 'Ours corrected (lr=0.5, warmup=5ep, patience=10)',\n",
    "     'lr': 0.5, 'warmup_epochs': 5, 'patience': 10},\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "colors = ['firebrick', 'steelblue', 'seagreen']\n",
    "\n",
    "for cfg, color in zip(configs, colors):\n",
    "    # simulate warmup only (plateau needs loss, skip)\n",
    "    warmup_iters = cfg['warmup_epochs'] * n_train_batches\n",
    "    iters = np.arange(0, 20 * n_train_batches + 1)\n",
    "    lrs = []\n",
    "    for it in iters:\n",
    "        if it < warmup_iters:\n",
    "            scale = (1 - 1/WARMUP_MULT) / max(warmup_iters, 1) * it + 1/WARMUP_MULT\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        lrs.append(cfg['lr'] * scale)\n",
    "\n",
    "    epochs_ax = iters / n_train_batches\n",
    "    ax.plot(epochs_ax, lrs, label=cfg['name'], color=color, linewidth=1.5)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.set_title('LR warmup comparison (plateau decay not simulated)')\n",
    "ax.set_yscale('log')\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_xlim(0, 20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'disruptcnn: start LR = {0.5/8:.4f}, peak LR = 0.5 at epoch 5')\n",
    "print(f'Ours old  : start LR = {2e-3/8:.6f}, peak LR = 0.002 at epoch 3')\n",
    "print(f'Ours fixed: start LR = {0.5/8:.4f}, peak LR = 0.5 at epoch 5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec-summary",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary of fixes needed for `train_tcn.ipynb`\n",
    "\n",
    "| # | Issue | Fix |\n",
    "|---|-------|-----|\n",
    "| 1 | **LR = 2e-3** (should be 0.5) | `LR = 0.5` |\n",
    "| 2 | **STRIDE hardcoded** (off by 17 dec samples) | Compute dynamically: `STRIDE = (NSUB // DATA_STEP - NRECEPT + 1) * DATA_STEP` |\n",
    "| 3 | **Warmup = 3 epochs** (should be 5) | `WARMUP_EPOCHS = 5` |\n",
    "| 4 | **Plateau patience = 5** (should be 10) | Remove explicit `patience=5` (use default 10) |\n",
    "| 5 | **Label +1 offset** missing | Add `+1` in `__getitem__`: `d = min((dl + 1) // step, T)` |\n",
    "| 6 | **Sampling** differs | Acceptable — both achieve balanced batches |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Corrected values for train_tcn.ipynb:')\n",
    "print(f'  LR             = 0.5')\n",
    "print(f'  STRIDE         = {STRIDE_CORRECT_RAW}  (computed: nsub_dec - nrecept + 1 = {STRIDE_CORRECT_DEC}, × {DATA_STEP})')\n",
    "print(f'  WARMUP_EPOCHS  = 5')\n",
    "print(f'  Plateau patience = 10 (PyTorch default)')\n",
    "print(f'  NRECEPT        = {NRECEPT}  (computed from model)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
